This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-20T13:47:10.394Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
en/
  learn-more/
    extended-reading/
      retrieval-augment/
        hybrid-search.md
        README.md
        rerank.md
        retrieval.md
      how-to-use-json-schema-in-dify.md
      README.md
      what-is-llmops.md
    faq/
      install-faq.md
      README.md
      use-llms-faq.md
    prompt-engineering/
      prompt-engineering-1/
        prompt-engineering-template.md
        README.md
      README.md
    use-cases/
      build-an-notion-ai-assistant.md
      create-a-midjourney-prompt-bot-with-dify.md
      create-an-ai-chatbot-with-business-data-in-minutes.md
      how-to-connect-aws-bedrock.md
      how-to-creat-dify-schedule.md
      how-to-integrate-dify-chatbot-to-your-wix-website.md
      README.md
    how-to-use-json-schema-in-dify.md
```

# Files

## File: en/learn-more/extended-reading/retrieval-augment/hybrid-search.md
```markdown
# Hybrid Search

### Why is Hybrid Search Needed?

The mainstream method in the retrieval phase of RAG (Retrieval-Augmented Generation) is vector search, which matches based on semantic relevance. The technical principle involves splitting the documents in the external knowledge base into semantically complete paragraphs or sentences, converting them into a series of numbers (multi-dimensional vectors) that the computer can understand, and performing the same conversion on the user's query.

The computer can detect subtle semantic relationships between the user's query and the sentences. For example, "cats chase mice" and "kittens hunt mice" will have a higher semantic relevance than "cats chase mice" and "I like eating ham." After finding the most relevant text content, the RAG system provides it as context for the user's query to the large model, helping it answer the question.

In addition to enabling complex semantic text retrieval, vector search has other advantages:

* Understanding similar semantics (e.g., mouse/mousetrap/cheese, Google/Bing/search engine)
* Multilingual understanding (cross-language understanding, such as matching English input with Chinese)
* Multimodal understanding (support for similar matching of text, images, audio, video, etc.)
* Fault tolerance (handling spelling errors and vague descriptions)

While vector search has clear advantages in the above scenarios, it performs poorly in certain situations, such as:

* Searching for names of people or objects (e.g., Elon Musk, iPhone 15)
* Searching for abbreviations or phrases (e.g., RAG, RLHF)
* Searching for IDs (e.g., `gpt-3.5-turbo`, `titan-xlarge-v1.01`)

These weaknesses are precisely the strengths of traditional keyword search, which excels in:

* Exact matching (e.g., product names, personal names, product numbers)
* Matching with a few characters (vector search performs poorly with few characters, but many users tend to input only a few keywords)
* Matching low-frequency words (low-frequency words often carry significant meaning in language, such as "Would you like to have coffee with me?" where "have" and "coffee" carry more importance than "you" and "like")

For most text search scenarios, the primary goal is to ensure that the most relevant potential results appear in the candidate results. Vector search and keyword search each have their advantages in the retrieval field. Hybrid search combines the strengths of both search technologies and compensates for their weaknesses.

In hybrid search, you need to establish vector indexes and keyword indexes in the database in advance. When a user query is input, the most relevant texts are retrieved from the documents using both retrieval methods.

<figure><img src="../../../.gitbook/assets/image (127).png" alt="" width="563"><figcaption><p>Hybrid Search</p></figcaption></figure>

"Hybrid search" does not have a precise definition. This article uses the combination of vector search and keyword search as an example. If we use other combinations of search algorithms, it can also be called "hybrid search." For instance, we can combine knowledge graph techniques for retrieving entity relationships with vector search techniques.

Different retrieval systems excel at finding various subtle relationships between texts (paragraphs, sentences, words), including exact relationships, semantic relationships, thematic relationships, structural relationships, entity relationships, temporal relationships, event relationships, etc. No single retrieval mode can be suitable for all scenarios. **Hybrid search achieves complementarity between multiple retrieval technologies through the combination of multiple retrieval systems.**

### Vector Search

Definition: Generating query embeddings and querying the text segments most similar to their vector representations.

<figure><img src="../../../.gitbook/assets/image (116).png" alt="" width="563"><figcaption><p>Vector Search Settings</p></figcaption></figure>

**TopK:** Used to filter the text fragments most similar to the user's query. The system will dynamically adjust the number of fragments based on the context window size of the selected model. The default value is 3.

**Score Threshold:** Used to set the similarity threshold for filtering text fragments, i.e., only recalling text fragments that exceed the set score. The system's default is to turn off this setting, meaning it does not filter the similarity values of recalled text fragments. When enabled, the default value is 0.5.

**Rerank Model:** After configuring the Rerank model's API key on the "Model Providers" page, you can enable the "Rerank Model" in the retrieval settings. The system will perform semantic re-ranking on the recalled document results after semantic retrieval to optimize the ranking results. When the Rerank model is set, the TopK and Score Threshold settings only take effect in the Rerank step.

### Full-Text Search

Definition: Indexing all words in the document, allowing users to query any word and return text fragments containing those words.

<figure><img src="../../../.gitbook/assets/image (122).png" alt="" width="563"><figcaption><p>Full-Text Search Settings</p></figcaption></figure>

**TopK:** Used to filter the text fragments most similar to the user's query. The system will dynamically adjust the number of fragments based on the context window size of the selected model. The default value is 3.

**Rerank Model:** After configuring the Rerank model's API key on the "Model Providers" page, you can enable the "Rerank Model" in the retrieval settings. The system will perform semantic re-ranking on the recalled document results after full-text retrieval to optimize the ranking results. When the Rerank model is set, the TopK and Score Threshold settings only take effect in the Rerank step.

### Hybrid Search

Simultaneously performs full-text search and vector search, applying a re-ranking step to select the best results matching the user's query from both types of query results. Requires configuring the Rerank model API.

<figure><img src="../../../.gitbook/assets/image (118).png" alt="" width="563"><figcaption><p>Hybrid Search Settings</p></figcaption></figure>

**TopK:** Used to filter the text fragments most similar to the user's query. The system will dynamically adjust the number of fragments based on the context window size of the selected model. The default value is 3.

**Rerank Model:** After configuring the Rerank model's API key on the "Model Providers" page, you can enable the "Rerank Model" in the retrieval settings. The system will perform semantic re-ranking on the recalled document results after hybrid retrieval to optimize the ranking results. When the Rerank model is set, the TopK and Score Threshold settings only take effect in the Rerank step.

### Setting Retrieval Mode When Creating a Dataset

Set different retrieval modes by entering the "Dataset -> Create Dataset" page and configuring the retrieval settings.

<figure><img src="../../../.gitbook/assets/image (119).png" alt="" width="563"><figcaption><p>Setting Retrieval Mode When Creating a Dataset</p></figcaption></figure>

### Modifying Retrieval Mode in Dataset Settings

Modify the retrieval mode of an existing dataset by entering the "Dataset -> Select Dataset -> Settings" page.

<figure><img src="../../../.gitbook/assets/image (120).png" alt="" width="563"><figcaption><p>Modifying Retrieval Mode in Dataset Settings</p></figcaption></figure>

### Modifying Retrieval Mode in Prompt Arrangement

Modify the retrieval mode when creating an application by entering the "Prompt Arrangement -> Context -> Select Dataset -> Settings" page.

<figure><img src="../../../.gitbook/assets/image (121).png" alt=""><figcaption><p>Modifying Retrieval Mode in Prompt Arrangement</p></figcaption></figure>
```

## File: en/learn-more/extended-reading/retrieval-augment/README.md
```markdown
# Retrieval-Augmented Generation (RAG)

### Explanation of RAG Concept

The RAG architecture, with vector retrieval at its core, has become the mainstream technical framework for enabling large models to access the latest external knowledge while addressing the problem of hallucinations in generated content. This technology has been implemented in a variety of application scenarios.

Developers can use this technology to build AI-powered customer service, enterprise knowledge bases, AI search engines, and more at a low cost. By using natural language input to interact with various forms of knowledge organization, they can create intelligent systems. Let's take a representative RAG application as an example:

In the diagram below, when a user asks, "Who is the President of the United States?", the system does not directly pass the question to the large model for an answer. Instead, it first performs a vector search in a knowledge base (such as Wikipedia shown in the diagram) to find relevant content through semantic similarity matching (e.g., "Joe Biden is the 46th and current president of the United States..."). Then, the system provides the user's question along with the retrieved relevant knowledge to the large model, allowing it to obtain sufficient information to answer the question reliably.

<figure><img src="../../../.gitbook/assets/image (129).png" alt=""><figcaption><p>Basic RAG Architecture</p></figcaption></figure>

**Why is this necessary?**

We can think of a large model as a super expert who is familiar with various fields of human knowledge. However, it has its limitations. For instance, it does not know personal information about you because such information is private and not publicly available on the internet, so it has no prior learning opportunity.

When you want to hire this super expert as your personal financial advisor, you need to allow them to review your investment records, household expenses, and other data before answering your questions. This way, the expert can provide professional advice based on your personal circumstances.

**This is exactly what the RAG system does: it helps the large model temporarily acquire external knowledge it does not possess, allowing it to find answers before responding to questions.**

From the example above, it is easy to see that the most critical part of the RAG system is the retrieval of external knowledge. Whether the expert can provide professional financial advice depends on whether they can accurately find the necessary information. If they find your weight loss plan instead of your investment records, even the most knowledgeable expert would be powerless.
```

## File: en/learn-more/extended-reading/retrieval-augment/rerank.md
```markdown
# Re-ranking

### Why is Re-ranking Needed?

Hybrid search can leverage the strengths of different retrieval technologies to achieve better recall results. However, the query results from different retrieval modes need to be merged and normalized (converting data to a uniform standard range or distribution for better comparison, analysis, and processing) before being provided to the large model together. This is where a scoring system comes in: the Re-rank Model.

**The re-rank model calculates the semantic match between the list of candidate documents and the user query, reordering them based on semantic match to improve the results of semantic sorting.** The principle is to compute a relevance score between the user query and each candidate document and return a list of documents sorted by relevance from high to low. Common re-rank models include Cohere rerank, bge-reranker, etc.

<figure><img src="../../../.gitbook/assets/image (128) (1).png" alt=""><figcaption><p>Hybrid Search + Re-ranking</p></figcaption></figure>

In most cases, there is a preliminary retrieval before re-ranking because calculating the relevance score between a query and millions of documents would be highly inefficient. Therefore, **re-ranking is typically placed at the final stage of the search process and is ideal for merging and sorting results from different retrieval systems.**

However, re-ranking is not only applicable for merging results from different retrieval systems. Even in a single retrieval mode, introducing a re-ranking step can effectively improve document recall. For example, semantic re-ranking can be added after keyword retrieval.

In practical applications, besides normalizing multiple query results, we generally limit the number of segments passed to the large model (i.e., TopK, which can be set in the re-rank model parameters) before handing over the relevant text segments to the large model. This is because the input window of the large model has size limitations (typically 4K, 8K, 16K, 128K tokens). You need to choose an appropriate segmentation strategy and TopK value based on the input window size of the selected model.

It is important to note that even if the model's context window is large enough, recalling too many segments may introduce less relevant content, reducing the quality of the response. Therefore, the TopK parameter for re-ranking is not necessarily the larger, the better.

Re-ranking is not a replacement for search technology but an auxiliary tool to enhance existing retrieval systems. **Its greatest advantage is that it provides a simple and low-complexity method to improve search results, allowing users to incorporate semantic relevance into existing search systems without significant infrastructure modifications.**

For example, with Cohere Rerank, you only need to register an account and apply for an API. Integration requires just two lines of code. Additionally, they offer multilingual models, meaning you can sort query results in different languages simultaneously.

### How to Configure the Re-rank Model?

Dify currently supports the Cohere Rerank model. You can enter the "Model Providers -> Cohere" page and fill in the API key for the Re-rank model:

<figure><img src="../../../.gitbook/assets/en-rerank-cohere.png" alt=""><figcaption><p>Configure Cohere Rerank Model in Model Providers</p></figcaption></figure>

### How to Obtain the Cohere Rerank Model?

Visit: [https://cohere.com/rerank](https://cohere.com/rerank), register on the page, and apply for the Rerank model usage qualification to obtain the API key.

### Setting the Re-rank Model in Dataset Retrieval Mode

Enter the "Dataset -> Create Dataset -> Retrieval Settings" page to add the Re-rank settings. Besides setting the Re-rank model when creating a dataset, you can also change the Re-rank configuration in the settings of an existing dataset and in the dataset recall mode settings in application orchestration.

<figure><img src="../../../.gitbook/assets/en-rerank-explore.png" alt="" width="563"><figcaption><p>Setting the Re-rank Model in Dataset Retrieval Mode</p></figcaption></figure>

**TopK:** Used to set the number of relevant documents returned after re-ranking.

**Score Threshold:** Used to set the minimum score for relevant documents returned after re-ranking. When the Re-rank model is set, the TopK and Score Threshold settings only take effect in the re-rank step.

### Setting the Re-rank Model in Multi-Path Recall Mode for Datasets

Enter the "Prompt Arrangement -> Context -> Settings" page to enable the Re-rank model when setting to multi-path recall mode.

Explanation about multi-path recall mode: 🔗Please check the section [Multi-path Retrieval](https://docs.dify.ai/guides/knowledge-base/integrate-knowledge-within-application#multi-path-retrieval-recommended)

<figure><img src="../../../.gitbook/assets/en-rerank-setting.png" alt=""><figcaption><p>Setting the Re-rank Model in Multi-Path Recall Mode for Datasets</p></figcaption></figure>
```

## File: en/learn-more/extended-reading/retrieval-augment/retrieval.md
```markdown
# Retrieval Modes

When users build AI applications with multiple knowledge bases, Dify's retrieval strategy will determine which content will be retrieved.

<figure><img src="../../../.gitbook/assets/en-rag-multiple.png" alt=""><figcaption><p>retrieval Mode Settings</p></figcaption></figure>

### Retrieval Setting

Matches all datasets based on user intent, querying related text fragments from multiple datasets simultaneously. After a re-ranking step, the best results matching the user query are selected from the multi-path query results, requiring a configured Rerank model API. In multi-path retrieval mode, the retriever searches for text content related to the user query across all datasets associated with the application merge the relevant document results from multi-path retrieval and re-ranks the retrieved documents semantically using the Rerank model.

In multi-path retrieval mode, it's recommended that the Rerank model be configured.

Below is the technical flowchart for the multi-path retrieval mode:

<figure><img src="../../../.gitbook/assets/rerank-flow-chart.png" alt=""><figcaption><p>Multi-Path retrieval</p></figcaption></figure>

Since multi-path retrieval mode does not rely on the model's inference capability or dataset descriptions, it can achieve higher-quality retrieval results when retrieving across multiple datasets. Additionally, incorporating a re-ranking step can effectively improve document retrieval effectiveness. Therefore, when creating knowledge base Q\&A applications associated with multiple datasets, we recommend configuring the retrieval mode as multi-path retrieval.
```

## File: en/learn-more/extended-reading/how-to-use-json-schema-in-dify.md
```markdown
# How to Use JSON Schema Output in Dify?

JSON Schema is a specification for describing JSON data structures. Developers can define JSON Schema structures to specify that LLM outputs strictly adhere to the defined data or content, such as generating clear document or code structures.

## Models Supporting JSON Schema Functionality

* `gpt-4o-mini-2024-07-18` and later versions
* `gpt-4o-2024-08-06` and later versions

> For more information on the structured output capabilities of OpenAI series models, please refer to [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction).

## Usage of Structured Outputs

1. Connect the LLM to tools, functions, data, and other components within the system. Set `strict: true` in the function definition. When enabled, the Structured Outputs feature ensures that the parameters generated by the LLM for function calls precisely match the JSON schema you provided in the function definition.
2. When the LLM responds to users, it outputs content in a structured format according to the definitions in the JSON Schema.

## Enabling JSON Schema in Dify

Switch the LLM in your application to one of the models supporting JSON Schema output mentioned above. Then, in the settings form, enable `JSON Schema` and fill in the JSON Schema template. Simultaneously, enable the `response_format` column and switch it to the `json_schema` format.

![](../../.gitbook/assets/learn-more-json-schema.png)

The content generated by the LLM supports output in the following format:

* **Text:** Output in text format

## Defining JSON Schema Templates

You can refer to the following JSON Schema format to define your template content:

```json
{
    "name": "template_schema",
    "description": "A generic template for JSON Schema",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "field1": {
                "type": "string",
                "description": "Description of field1"
            },
            "field2": {
                "type": "number",
                "description": "Description of field2"
            },
            "field3": {
                "type": "array",
                "description": "Description of field3",
                "items": {
                    "type": "string"
                }
            },
            "field4": {
                "type": "object",
                "description": "Description of field4",
                "properties": {
                    "subfield1": {
                        "type": "string",
                        "description": "Description of subfield1"
                    }
                },
                "required": ["subfield1"],
                "additionalProperties": false
            }
        },
        "required": ["field1", "field2", "field3", "field4"],
        "additionalProperties": false
    }
}
```

Step-by-step guide:

1. Define basic information:
   * Set `name`: Choose a descriptive name for your schema.
   * Add `description`: Briefly explain the purpose of the schema.
   * Set `strict`: true to ensure strict mode.
2. Create the `schema` object:
   * Set `type: "object"` to specify the root level as an object type.
   * Add a `properties` object to define all fields.
3. Define fields:
   * Create an object for each field, including `type` and `description`.
   * Common types: `string`, `number`, `boolean`, `array`, `object`.
   * For arrays, use `items` to define element types.
   * For objects, recursively define `properties`.
4. Set constraints:
   * Add a `required` array at each level, listing all required fields.
   * Set `additionalProperties: false` at each object level.
5. Handle special fields:
   * Use `enum` to restrict optional values.
   * Use `$ref` to implement recursive structures.

## Example

### 1. Chain of thought（routine）

**JSON Schema Example**

```json
{
    "name": "math_reasoning",
    "description": "Records steps and final answer for mathematical reasoning",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "steps": {
                "type": "array",
                "description": "Array of reasoning steps",
                "items": {
                    "type": "object",
                    "properties": {
                        "explanation": {
                            "type": "string",
                            "description": "Explanation of the reasoning step"
                        },
                        "output": {
                            "type": "string",
                            "description": "Output of the reasoning step"
                        }
                    },
                    "required": ["explanation", "output"],
                    "additionalProperties": false
                }
            },
            "final_answer": {
                "type": "string",
                "description": "The final answer to the mathematical problem"
            }
        },
        "additionalProperties": false,
        "required": ["steps", "final_answer"]
    }
}
```

**Prompts**

```
You are a helpful math tutor. You will be provided with a math problem,
and your goal will be to output a step by step solution, along with a final answer.
For each step, just provide the output as an equation use the explanation field to detail the reasoning.
```

### UI generation（root recursion mode）

**JSON Schema Example**

```json
{
        "name": "ui",
        "description": "Dynamically generated UI",
        "strict": true,
        "schema": {
            "type": "object",
            "properties": {
                "type": {
                    "type": "string",
                    "description": "The type of the UI component",
                    "enum": ["div", "button", "header", "section", "field", "form"]
                },
                "label": {
                    "type": "string",
                    "description": "The label of the UI component, used for buttons or form fields"
                },
                "children": {
                    "type": "array",
                    "description": "Nested UI components",
                    "items": {
                        "$ref": "#"
                    }
                },
                "attributes": {
                    "type": "array",
                    "description": "Arbitrary attributes for the UI component, suitable for any element",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "The name of the attribute, for example onClick or className"
                            },
                            "value": {
                                "type": "string",
                                "description": "The value of the attribute"
                            }
                        },
                      "additionalProperties": false,
                      "required": ["name", "value"]
                    }
                }
            },
            "required": ["type", "label", "children", "attributes"],
            "additionalProperties": false
        }
    }
```

**Prompts**

```
You are a UI generator AI. Convert the user input into a UI.
```

**Example Output:**

![](../../.gitbook/assets/best-practice-json-schema-ui-example.png)

## Tips

* Ensure that the application prompt includes instructions on how to handle cases where user input cannot produce a valid response.
* The model will always attempt to follow the provided schema. If the input content is completely unrelated to the specified schema, it may cause the LLM to generate hallucinations.
* If the LLM detects that the input is incompatible with the task, you can include language in the prompt to specify returning empty parameters or specific sentences.
* All fields must be `required`, For details, please refer to [Supported Schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas).
* [additionalProperties: false](https://platform.openai.com/docs/guides/structured-outputs/additionalproperties-false-must-always-be-set-in-objects) must always be set in objects.
* The root level object of the schema must be an object.

## Appendix

* [Introduction to Structured Outputs](https://cookbook.openai.com/examples/structured\_outputs\_intro)
* [Structured Output](https://platform.openai.com/docs/guides/structured-outputs/json-mode?context=without\_parse)
```

## File: en/learn-more/extended-reading/README.md
```markdown
# Under Maintenance
```

## File: en/learn-more/extended-reading/what-is-llmops.md
```markdown
# What is LLMOps?

**LLMOps (Large Language Model Operations) is a comprehensive set of practices and processes that cover the development, deployment, maintenance, and optimization of large language models (such as the GPT series). The goal of LLMOps is to ensure the efficient, scalable, and secure use of these powerful AI models to build and run real-world applications. It involves aspects such as model training, deployment, monitoring, updating, security, and compliance.**

The table below illustrates the differences in various stages of AI application development before and after using Dify:

| Steps                                                            | Before                                                                                                   | After                                                                      | Save time |
| ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- | --------- |
| Developing Frontend & Backend for Applications | Integrating and encapsulating LLM capabilities requires a lot of time to develop front-end applications. | Directly use Dify' backend services to develop based on a WebApp scaffold. | -80%      |
| Prompt Engineering                                               | Can only be done by calling APIs or Playground.                                                          | Debug based on the user's input data.                                      | -25%      |
| Data Preparation and Embedding                                   | Writing code to implement long text data processing and embedding.                                       | Upload text or bind data sources to the platform.                          | -80%      |
| Application Logging and Analysis                                 | Writing code to record logs and accessing databases to view them.                                        | The platform provides real-time logging and analysis.                      | -70%      |
| Data Analysis and Fine-Tuning                                    | Technical personnel manage data and create fine-tuning queues.                                           | Non-technical personnel can collaborate and adjust the model visually.     | -60%      |
| AI Plugin Development and Integration                            | Writing code to create and integrate AI plugins.                                                         | The platform provides visual tools for creating and integrating plugins.   | -50%      |



Before using an LLMOps platform like Dify, the process of developing applications based on LLMs can be cumbersome and time-consuming. Developers need to handle tasks at each stage on their own, which can lead to inefficiencies, difficulties in scaling, and security issues. Here is the development process before using an LLMOps platform:

1. Data Preparation: Manually collect and preprocess data, which may involve complex data cleaning and annotation work, requiring a significant amount of code.
2. Prompt Engineering: Developers can only write and debug Prompts through API calls or Playgrounds, lacking real-time feedback and visual debugging.
3. Embedding and Context Management: Manually handling the embedding and storage of long contexts, which can be difficult to optimize and scale, requiring a fair amount of programming work and familiarity with model embedding and vector databases.
4. Application Monitoring and Maintenance: Manually collect and analyze performance data, possibly unable to detect and address issues in real-time, and may even lack log records.
5. Model Fine-tuning: Independently manage the fine-tuning data preparation and training process, which can lead to inefficiencies and require more code.
6. System and Operations: Technical personnel involvement or cost required for developing a management backend, increasing development and maintenance costs, and lacking support for collaboration and non-technical users.

With the introduction of an LLMOps platform like Dify, the process of developing applications based on LLMs becomes more efficient, scalable, and secure. Here are the advantages of developing LLM applications using Dify:

1. Data Preparation: The platform provides data collection and preprocessing tools, simplifying data cleaning and annotation tasks, and minimizing or even eliminating coding work.
2. Prompt Engineering: WYSIWYG Prompt editing and debugging, allowing real-time optimization and adjustments based on user input data.
3. Embedding and Context Management: Automatically handling the embedding, storage, and management of long contexts, improving efficiency and scalability without the need for extensive coding.
4. Application Monitoring and Maintenance: Real-time monitoring of performance data, quickly identifying and addressing issues, ensuring the stable operation of applications, and providing complete log records.
5. Model Fine-tuning: The platform offers one-click fine-tuning functionality based on previously annotated real-use data, improving model performance and reducing coding work.
6.  System and Operations: User-friendly interface accessible to non-technical users, supporting collaboration among multiple team members, and reducing development and maintenance costs. Compared to traditional development methods, Dify offers more transparent and easy-to-monitor application management, allowing team members to better understand the application's operation.



    Additionally, Dify will provide AI plugin development and integration features, enabling developers to easily create and deploy LLM-based plugins for various applications, further enhancing development efficiency and application value.

**Dify** is an easy-to-use LLMOps platform designed to empower more people to create sustainable, AI-native applications. With visual orchestration for various application types, Dify offers out-of-the-box, ready-to-use applications that can also serve as Backend-as-a-Service APIs. Unify your development process with one API for plugins and knowledge integration, and streamline your operations using a single interface for prompt engineering, visual analytics, and continuous improvement.
```

## File: en/learn-more/faq/install-faq.md
```markdown
# Self Host / Local Deployment Frequently Asked Questions (FAQs)

### 1. How to reset the password if it is incorrect after local deployment initialization?

If you deployed using Docker Compose, you can reset the password with the following command:

```
docker exec -it docker-api-1 flask reset-password
```

Enter the account email and the new password twice.

### 2. How to fix the "File not found" error in local deployment logs?

```
ERROR:root:Unknown Error in completion
Traceback (most recent call last):
  File "/www/wwwroot/dify/dify/api/libs/rsa.py", line 45, in decrypt
    private_key = storage.load(filepath)
  File "/www/wwwroot/dify/dify/api/extensions/ext_storage.py", line 65, in load
    raise FileNotFoundError("File not found")
FileNotFoundError: File not found
```

This error might be due to changing the deployment method or deleting the `api/storage/privkeys` directory. This file is used to encrypt the large model keys, so its loss is irreversible. You can reset the encryption key pair with the following commands:

* Docker Compose deployment

    ```
    docker exec -it docker-api-1 flask reset-encrypt-key-pair
    ```
* Source code startup

    Navigate to the `api` directory

    ```
    flask reset-encrypt-key-pair
    ```

    Follow the prompts to reset.

### 3. Unable to log in after installation, or receiving a 401 error on subsequent interfaces after a successful login?

This might be due to switching the domain/URL, causing cross-domain issues between the frontend and backend. Cross-domain and identity issues involve the following configurations:

1. CORS Cross-Domain Configuration
   1. `CONSOLE_CORS_ALLOW_ORIGINS`

       Console CORS policy, default is `*`, meaning all domains can access.
   2. `WEB_API_CORS_ALLOW_ORIGINS`

       WebAPP CORS policy, default is `*`, meaning all domains can access.

### 4. The page keeps loading after startup, and requests show CORS errors?

This might be due to switching the domain/URL, causing cross-domain issues between the frontend and backend. Update the following configuration items in `docker-compose.yml` to the new domain:

`CONSOLE_API_URL:` Backend URL for the console API.
`CONSOLE_WEB_URL:` Frontend URL for the console web.
`SERVICE_API_URL:` URL for the service API.
`APP_API_URL:` Backend URL for the WebApp API.
`APP_WEB_URL:` URL for the WebApp.

For more information, please refer to: [Environment Variables](../../getting-started/install-self-hosted/environments.md)

### 5. How to upgrade the version after deployment?

If you started with an image, pull the latest image to complete the upgrade. If you started with source code, pull the latest code and then start it to complete the upgrade.

For source code deployment updates, navigate to the `api` directory and run the following command to migrate the database structure to the latest version:

`flask db upgrade`

### 6. How to configure environment variables when importing using Notion?

[**Notion Integration Configuration Address**](https://www.notion.so/my-integrations). When performing a private deployment, set the following configurations:

1. **`NOTION_INTEGRATION_TYPE`**: This value should be configured as **public/internal**. Since Notion's OAuth redirect address only supports https, use Notion's internal integration for local deployment.
2. **`NOTION_CLIENT_SECRET`**: Notion OAuth client secret (for public integration type).
3. **`NOTION_CLIENT_ID`**: OAuth client ID (for public integration type).
4. **`NOTION_INTERNAL_SECRET`**: Notion internal integration secret. If the value of `NOTION_INTEGRATION_TYPE` is **internal**, configure this variable.

### 7. How to change the name of the space in the local deployment version?

Modify it in the `tenants` table of the database.

### 8. Where to modify the domain for accessing the application?

Find the `APP_WEB_URL` configuration domain in `docker_compose.yaml`.

### 9. What to back up if a database migration occurs?

Back up the database, configured storage, and vector database data. If deployed using Docker Compose, directly back up all data in the `dify/docker/volumes` directory.

### 10. Why can't Docker deployment Dify access the local port using 127.0.0.1 when starting OpenLLM locally?

127.0.0.1 is the internal address of the container. Dify's configured server address needs to be the host's local network IP address.

### 11. How to resolve the size and quantity limit of document uploads in the dataset for the local deployment version?

Refer to the official website [Environment Variables Documentation](https://docs.dify.ai/v/zh-hans/getting-started/install-self-hosted/environments) for configuration.

### 12. How to invite members via email in the local deployment version?

In the local deployment version, invite members via email. After entering the email and sending the invitation, the page will display an invitation link. Copy the invitation link and forward it to the user. The user can open the link, log in via email, set a password, and log in to your space.

### 13. What to do if you encounter the error "Can't load tokenizer for 'gpt2'" in the local deployment version?

```
Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.
```

Refer to the official website [Environment Variables Documentation](https://docs.dify.ai/v/zh-hans/getting-started/install-self-hosted/environments) for configuration, and the related [Issue](https://github.com/langgenius/dify/issues/1261).

### 14. How to resolve a port 80 conflict in the local deployment version?

If port 80 is occupied, stop the service occupying port 80 or modify the port mapping in `docker-compose.yaml` to map port 80 to another port. Typically, Apache and Nginx occupy this port, which can be resolved by stopping these two services.

### 15. What to do if you encounter the error "[openai] Error: ffmpeg is not installed" during text-to-speech?

```
[openai] Error: ffmpeg is not installed
```

Since OpenAI TTS implements audio stream segmentation, ffmpeg needs to be installed for source code deployment to work properly. Detailed steps:

**Windows:**

1. Visit [FFmpeg Official Website](https://ffmpeg.org/download.html) and download the precompiled Windows shared library.
2. Download and extract the FFmpeg folder, which will generate a folder like "ffmpeg-20200715-51db0a4-win64-static".
3. Move the extracted folder to your desired location, e.g., C:\Program Files\.
4. Add the absolute path of the FFmpeg bin directory to the system environment variables.
5. Open Command Prompt and enter "ffmpeg -version". If you see the FFmpeg version information, the installation is successful.

**Ubuntu:**

1. Open Terminal.
2. Enter the following commands to install FFmpeg: `sudo apt-get update`, then `sudo apt-get install ffmpeg`.
3. Enter "ffmpeg -version" to check if the installation is successful.

**CentOS:**

1. First, enable the EPEL repository. Enter in Terminal: `sudo yum install epel-release`
2. Then, enter: `sudo rpm -Uvh http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-5.el7.nux.noarch.rpm`
3. Update yum packages, enter: `sudo yum update`
4. Finally, install FFmpeg, enter: `sudo yum install ffmpeg ffmpeg-devel`
5. Enter "ffmpeg -version" to check if the installation is successful.

**Mac OS X:**

1. Open Terminal.
2. If you haven't installed Homebrew, you can install it by entering the following command in Terminal: `/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"`
3. Use Homebrew to install FFmpeg, enter: `brew install ffmpeg`
4. Enter "ffmpeg -version" to check if the installation is successful.

### 16. How to resolve an Nginx configuration file mount failure during local deployment?

```
Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting "/run/desktop/mnt/host/d/Documents/docker/nginx/nginx.conf" to rootfs at "/etc/nginx/nginx.conf": mount /run/desktop/mnt/host/d/Documents/docker/nginx/nginx.conf:/etc/nginx/nginx.conf (via /proc/self/fd/9), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type
```

Download the complete project, navigate to the docker directory, and execute `docker-compose up -d`.

```
git clone https://github.com/langgenius/dify.git
cd dify/docker
docker compose up -d
```

### 17. Migrate weaviate to another vector database

To migrate from Weaviate to another vector database, follow these steps:

1. For local source code deployment:
   - Update the vector database setting in the `.env` file
   - Example: Set `VECTOR_STORE=qdrant` to migrate to Qdrant

2. For Docker Compose deployment:
   - Update the vector database settings in `docker-compose.yaml`
   - Make sure to modify both API and worker service configurations

```
# The type of vector store to use. Supported values are `weaviate`, `qdrant`, `milvus`, `analyticdb`.
VECTOR_STORE: weaviate
```

3. Execute the below command in your terminal or docker container

```
flask vdb-migrate # or docker exec -it docker-api-1 flask vdb-migrate
```

**Tested target database:**

- qdrant
- milvus
- analyticdb

### 18. Why is SSRF_PROXY needed?

In the community edition's `docker-compose.yaml`, you might notice some services configured with `SSRF_PROXY` and `HTTP_PROXY` environment variables, all pointing to an `ssrf_proxy` container. This is to prevent SSRF attacks. For more information on SSRF attacks, you can read [this article](https://portswigger.net/web-security/ssrf).

To avoid unnecessary risks, we configure a proxy for all services that might cause SSRF attacks and force services like Sandbox to only access external networks through the proxy, ensuring your data and service security. By default, this proxy does not intercept any local requests, but you can customize the proxy behavior by modifying the `squid` configuration file.

#### How to customize the proxy behavior?

In `docker/volumes/ssrf_proxy/squid.conf`, you can find the `squid` configuration file. You can customize the proxy behavior here, such as adding ACL rules to restrict proxy access or adding `http_access` rules to restrict proxy access. For example, your local network can access the `192.168.101.0/24` segment, but `192.168.101.19` has sensitive data that you don't want local deployment Dify users to access, but other IPs can. You can add the following rules in `squid.conf`:

```
acl restricted_ip dst 192.168.101.19
acl localnet src 192.168.101.0/24

http_access deny restricted_ip
http_access allow localnet
http_access deny all
```

This is just a simple example. You can customize the proxy behavior according to your needs. If your business is more complex, such as needing to configure an upstream proxy or cache, you can refer to the [squid configuration documentation](http://www.squid-cache.org/Doc/config/) for more information.

### 19. How to set your created application as a template?

Currently, it is not supported to set your created application as a template. The existing templates are provided by Dify official for cloud version users to refer to. If you are using the cloud version, you can add applications to your workspace or customize them after modification to create your own applications. If you are using the community version and need to create more application templates for your team, you can contact our business team for paid technical support: [business@dify.ai](mailto:business@dify.ai)

### 20. 502 Bad Gateway

This is because Nginx is forwarding the service to the wrong location. First, ensure the container is running, then run the following command with root privileges:

```
docker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
```

Find these two lines in the output:

```
/docker-web-1: 172.19.0.5
/docker-api-1: 172.19.0.7
```

Remember the IP addresses. Then open the directory where you store the Dify source code, open `dify/docker/nginx/conf.d`, replace `http://api:5001` with `http://172.19.0.7:5001`, and replace `http://web:3000` with `http://172.19.0.5:3000`, then restart the Nginx container or reload the configuration.

These IP addresses are _**examples**_, you must execute the command to get your own IP addresses, do not fill them in directly. You might need to reconfigure the IP addresses when restarting the relevant containers.

### 21. How to modify the API service port number?

The API service port is consistent with the one used by the Dify platform. You can reassign the running port by modifying the `nginx` configuration in the `docker-compose.yaml` file.

### 22. How to Migrate from Local to Cloud Storage?

To migrate files from local storage to cloud storage (e.g., Alibaba Cloud OSS), you'll need to transfer data from the 'upload_files' and 'privkeys' folders. Follow these steps:

1. Configure Storage Settings

   For local source code deployment:
   - Update storage settings in `.env` file
   - Set `STORAGE_TYPE=aliyun-oss`
   - Configure Alibaba Cloud OSS credentials

   For Docker Compose deployment:
   - Update storage settings in `docker-compose.yaml`
   - Set `STORAGE_TYPE: aliyun-oss`
   - Configure Alibaba Cloud OSS credentials

2. Execute Migration Commands

   For local source code:
   ```bash
   flask upload-private-key-file-to-cloud-storage
   flask upload-local-files-to-cloud-storage
   ```

   For Docker Compose:
   ```bash
   docker exec -it docker-api-1 flask upload-private-key-file-to-cloud-storage
   docker exec -it docker-api-1 flask upload-local-files-to-cloud-storage
   ```
```

## File: en/learn-more/faq/README.md
```markdown
# Frequently Asked Questions (FAQs)

[Self hosted / local deployment frequently asked questions (FAQs)](https://docs.dify.ai/learn-more/faq/install-faq)

[LLM configuration and usage frequently asked questions (FAQs)](https://docs.dify.ai/learn-more/faq/use-llms-faq)
```

## File: en/learn-more/faq/use-llms-faq.md
```markdown
# LLM Configuration and Usage

### 1. How to access OpenAI via a proxy server in China?

Dify supports custom API domain names for OpenAI and any large model API server compatible with OpenAI. In the community edition, you can fill in the target server address through **Settings --> Model Providers --> OpenAI --> Edit API**.

### 2. How to choose a base model?

* **gpt-3.5-turbo**: gpt-3.5-turbo is an upgraded version of the gpt-3 model series. It is more powerful than gpt-3 and can handle more complex tasks. It has significant improvements in understanding long texts and cross-document reasoning. gpt-3.5-turbo can generate more coherent and persuasive text. It has also greatly improved in summarization, translation, and creative writing. Specializes in: **Long text understanding, cross-document reasoning, summarization, translation, creative writing.**
* **gpt-4**: gpt-4 is the latest and most powerful Transformer language model. It has approximately 20 billion pre-trained parameters, making it top-notch in all language tasks, especially those requiring deep understanding and generation of long, complex responses. gpt-4 can handle all aspects of human language, including understanding abstract concepts and cross-page reasoning. gpt-4 is the first truly universal language understanding system capable of handling any natural language processing task within the AI domain. Specializes in: **All NLP tasks, language understanding, long text generation, cross-document reasoning, abstract concept understanding.** For more details, refer to the [documentation](https://platform.openai.com/docs/models/overview).

### 3. Why is it recommended to set max_tokens smaller?

In natural language processing, longer text outputs usually require more computation time and resources. Therefore, limiting the length of the output text can reduce computational cost and time to some extent. For example, setting max_tokens=500 means only considering the first 500 tokens of the output text, and any part beyond this length will be discarded. This ensures that the output text length does not exceed the LLM's acceptable range and optimizes computational resources, improving model efficiency. Additionally, setting a smaller max_tokens allows for a longer prompt. For instance, gpt-3.5-turbo has a limit of 4097 tokens; if max_tokens=4000, only 97 tokens are left for the prompt, and exceeding this will cause an error.

### 4. How to reasonably split long texts in datasets?

In some natural language processing applications, texts are typically split by paragraphs or sentences to better handle and understand the semantic and structural information in the text. The smallest splitting unit depends on the specific task and technical implementation. For example:

* For text classification tasks, texts are usually split by sentences or paragraphs.
* For machine translation tasks, entire sentences or paragraphs are used as splitting units.

Finally, experiments and evaluations are needed to determine the most suitable embedding technique and splitting unit. You can compare the performance of different techniques and splitting units on the test set and choose the optimal solution.

### 5. What distance function do we use for dataset segmentation?

We use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). The choice of distance function is generally not critical. OpenAI embeddings are normalized to a length of 1, which means:

Using dot product can slightly speed up the calculation of cosine similarity

Cosine similarity and Euclidean distance will result in the same ranking

* If normalized embedding vectors are used to calculate cosine similarity or Euclidean distance and vectors are ranked based on these similarity measures, the ranking results will be the same. This is because, after normalization, the length of the vectors no longer affects their relative relationships; only directional information is retained. Therefore, when using normalized vectors for similarity measurement, different measurement methods will yield the same ranking results. After normalization, all vectors are scaled to a length of 1, meaning they all lie on the unit length. Unit vectors describe only direction without magnitude, as their length is always 1. _For specific principles, you can ask ChatGPT._

When embedding vectors are normalized to a length of 1, calculating the cosine similarity between two vectors can be simplified to their dot product. Since the normalized vector lengths are all 1, the dot product result is equivalent to the cosine similarity result. Given that dot product operations are faster than other similarity measures (like Euclidean distance), using normalized vectors for dot product calculations can slightly improve computational efficiency.

### 6. How to get free trial quotas for Zhipu·AI, iFlytek Spark, and MiniMax models?

We collaborate with major model providers to offer a certain amount of free token trial quotas to Chinese users. Through Dify **Settings --> Model Providers --> Show more model providers**, click "Get Free" on the Zhipu·AI, iFlytek Spark, or MiniMax icons. If you can't see the entrance in the English interface, switch the product language to Chinese:

* **Zhipu·AI: Get 10 million tokens for free.** Click "Get Free", enter your phone number and verification code to receive the quota, regardless of whether you have registered with Zhipu·AI before.
* **iFlytek Spark (V1.5 model, V2.0 model): Get 6 million tokens for free, 3 million tokens for each model, quotas are not interchangeable**. Enter through Dify, complete the registration on iFlytek Spark's open platform (only for phone numbers not previously registered with iFlytek Spark), return to Dify, wait for 5 minutes, and refresh the page to see the available quota.
* **MiniMax: Get 1 million tokens for free.** Click "Get Free" to receive the quota without manual registration, regardless of whether you have registered with MiniMax before.

Once the trial quota is credited, select the model you need to use in **Prompt Arrangement --> Model and Parameters --> Language Model**.

### 7. When filling in the OpenAI key, the validation failed with the error: "Validation failed: You exceeded your current quota, please check your plan and billing details." What is the reason?

This indicates that your OpenAI key's account has run out of funds. Please go to OpenAI to recharge.

### 8. When using OpenAI's key for conversation in the application, I encountered the following errors. What is the reason?

Error one:

```JSON
The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application
```

Error two:

```JSON
Rate limit reached for default-gpt-3.5-turbo in organization org-wDrZCxxxxxxxxxissoZb on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.
```

Please check if you have reached the official API call rate limit. Refer to the [OpenAI official documentation](https://platform.openai.com/docs/guides/rate-limits) for details.

### 9. After user self-deployment, Zhichat is not available, and the error is as follows: "Unrecognized request argument supplied: functions." How to resolve this?

First, check if the front-end and back-end versions are the latest and consistent. Second, this error may occur because you are using an Azure OpenAI key but have not successfully deployed the model. Check if the model is deployed in your Azure OpenAI. The gpt-3.5-turbo model version must be 0613 or above (as versions before 0613 do not support the function call capability used by Zhichat, making it unusable).

### 10. When setting the OpenAI key, the error is as follows. What is the reason?

```JSON
Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7f0462ed7af0>; Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))
```

Usually, this is due to your environment setting a proxy. Please check if a proxy is set.

### 11. When switching models in the application, I encountered the following error. How to resolve this?

```JSON
Anthropic: Error code: 400 - 'error': 'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
```

Each model has different parameter values. Set the parameter values according to the current model's range.

### 12. I encountered the following error. How to resolve this?

```JSON
Query or prefix prompt is too long, you can reduce the prefix prompt, or shrink the max token, or switch to a llm with a larger token limit size
```

In the parameter settings on the orchestration page, reduce the value of "max token."

### 13. What is the default model in Dify, and can open-source models be used?

The default model can be configured in **Settings - Model Providers**. Currently, it supports text generation models from providers like OpenAI / Azure OpenAI / Anthropic, and also supports integration of open-source models hosted on Hugging Face / Replicate / xinference.

### 14. In the community edition, why does the dataset's **Q&A Segmentation Mode** keep showing "queued"?

Check if the API key for the Embedding model you are using has reached the rate limit.

### 15. When users encounter the error "Invalid token" while using the application, how to resolve it?

If you encounter the error "Invalid token," try the following solutions:

* Clear the browser cache (Cookies, Session Storage, and Local Storage). If using a mobile app, clear the corresponding app's cache and re-access it.
* Generate a new App URL and re-enter the URL.

### 16. What are the size limits for uploading dataset documents?

Currently, the maximum size for a single document upload is 15MB, with a total document limit of 100. If you need to adjust these limits for a locally deployed version, refer to [documentation](https://docs.dify.ai/v/zh-hans/getting-started/faq/install-faq#11.-ben-di-bu-shu-ban-ru-he-jie-jue-shu-ju-ji-wen-dang-shang-chuan-de-da-xiao-xian-zhi-he-shu-liang).

### 17. Why does choosing the Claude model still consume OpenAI's quota?

Because Claude does not support the Embedding model, the Embedding process and other dialogue generation by default use OpenAI's key, thus consuming OpenAI's quota. You can also set other default inference models and Embedding models in **Settings - Model Providers**.

### 18. How can I control the use of more contextual data rather than the model's own generation capabilities?

Whether to use the dataset depends on the dataset's description. Make the dataset description as clear as possible. For specific writing techniques, refer to [this documentation](https://docs.dify.ai/v/zh-hans/advanced/datasets).

### 19. When uploading dataset documents in Excel, how to better segment them?

Set the header in the first row, and display content in each subsequent row without additional header settings or complex table formats.

For example, in the table below, only retain the second row's header. The first row (Table 1) is an extra header and should be removed.

### 20. Why can't I use GPT-4 in Dify even though I bought ChatGPT Plus?

OpenAI's GPT-4 model API and ChatGPT Plus are two separate products with separate charges. The model API has its own pricing. Refer to [OpenAI pricing documentation](https://openai.com/pricing). To apply for paid access, you must first bind a card. Binding a card grants GPT-3.5 access but not GPT-4 access. GPT-4 access requires a paid bill. Refer to [OpenAI official documentation](https://platform.openai.com/account/billing/overview) for details.

### 21. How to add other Embedding Models?

Dify supports the following for use as Embedding models. Simply select the `Embeddings` type in the configuration box.

* Azure
* LocalAI
* MiniMax
* OpenAI
* Replicate
* XInference
* GPUStack

### 22. How to set an application I created as an application template?

This feature provides application templates for cloud version users to reference, and currently does not support setting your created applications as templates. If you use the cloud version, you can **Add to Workspace** or **Customize** it to become your own application. If you use the community version and need to create more application templates for your team, you can contact our commercialization team for paid technical support: `business@dify.ai`.
```

## File: en/learn-more/prompt-engineering/prompt-engineering-1/prompt-engineering-template.md
```markdown
# Initial Prompt Template References

To meet the more customized requirements of LLMs for developers, Dify fully opens up the complete prompts in **Expert Mode** and provides initial templates in the orchestration interface. Here are references for four initial templates:

### 1. Template for Building Conversational Applications Using Chat Models

* **SYSTEM**

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answering the user:
- If you don't know, just say that you don't know.
- If you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.
{{pre_prompt}}
```

* **USER**

```
{{Query}} // Input the query variable here
```

* **ASSISTANT**

```Python
"" 
```

#### **Template Structure:**

* Context (`Context`)
* Pre-prompt (`Pre-prompt`)
* Query Variable (`Query`)

### 2. Template for Building Text Generation Applications Using Chat Models

* **SYSTEM**

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answering the user:
- If you don't know, just say that you don't know.
- If you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.
{{pre_prompt}}
```

* **USER**

```
{{Query}} // Input the query variable here, commonly in the form of a paragraph
```

* **ASSISTANT**

```Python
"" 
```

#### **Template Structure:**

* Context (`Context`)
* Pre-prompt (`Pre-prompt`)
* Query Variable (`Query`)

### 3. Template for Building Conversational Applications Using Text Completion Models

```Python
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answering the user:
- If you don't know, just say that you don't know.
- If you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

{{pre_prompt}}

Here are the chat histories between human and assistant, inside <histories></histories> XML tags.

<histories>
{{#histories#}}
</histories>

Human: {{#query#}}

Assistant: 
```

**Template Structure:**

* Context (`Context`)
* Pre-prompt (`Pre-prompt`)
* Conversation History (`History`)
* Query Variable (`Query`)

### 4. Template for Building Text Generation Applications Using Text Completion Models

```Python
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answering the user:
- If you don't know, just say that you don't know.
- If you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

{{pre_prompt}}
{{query}}
```

**Template Structure:**

* Context (`Context`)
* Pre-prompt (`Pre-prompt`)
* Query Variable (`Query`)

{% hint style="warning" %}
Dify and some model vendors have jointly optimized the system prompts deeply. Therefore, the initial templates under some models may differ from the examples above.
{% endhint %}

### Parameter Descriptions

* Context (`Context`): Used to insert relevant text from the dataset as the context of the complete prompt.
* Pre-prompt (`Pre-prompt`): In **Easy Mode**, the pre-prompt orchestrated will be inserted into the complete prompt.
* Conversation History (`History`): When building chat applications using text generation models, the system will insert the user's conversation history as context into the complete prompt. Since some models respond differently to role prefixes, you can also modify the role prefix in the conversation history settings, e.g., changing "Assistant" to "AI".
* Query (`Query`): The query content is the variable value used to insert the user's question in the chat.
```

## File: en/learn-more/prompt-engineering/prompt-engineering-1/README.md
```markdown
# Expert Mode for Prompt Engineering (Discontinued)

When creating an app on Dify, the default orchestration mode is **Simple Mode**, which is ideal for non-technical users who want to quickly create applications like a company knowledge base chatbot or an article summarizer. Using **Simple Mode**, you can orchestrate pre-prompt phrases, add variables, and context with simple steps to publish a complete application (refer to 👉[conversation-application.md](../../../../guides/application\_orchestrate/conversation-application.md "mention")).

However, if you are a technical user proficient in using **OpenAI's** **Playground** and want to create a learning tutor application that requires embedding different contexts and variables into the prompts for various teaching modules, you can choose **Expert Mode**. In this mode, you can freely write complete prompts, including modifying built-in prompts, adjusting the position of context and chat history within the prompts, and setting necessary parameters. If you are familiar with both Chat and Complete models, **Expert Mode** allows you to quickly switch between these models to meet your needs, and both are suitable for conversational and text generation applications.

Before you start experimenting with the new mode, you need to know the essential elements of **Expert Mode**:

* **Text Completion Model** ![](../../../../.gitbook/assets/screenshot-20231017-092613.png)

  When selecting a model, the name with COMPLETE on the right is a text completion model. This model accepts a free-form text string called a "prompt" and generates a text completion that tries to match any context or pattern you give it. For example, if your prompt is: "As Descartes said, I think therefore," it will likely return "I am" as the completion.
* **Chat Model** <img src="../../../../.gitbook/assets/screenshot-20231017-092957.png" alt="" data-size="line">

  When selecting a model, the name with CHAT on the right is a chat model. This model takes a list of messages as input and returns a generated message as output. Although the chat format is designed to simplify multi-turn conversations, it is also useful for single-turn tasks without any conversation. Chat models use chat messages as input and output, including three types of messages: SYSTEM, USER, and ASSISTANT:

  * `SYSTEM`
    * System messages help set the behavior of the AI assistant. For example, you can modify the AI assistant's personality or provide specific instructions on how it should behave throughout the conversation. System messages are optional, and the model's behavior without system messages may be similar to using a generic message like "You are a helpful assistant."
  * `USER`
    * User messages provide requests or comments for the AI assistant to respond to.
  * `ASSISTANT`
    * Assistant messages store previous assistant responses but can also be written by you to provide examples of the desired behavior.
* **Stop Sequences**

  These are specific words, phrases, or characters used to signal the LLM to stop generating text.
* **Content Blocks in Expert Mode Prompts**
  *   <img src="../../../../.gitbook/assets/3.png" alt="" data-size="line">

      In an app configured with a dataset, the user inputs a query, and the app uses this query as a retrieval condition for the dataset. The retrieved results are organized and replace the `context` variable, allowing the LLM to reference the context content to provide an answer.
  *   <img src="../../../../.gitbook/assets/4.png" alt="" data-size="line">

      The query content is only available in text completion models for conversational applications. The content input by the user in the conversation will replace this variable, triggering a new round of dialogue.
  *   <img src="../../../../.gitbook/assets/5.png" alt="" data-size="line">

      Conversation history is only available in text completion models for conversational applications. During multiple conversations in a conversational application, Dify assembles and concatenates the historical conversation records according to built-in rules and replaces the `conversation history` variable. The Human and Assistant prefixes can be modified by clicking the `...` after `conversation history`.
* **Initial Template**

  In **Expert Mode**, before formal orchestration, the prompt box provides an initial template that you can directly modify to make more customized requests to the LLM. Note: There are differences based on the type of application and mode.

  For details, please refer to 👉[prompt-engineering-template.md](prompt-engineering-template.md "mention")

## Comparison of Two Modes

| Comparison Dimension | Simple Mode | Expert Mode |
|----------------------|-------------|-------------|
| Built-in Prompt Visibility | Encapsulated and Invisible | Open and Visible |
| Automatic Orchestration | Available | Unavailable |
| Difference in Text Completion and Chat Model Selection | None | Different orchestration after selecting text completion and chat models |
| Variable Insertion | Available | Available |
| Content Block Validation | None | Available |
| SYSTEM / USER / ASSISTANT Message Type Orchestration | None | Available |
| Context Parameter Settings | Configurable | Configurable |
| View PROMPT LOG | View full prompt log | View full prompt log |
| Stop Sequences Parameter Settings | None | Configurable |

## Operating Instructions

### 1. How to Enter Expert Mode

After creating an application, you can switch to **Expert Mode** on the prompt orchestration page, where you can edit the complete application prompts.

<figure><img src="../../../../.gitbook/assets/专家模式.png" alt=""><figcaption><p>Expert Mode Entry</p></figcaption></figure>

{% hint style="warning" %}
After modifying prompts and publishing the application in **Expert Mode**, you cannot return to **Simple Mode**.
{% endhint %}

### 2. Modify Inserted Context Parameters

In both **Simple Mode** and **Expert Mode**, you can modify the parameters for the inserted context, including **TopK** and **Score Threshold**.

{% hint style="warning" %}
Note that the built-in prompt containing \{{#context#\}} will only be displayed in **Expert Mode** after uploading the context.
{% endhint %}

<figure><img src="../../../../.gitbook/assets/参数设置.png" alt=""><figcaption><p>Context Parameter Settings</p></figcaption></figure>

**TopK: Value range is an integer from 1 to 10**

Used to filter text fragments with the highest similarity to the user's question. The system will dynamically adjust the number of fragments based on the context window size of the selected model. The default value is 2. It is recommended to set this value between 2 and 5, as we expect to get answers that better match the embedded context.

**Score Threshold: Value range is a floating-point number with two decimal places from 0 to 1**

Used to set the similarity threshold for filtering text fragments, i.e., only recalling text fragments that exceed the set score (you can view the hit score of each fragment in the "Hit Test"). The system defaults to this setting being off, meaning it will not filter the recalled text fragments by similarity value. When turned on, the default value is 0.7. It is recommended to keep this setting off by default, but if you require more precise responses, you can set a higher value (the maximum value is 1, but it is not recommended to set it too high).

### 3. Set **Stop Sequences**

We do not want the LLM to generate unnecessary content, so specific words, phrases, or characters (default setting is `Human:`) need to be set to inform the LLM to stop generating text.

For example, if you write a _Few-Shot_ prompt:

```
Human1: What color is the sky?
Assistant1: The sky is blue.
Human1: What color is fire?
Assistant1: Fire is red.
Human1: What color is soil?
Assistant1: 
```

Then in the model parameters' `Stop Sequences`, input `Human1:`, and press the "Tab" key.

This way, the LLM will only respond with one sentence:

```
Assistant1: Soil is yellow.
```

And will not generate additional dialogue (i.e., the LLM will stop generating content before reaching the next "Human1:").

### 4. Quick Insert Variables and Content Blocks

In **Expert Mode**, you can type "`/`" in the text editor to quickly bring up content blocks to insert into the prompt. Content blocks include: `context`, `variable`, `conversation history`, `query content`. You can also type "`{`" to quickly insert a list of previously created variables.

<figure><img src="../../../../.gitbook/assets/快捷键.png" alt=""><figcaption><p>Shortcut Key “/”</p></figcaption></figure>

{% hint style="warning" %}
Content blocks other than "variables" cannot be inserted repeatedly. The available content blocks may vary based on the prompt template structure in different applications and models. `Conversation history` and `query content` are only available in text completion models for conversational applications.
{% endhint %}

### 5. Input Pre-prompt

The initial template of the system's prompt provides necessary parameters and LLM response requirements. For details, see 👉[prompt-engineering-template.md](prompt-engineering-template.md "mention").

The core of early orchestration by developers is the pre-prompt, which needs to be edited and inserted into the built-in prompt. The suggested insertion position is as follows (taking the creation of an "iPhone Consultation Customer Service" as an example):

```
When answering the user:
- If you don't know, just say that you don't know.
- If you don't know or are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

You are a customer service assistant for Apple Inc., and you can provide consultation services for iPhones.
When you answer, you need to list detailed iPhone parameters, and you must output this information as a vertical MARKDOWN table. If the list is too long, transpose it.
You are allowed to think for a long time to generate a more reasonable output.
Note: You currently only have information on some iPhone models, not all of them.
```

Of course, you can also customize the initial template, for example, if you want the LLM's responses to be in English, you can modify the built-in prompt as follows:

```
When answering the user:
- If you don't know, just say that you don't know.
- If you don't know or are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language English.
```

### 6. Debug Logs

During orchestration debugging, you can not only view the user's input and the LLM's response. In **Expert Mode**, click the icon at the top left of the send message button to see the complete prompt, making it easier for developers to confirm whether the input variable content, context, chat history, and query content meet expectations. For a detailed explanation of the log list, please refer to the log documentation 👉: [logs.md](../../../../guides/biao-zhu/logs.md "mention")

#### 6.1 **View Debug Logs**

In the debug preview interface, after a conversation between the user and the AI, move the mouse pointer to any user session, and you will see the "Log" icon button at the top left. Click it to view the prompt log.

<figure><img src="../../../../.gitbook/assets/日志.png" alt=""><figcaption><p>Debug Log Entry</p></figcaption></figure>

In the log, you can clearly see:

* The complete built-in prompt
* Relevant text fragments referenced in the current session
* Historical conversation records

<figure><img src="../../../../.gitbook/assets/11.png" alt=""><figcaption><p>View Prompt Log in Debug Preview Interface</p></figcaption></figure>

From the log, you can see the complete prompt sent to the LLM after system assembly and continuously improve the prompt input based on the debugging results.

#### **6.2 Trace Debug History**

On the main interface for initial app construction, you can see "Logs and Annotations" in the left navigation bar. Click it to view the complete logs. On the main interface of Logs and Annotations, click any conversation log entry, and in the pop-up right dialog box, move the mouse pointer to the conversation to click the "Log" button to view the prompt log.

<figure><img src="../../../../.gitbook/assets/12.png" alt=""><figcaption><p>View Prompt Log in Logs and Annotations Interface</p></figcaption></figure>
```

## File: en/learn-more/prompt-engineering/README.md
```markdown
# Designing Prompts & Orchestrating Applications

Master how to use Dify to orchestrate applications and practice Prompt Engineering. By leveraging two built-in application types, you can build high-value AI applications.

Dify's core philosophy is the declarative definition of AI applications. Everything, including prompts, context, and plugins, can be described through a YAML file (hence the name Dify). The final output is a single API or a ready-to-use WebApp.

At the same time, Dify provides an easy-to-use prompt orchestration interface, allowing developers to visually orchestrate various application features based on prompts. Sounds simple, right?

Whether the AI application is simple or complex, a good prompt can effectively improve the model's output quality, reduce error rates, and meet specific scenario requirements. Dify already offers two common application types: conversational and text generation. This chapter will guide you through orchestrating AI applications in a visual manner.

### Steps for Application Orchestration

1. Determine the application scenario and functional requirements
2. Design and test prompts and model parameters
3. Orchestrate prompts with user input
4. Publish the application
5. Observe and continuously iterate

### Understanding the Differences Between Application Types

Text generation applications and conversational applications in Dify have slight differences in prompt orchestration. Conversational applications need to incorporate the "conversation lifecycle" to meet more complex user scenarios and context management requirements.

Prompt Engineering has evolved into a promising field worth continuous exploration. Continue reading to learn the orchestration guidelines for the two types of applications.

### Further Reading

1. [Learn Prompting](https://learnprompting.org/zh-Hans/)
2. [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
3. [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
```

## File: en/learn-more/use-cases/build-an-notion-ai-assistant.md
```markdown
# Build a Notion AI Assistant

### Intro

Notion is a powerful tool for managing knowledge. Its flexibility and extensibility make it an excellent personal knowledge library and shared workspace. Many people use it to store their knowledge and work in collaboration with others, facilitating the exchange of ideas and the creation of new knowledge.

However, this knowledge remains static, as users must search for the information they need and read through it to find the answers they're seeking. This process is neither particularly efficient nor intelligent.

Have you ever dreamed of having an AI assistant based on your Notion library? This assistant would not only assist you in reviewing your knowledge base, but also engage in the communication like a seasoned butler, even answering other people's questions as if you were the master of your personal Notion library.

### How to Make Your Notion AI Assistant Come True?

Now, you can make this dream come true through [Dify](https://dify.ai/). Dify is an open-source LLMOps (Large Language Models Ops) platform.

Large Language Models like ChatGPT and Claude, have been using their impressive abilities to reshape the world. Their powerful learning aptitude primarily attributable to robust training data. Luckily, they've evolved to be sufficiently intelligent to learn from the content you provide, thus making the process of ideating from your personal Notion library, a reality.

Without Dify, you might need to acquaint yourself with langchain, an abstraction that streamlines the process of assembling these pieces.

### How to Use Dify to Build Your Personal Notion AI Assistant?

The process to train a Notion AI assistant is relatively straightforward. Just follow these steps:

1. Login to Dify.
2. Create a new datasets.
3. Connect with Notion and your datasets.
4. Start training.
5. Create your own AI application.

#### 1. Login to dify

Click [here](https://dify.ai/) to login to Dify. You can conveniently log in using your GitHub or Google account.

> If you are using GitHub account to login, how about getting this [project](https://github.com/langgenius/dify) a star? It really help us a lot!

#### 2. Create new knowledge base

Click the `Knowledge` button on the top side bar, followed by the `Create Knowledge` button.

![login-2](../../.gitbook/assets/login-2.png)

#### 3. Connect with Notion and Your Knowledge[​](https://wsyfin.com/notion-dify#3-connect-with-notion-and-datasets)

Select "Sync from Notion" and then click the "Connect" button..

![connect-with-notion-1](../../.gitbook/assets/connect-with-notion-1.png)

Afterward, you'll be redirected to the Notion login page. Log in with your Notion account.

<figure><img src="../../.gitbook/assets/connect-with-notion-2.png" alt=""><figcaption></figcaption></figure>

Check the permissions needed by Dify, and then click the "Select pages" button.

<figure><img src="../../.gitbook/assets/connect-with-notion-3.png" alt=""><figcaption></figcaption></figure>

Select the pages you want to synchronize with Dify, and press the "Allow access" button.

<figure><img src="../../.gitbook/assets/connect-with-notion-4.png" alt=""><figcaption></figcaption></figure>

#### 4. Start training[​](https://wsyfin.com/notion-dify#4-start-training) <a href="#id-4-start-training" id="id-4-start-training"></a>

Specifying the pages for AI need to study, enabling it to comprehend the content within this section of Notion. Then click the "next" button.

![train-1](../../.gitbook/assets/train-1.png)

We suggest selecting the "Automatic" and "High Quality" options to train your AI assistant. Then click the "Save & Process" button.

![train-2](../../.gitbook/assets/train-2.png)

Enjoy your coffee while waiting for the training process to complete.

![train-3](../../.gitbook/assets/train-3.png)

#### 5. Create Your AI application[​](https://wsyfin.com/notion-dify#5-create-your-ai-application) <a href="#id-5-create-your-own-ai-application" id="id-5-create-your-own-ai-application"></a>

You must create an AI application and link it with the knowledge you've recently created.

Return to the dashboard, and click the "Create new APP" button. It's recommended to use the Chat App directly.

![create-app-1](../../.gitbook/assets/create-app-1.png)

Select the "Prompt Eng." and link your notion datasets in the "context".

![create-app-2](../../.gitbook/assets/create-app-2.png)

I recommend adding a 'Pre Prompt' to your AI application. Just like spells are essential to Harry Potter, similarly, certain tools or features can greatly enhance the ability of AI application.

For example, if your Notion notes focus on problem-solving in software development, could write in one of the prompts:

_I want you to act as an IT Expert in my Notion workspace, using your knowledge of computer science, network infrastructure, Notion notes, and IT security to solve the problems_.

<figure><img src="../../.gitbook/assets/image (40) (2).png" alt=""><figcaption></figcaption></figure>

It's recommended to initially enable the AI to actively furnish the users with a starter sentence, providing a clue as to what they can ask. Furthermore, activating the 'Speech to Text' feature can allow users to interact with your AI assistant using their voice.

<figure><img src="../../.gitbook/assets/notion-speech-to-text.png" alt=""><figcaption></figcaption></figure>

Finally, Click the "Publish" button on the top right of the page. Now you can click the public URL in the "Monitoring" section to converse with your personalized AI assistant!

![create-app-4](../../.gitbook/assets/app-url.png)

### Utilizing API to Integrate With Your Project <a href="#utilizing-api-to-integrate-with-your-project" id="utilizing-api-to-integrate-with-your-project"></a>

Each AI application baked by Dify can be accessed via its API. This method allows developers to tap directly into the robust characteristics of large language models (LLMs) within frontend applications, delivering a true "Backend-as-a-Service" (BaaS) experience.

With effortless API integration, you can conveniently invoke your Notion AI application without the need for intricate configurations.

Click the "API Reference" button on the page of Overview page. You can refer to it as your App's API document.

![using-api-1](../../.gitbook/assets/api-reference.png)

#### 1. Generate API Secret Key[​](https://wsyfin.com/notion-dify#1-generate-api-secret-key) <a href="#id-1-generate-api-secret-key" id="id-1-generate-api-secret-key"></a>

For security reasons, it's recommended to create a new API secret key to access your AI application.

![using-api-2](../../.gitbook/assets/using-api-2.png)

#### 2. Retrieve Conversation ID[​](https://wsyfin.com/notion-dify#2-retrieve-conversation-id) <a href="#id-2-retrieve-conversation-id" id="id-2-retrieve-conversation-id"></a>

After chatting with your AI application, you can retrieve the session ID from the "Logs & Ann." pages.

![using-api-3](../../.gitbook/assets/using-api-3.png)

#### 3. Invoke API[​](https://wsyfin.com/notion-dify#3-invoke-api) <a href="#id-3-invoke-api" id="id-3-invoke-api"></a>

You can run the example request code on the API document to invoke your AI application in terminal.

Remember to replace `YOUR SECRET KEY` and `conversation_id` on your code.

> You can input empty `conversation_id` at the first time, and replace it after you receive response contained `conversation_id`.

```
curl --location --request POST 'https://api.dify.ai/v1/chat-messages' \
--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \
--header 'Content-Type: application/json' \
--data-raw '{
    "inputs": {},
    "query": "eh",
    "response_mode": "streaming",
    "conversation_id": "",
    "user": "abc-123"
}'
```

Sending request in terminal and you will get a successful response.

![using-api-4](../../.gitbook/assets/using-api-4.png)

If you want to continue this chat, go to replace the `conversation_id` of the request code to the `conversation_id` you get from the response.

And you can check all the conversation history on the "Logs & Ann." page.

![using-api-5](../../.gitbook/assets/using-api-5.png)

### Sync with notion periodically[​](https://wsyfin.com/notion-dify#sync-with-notion-periodically) <a href="#sync-with-notion-periodically" id="sync-with-notion-periodically"></a>

If your Notion's pages have updated, you can sync with Dify periodically to keep your AI assistant up-to-date. Your AI assistant will learn from the new content.

![create-app-5](../../.gitbook/assets/create-app-5.png)

### Summary[​](https://wsyfin.com/notion-dify#summary) <a href="#summary" id="summary"></a>

In this tutorial, we have learned not only how to import Your Notion data into Dify, but also know how to use the API to integrate it with your project.

[Dify](https://dify.ai/) is a user-friendly LLMOps platform targeted to empower more individuals to create sustainable, AI-native applications. With visual orchestration designed for various application types, Dify offers ready-to-use applications that can assist you in utilizing data to craft your distinctive AI assistant. Do not hesitate to contact us if you have any inquiries.
```

## File: en/learn-more/use-cases/create-a-midjourney-prompt-bot-with-dify.md
```markdown
# Create a MidJourney Prompt Bot with Dify

via [@op7418](https://twitter.com/op7418) on Twitter

I recently tried out a natural language programming tool called Dify, developed by [@goocarlos](https://twitter.com/goocarlos). It allows someone without coding knowledge to create a web application just by writing prompts. It even generates the API for you, making it easy to deploy your application on your preferred platform.

The application I created using Dify took me only 20 minutes, and the results were impressive. Without Dify, it might have taken me much longer to achieve the same outcome. The specific functionality of the application is to generate Midjourney prompts based on short input topics, assisting users in quickly filling in common Midjourney commands. In this tutorial, I will walk you through the process of creating this application to familiarize you with the platform.

Dify offers two types of applications: conversational applications similar to ChatGPT, which involve multi-turn dialogue, and text generation applications that directly generate text content with the click of a button. Since we want to create a Midjoureny prompt bot, we'll choose the text generator.

You can access Dify here: https://dify.ai/

<figure><img src="../../.gitbook/assets/mj-prompt-bot-with-dify.png" alt=""><figcaption></figcaption></figure>

Once you've created your application, the dashboard page will display some data monitoring and application settings. Click on "Prompt Engineering" on the left, which is the main working page.

<figure><img src="../../.gitbook/assets/screenshot-20230802-114025.png" alt=""><figcaption></figcaption></figure>

On this page, the left side is for prompt settings and other functions, while the right side provides real-time previews and usage of your created content. The prefix prompts are the triggers that the user inputs after each content, and they instruct the GPT model how to process the user's input information.

<figure><img src="../../.gitbook/assets/WechatIMG38.jpg" alt=""><figcaption></figcaption></figure>

Take a look at my prefix prompt structure: the first part instructs GPT to output a description of a photo in the following structure. The second structure serves as the template for generating the prompt, mainly consisting of elements like 'Color photo of the theme,' 'Intricate patterns,' 'Stark contrasts,' 'Environmental description,' 'Camera model,' 'Lens focal length description related to the input content,' 'Composition description relative to the input content,' and 'The names of four master photographers.' This constitutes the main content of the prompt. In theory, you can now save this to the preview area on the right, input the theme you want to generate, and the corresponding prompt will be generated.

<figure><img src="../../.gitbook/assets/pre-prompt.png" alt=""><figcaption></figcaption></figure>

You may have noticed the "\{{proportion\}}" and "\{{version\}}" at the end. These are variables used to pass user-selected information. On the right side, users are required to choose image proportions and model versions, and these two variables help carry that information to the end of the prompt. Let's see how to set them up.

<figure><img src="../../.gitbook/assets/screenshot-20230802-145326.png" alt=""><figcaption></figcaption></figure>

Our goal is to fill in the user's selected information at the end of the prompt, making it easy for users to copy without having to rewrite or memorize these commands. For this, we use the variable function.

Variables allow us to dynamically incorporate the user's form-filled or selected content into the prompt. For example, I've created two variables: one represents the image proportion, and the other represents the model version. Click the "Add" button to create the variables.

<figure><img src="../../.gitbook/assets/WechatIMG157.jpg" alt=""><figcaption></figcaption></figure>

After creation, you'll need to fill in the variable key and field name. The variable key should be in English. The optional setting means the field will be non-mandatory when the user fills it. Next, click "Settings" in the action bar to set the variable content.

<figure><img src="../../.gitbook/assets/WechatIMG158.jpg" alt=""><figcaption></figcaption></figure>

Variables can be of two types: text variables, where users manually input content, and select options where users select from given choices. Since we want to avoid manual commands, we'll choose the dropdown option and add the required choices.

<figure><img src="../../.gitbook/assets/app-variables.png" alt=""><figcaption></figcaption></figure>

Now, let's use the variables. We need to enclose the variable key within double curly brackets {} and add it to the prefix prompt. Since we want the GPT to output the user-selected content as is, we'll include the phrase "Producing the following English photo description based on user input" in the prompt.

<figure><img src="../../.gitbook/assets/WechatIMG160.jpg" alt=""><figcaption></figcaption></figure>

However, there's still a chance that GPT might modify our variable content. To address this, we can lower the diversity in the model selection on the right, reducing the temperature and making it less likely to alter our variable content. You can check the tooltips for other parameters' meanings.

<figure><img src="../../.gitbook/assets/screenshot-20230802-141913.png" alt=""><figcaption></figcaption></figure>

With these steps, your application is now complete. After testing and ensuring there are no issues with the output, click the "Publish" button in the upper right corner to release your application. You and users can access your application through the publicly available URL. You can also customize the application name, introduction, icon, and other details in the settings.

<figure><img src="../../.gitbook/assets/screenshot-20230802-142407.png" alt=""><figcaption></figcaption></figure>

That's how you create a simple AI application using Dify. You can also deploy your application on other platforms or modify its UI using the generated API. Additionally, Dify supports uploading your own data, such as building a customer service bot to assist with product-related queries. This concludes the tutorial, and a special thanks to @goocarlos for creating such a fantastic product.
```

## File: en/learn-more/use-cases/create-an-ai-chatbot-with-business-data-in-minutes.md
```markdown
# Create an AI Chatbot with Business Data in Minutes

AI-powered customer service may be a standard feature for every business website, and it is becoming easier to implement with higher levels of customization. The following content will guide you on how to create an AI-powered customer service for your website in just a few minutes using Dify.

### Prerequisite

**Register or Deploy Dify.AI**

Dify is an open source product which you can find on[ GitHub](https://github.com/langgenius/dify) and deploy it to your local or company intranet. Meanwhile, it provides a cloud SaaS version, access [Didy.AI ](https://dify.ai/)to register and use it.

**Apply for API key from OpenAI and other model providers.**

Dify provides free message call usage quotas for OpenAI GPT series (200 times) and Antropic Claude (1000 times) AI models, which require tokens to be consumed. Before you run out, you need to apply for your own API key through the official channel of the model provider. You can enter the key in Dify's "Settings" - "Model Provider".

### Upload your product documentation or knowledge base.

If you want to build an AI Chatbot based on the company's existing knowledge base and product documents, then you need to upload as many product-related documents as possible to Dify's knowledge. Dify helps you **complete segmentation and cleaning of the data.** The Dify knowledge supports two indexing modes: high quality and economical. We recommend using the high quality mode, which consumes tokens but provides higher accuracy.

1. Create a new knowledge base
2. upload your business data (support batch uploading multiple texts)
3. select the cleaning method
4. Click \[Save and Process], and it will take only a few seconds to complete the processing.

![](../../.gitbook/assets/ai-chatbot-knowledge-base.png)

### Create an AI application and give it instructions

Create a conversational app on the \[Build App] page. Then start setting up the prompt and its front-end user experience interactions.

1. Give the AI instruction: Click on the "Pre Prompt" on the left to edit your Prompt, so that it can play the role of customer service and communicate with users. You can specify its tone, style, and limit it to answer or not answer certain questions.
2. Let AI possess your business knowledge: add the target knowledge you just uploaded in the \[context].
3. Set up the opening remarks: click "Add Feature" to turn on the feature. The purpose is to add an opening line for AI applications, so that when the user opens the customer service window, it will greet the user first and increase affinity.
4. Set up the "Next Question Suggestion": turn on this feature to "Add Feature". The purpose is to give users a direction for their next question after they have asked one.
5. Choose a suitable model and adjust the parameters: different models can be selected in the upper right corner of the page. The performance and token price consumed by different models are different. In this example, we use the GPT3.5 model.

In this case, we assign a role to the AI:

> Pre prompt：You are Bob, the AI customer service for Dify, specializing in answering questions about Dify's products, team, or LLMOps for users.Please note, refuse to answer when users ask "inappropriate questions", i.e., content beyond the scope of this document.

> Opening remarks：Hey \{{User\_name\}}, I'm Bob☀️, the first AI member of Dify. You can discuss with me any questions related to Dify products, team, and even LLMOps.

<figure><img src="../../.gitbook/assets/app-pre-prompt-opening-remarks.png" alt=""><figcaption></figcaption></figure>

### Debug the performance of AI Chatbot and publish.

After completing the setup, you can send messages to it on the right side of the current page to debug whether its performance meets expectations. Then click "Publish". And then you get an AI chatbot.

<figure><img src="../../.gitbook/assets/debug-ai-chatbot-publish.png" alt=""><figcaption></figcaption></figure>

### Embed AI Chatbot application into your front-end page.

This step is to embed the prepared AI chatbot into your official website . Click \[Overview] -> \[Embedded], select the script tag method, and copy the script code into the \<head> or \<body> tag of your website. If you are not a technical person, you can ask the developer responsible for the official website to paste and update the page.

<figure><img src="../../.gitbook/assets/ai-chatbot-embedded.png" alt=""><figcaption></figcaption></figure>

1. Paste the copied code into the target location on your website.
2. Update your official website and you can get an AI intelligent customer service with your business data. Try it out to see the effect.

Above is an example of how to embed Dify into the official website through the AI chatbot Bob of Dify official website. Of course, you can also use more features provided by Dify to enhance the performance of the chatbot, such as adding some variable settings, so that users can fill in necessary judgment information before interaction, such as name, specific product used and so on.

Welcome to explore in Dify together!
```

## File: en/learn-more/use-cases/how-to-connect-aws-bedrock.md
```markdown
# How to connect with AWS Bedrock Knowledge Base？

This article will briefly introduce how to connect the Dify platform with the AWS Bedrock knowledge base through the [external knowledge base API](https://docs.dify.ai/guides/knowledge-base/external-knowledge-api-documentation), so that AI applications in the Dify platform can directly obtain content stored in the AWS Bedrock knowledge base and expand new information source channels.

### Pre-preparation

* AWS Bedrock Knowledge Base
* Dify SaaS Service / Dify Community Version
* Backend API Development Basics

### 1. Register and Create AWS Bedrock Knowledge Base

Visit [AWS Bedrock](https://aws.amazon.com/bedrock/) and create the Knowledge Base service.

<figure><img src="../../../en/.gitbook/assets/image (360).png" alt=""><figcaption><p>Create AWS Bedrock Knowledge Base</p></figcaption></figure>

### 2. Build the Backend API Service

The Dify platform cannot directly connect to AWS Bedrock Knowledge Base. The developer needs to refer to Dify's [API definition](../../guides/knowledge-base/external-knowledge-api-documentation.md) on external knowledge base connection, manually create the backend API service, and establish a connection with AWS Bedrock. Please refer to the specific architecture diagram:

<figure><img src="../../../zh_CN/.gitbook/assets/image (1).png" alt=""><figcaption><p>Build the backend API service</p></figcaption></figure>

You can refer to the following 2 demo code.

`knowledge.py`

```python
from flask import request
from flask_restful import Resource, reqparse

from bedrock.knowledge_service import ExternalDatasetService


class BedrockRetrievalApi(Resource):
    # url : <your-endpoint>/retrieval
    def post(self):
        parser = reqparse.RequestParser()
        parser.add_argument("retrieval_setting", nullable=False, required=True, type=dict, location="json")
        parser.add_argument("query", nullable=False, required=True, type=str,)
        parser.add_argument("knowledge_id", nullable=False, required=True, type=str)
        args = parser.parse_args()

        # Authorization check
        auth_header = request.headers.get("Authorization")
        if " " not in auth_header:
            return {
                "error_code": 1001,
                "error_msg": "Invalid Authorization header format. Expected 'Bearer <api-key>' format."
            }, 403
        auth_scheme, auth_token = auth_header.split(None, 1)
        auth_scheme = auth_scheme.lower()
        if auth_scheme != "bearer":
            return {
                "error_code": 1001,
                "error_msg": "Invalid Authorization header format. Expected 'Bearer <api-key>' format."
            }, 403
        if auth_token:
            # process your authorization logic here
            pass

        # Call the knowledge retrieval service
        result = ExternalDatasetService.knowledge_retrieval(
            args["retrieval_setting"], args["query"], args["knowledge_id"]
        )
        return result, 200
```

`knowledge_service.py`

```python
import boto3


class ExternalDatasetService:
    @staticmethod
    def knowledge_retrieval(retrieval_setting: dict, query: str, knowledge_id: str):
        # get bedrock client
        client = boto3.client(
            "bedrock-agent-runtime",
            aws_secret_access_key="AWS_SECRET_ACCESS_KEY",
            aws_access_key_id="AWS_ACCESS_KEY_ID",
            # example: us-east-1
            region_name="AWS_REGION_NAME",
        )
        # fetch external knowledge retrieval
        response = client.retrieve(
            knowledgeBaseId=knowledge_id,
            retrievalConfiguration={
                "vectorSearchConfiguration": {"numberOfResults": retrieval_setting.get("top_k"), "overrideSearchType": "HYBRID"}
            },
            retrievalQuery={"text": query},
        )
        # parse response
        results = []
        if response.get("ResponseMetadata") and response.get("ResponseMetadata").get("HTTPStatusCode") == 200:
            if response.get("retrievalResults"):
                retrieval_results = response.get("retrievalResults")
                for retrieval_result in retrieval_results:
                    # filter out results with score less than threshold
                    if retrieval_result.get("score") < retrieval_setting.get("score_threshold", .0):
                        continue
                    result = {
                        "metadata": retrieval_result.get("metadata"),
                        "score": retrieval_result.get("score"),
                        "title": retrieval_result.get("metadata").get("x-amz-bedrock-kb-source-uri"),
                        "content": retrieval_result.get("content").get("text"),
                    }
                    results.append(result)
        return {
            "records": results
        }
```

During the process, you can construct the API endpoint address and the API Key for authentication and use them for the next connections.

### 3. Get the AWS Bedrock Knowledge Base ID

After log in to the AWS Bedrock Knowledge backend and get the ID of the created Knowledge Base, you can use this parameter to connect to the Dify platform in the subsequent steps.

<figure><img src="../../../zh_CN/.gitbook/assets/image (359).png" alt=""><figcaption><p>Get the AWS Bedrock Knowledge Base ID</p></figcaption></figure>

### 4. Associate the External Knowledge API

Go to the **"Knowledge"** page in the Dify platform, click **"External Knowledge API"** in the upper right corner, and tap **"Add an External Knowledge API"**.

Follow the prompts on the page and fill in the following information:

* The name of the knowledge base. Custom names are allowed to distinguish different external knowledge APIs connected to the Dify platform;
* API endpoint address, the connection address of the external knowledge base, which can be customized in [Step 2](how-to-connect-aws-bedrock.md#id-2.build-the-backend-api-service). Example: `api-endpoint/retrieval`;
* API Key, the external knowledge base connection key, which can be customized in [Step 2](how-to-connect-aws-bedrock.md#id-2.build-the-backend-api-service).

<figure><img src="../../../zh_CN/.gitbook/assets/image (362).png" alt=""><figcaption></figcaption></figure>

### 5. Connect to External Knowledge Base

Go to the **“Knowledge** page, click **“Connect to an External Knowledge Base”** below the add knowledge base card to jump to the parameter configuration page.

<figure><img src="../../../zh_CN/.gitbook/assets/image (363).png" alt=""><figcaption></figcaption></figure>

Fill in the following parameters:

* **Knowledge base name and description**
* **External knowledge base API**

Select the external knowledge base API associated in Step 4.
* **External knowledge base ID**

Fill in the AWS Bedrock knowledge base ID obtained in Step 3.
* **Adjust recall settings**

**Top K:** When a user asks a question, the external knowledge API will be requested to obtain the most relevant content chunks. This parameter is used to filter text chunks with high similarity to user questions. The default value is 3. The higher the value, the more relevant text chunks will be recalled.

**Score threshold:** The similarity threshold for text chunk filtering. Only text chunks with a score exceeding the set score will be recalled. The default value is 0.5. The higher the value, the higher the similarity required between the text and the question, the smaller the number of texts expected to be recalled, and the more accurate the result will be.

<figure><img src="../../../zh_CN/.gitbook/assets/image (364).png" alt=""><figcaption></figcaption></figure>

After the settings are completed, you can establish a connection with the external knowledge base API.

### 6. Test External Knowledge Base Connection and Retrieval

After establishing a connection with an external knowledge base, developers can simulate possible user's question keywords in **"Retrieval Test"** and preview the text chunks retrieval from the AWS Bedrock Knowledge Base.

<figure><img src="../../../zh_CN/.gitbook/assets/image (366).png" alt=""><figcaption><p>Test the connection and retrieval of the external knowledge base</p></figcaption></figure>

If you are not satisfied with the retrieval results, you can try to modify the retrieval parameters or adjust the retrieval settings of AWS Bedrock Knowledge Base.

<figure><img src="../../../zh_CN/.gitbook/assets/image (367).png" alt=""><figcaption><p>Adjust the text chunking parameters of AWS Bedrock Knowledge Base</p></figcaption></figure>
```

## File: en/learn-more/use-cases/how-to-creat-dify-schedule.md
```markdown
# ⏰ Dify Schedule Helper

> Author:[Leo_chen](https://github.com/leochen-g) 

> [X(Twitter)](https://x.com/leochen_code)

Tired of manually running your Dify workflows? Let's add some automation magic! 

✨ The Dify Schedule Helper brings you the power of scheduled tasks - a feature not available in the official Dify platform. Using GitHub Actions, you can now schedule your workflows to run automatically and get instant notifications about their execution.

## 🎯 What Can It Do?

- 🔄 Automatically execute multiple Dify workflows on schedule
- ⏰ Run tasks at your preferred time (default: 06:30 Beijing Time)
- 📱 Send notifications through various channels
- 🆓 Completely free to use
- 🔒 Secure and reliable with GitHub Actions

## 🚀 Getting Started

You can use this automation in two ways:
- 🌐 Quick Start (Cloud-based)

### 🌐 Quick Start

Let's get your automated workflows up and running in just a few steps!

1. 🍴 First, [Fork the repository](https://github.com/leochen-g/dify-schedule)

2. ⚙️ Set up your secrets:
   Navigate to: Repository Settings -> Secrets -> New repository secret

   | Secret Name | What to Put | Required? |
   |------------|-------------|-----------|
   | DIFY_BASE_URL | Your Dify API URL (default: https://api.dify.ai/v1) | No |
   | DIFY_TOKENS | Your Dify workflow API keys (use `;` to separate multiple keys) | Yes |
   | DIFY_INPUTS | JSON format workflow variables (if required by your Dify setup) | No |
   
   ### 📱 Notification Settings (Optional but recommended!)
   
   | Secret Name | What to Put | For |
   |------------|-------------|-----|
   | EMAIL_USER | SMTP-enabled email address | Email notifications |
   | EMAIL_PASS | SMTP password | Email notifications |
   | EMAIL_TO | Recipient email(s) (use `, ` for multiple) | Email notifications |
   | PUSHPLUS_TOKEN | [Pushplus](http://www.pushplus.plus/) token | WeChat notifications |
   | SERVERPUSHKEY | [Server push](https://sct.ftqq.com/) key | WeChat notifications |
   | DINGDING_WEBHOOK | DingTalk robot webhook | DingTalk notifications |
   | WEIXIN_WEBHOOK | WeCom robot webhook | WeCom notifications |
   | FEISHU_WEBHOOK | Feishu robot webhook | Feishu notifications |
   | AIBOTK_KEY | [Wechat Assistant](https://wechat.aibotk.com?r=dBL0Bn&f=difySchedule) apikey | WeChat notifications |
   | AIBOTK_ROOM_RECIVER | Group chat name for Wechat Assistant | WeChat group notifications |
   | AIBOTK_CONTACT_RECIVER | Contact name for Wechat Assistant | WeChat private notifications |

3. ▶️ Enable the workflow:
   Go to Actions tab and enable the workflows

## 📸 Preview

| WeChat Notification | Email Notification |
|:------------------:|:------------------:|
| ![WeChat](../../.gitbook/assets/schedule-chat.png) | ![Email](../../.gitbook/assets/schedule-chat2.png) |

## ❓ FAQ

### 🔑 How to get Dify workflow tokens?

1. Open your Dify dashboard
2. Navigate to your workflow application
3. Go to API Reference
4. Copy your API Key

> 💡 Note: Only workflow applications are supported!

![Token Step 1](../../.gitbook/assets/schedule-dify1.png)
![Token Step 2](../../.gitbook/assets/schedule-dify2.png)

### 🚫 Connection Issues?

If you're using a self-hosted Dify instance, make sure it's accessible from the public internet - GitHub Actions needs to reach your server!

### ❌ Execution Errors?

1. 🔍 Check if your application is a workflow application
2. ⚙️ If your workflow requires input variables, configure `DIFY_INPUTS` with valid JSON
3. 📝 Read the error logs carefully and ensure all required variables are set correctly

Need more help? Feel free to open an issue with your logs (remember to remove sensitive information)!
```

## File: en/learn-more/use-cases/how-to-integrate-dify-chatbot-to-your-wix-website.md
```markdown
# Integrating Dify Chatbot into Your Wix Website

Wix, a popular website creation platform, allows users to visually design their websites through drag-and-drop functionality. By leveraging Wix's iframe code feature, you can seamlessly integrate a Dify chatbot into your Wix site.

This functionality extends beyond chatbot integration, enabling you to display content from external servers and other sources within your Wix pages. Examples include weather widgets, stock tickers, calendars, or any custom web elements.

This guide will walk you through the process of embedding a Dify chatbot into your Wix website using iframe code. The same method can be applied to integrate Dify applications into other websites, blogs, or web pages.

## 1. Obtaining the Dify Application iFrame Code Snippet

Assuming you've already created a [Dify AI application](https://docs.dify.ai/guides/application-orchestrate/creating-an-application), follow these steps to acquire the iFrame code snippet:

1. Log into your Dify account
2. Select the Dify application you wish to embed
3. Click the "Publish" button in the upper right corner
4.  On the publish page, choose the "Embed Into Site" option

    ![Embed Into Site Option](../../.gitbook/assets/best-practice-wix-2.png)
5.  Select an appropriate style and copy the displayed iFrame code. For example:

    ![iFrame Code Example](../../.gitbook/assets/best-practice-wix-3.png)

## 2. Embedding the iFrame Code Snippet in Your Wix Site

1. Log into your Wix website and open the page you want to edit
2. Click the blue `+` (Add Elements) button on the left side of the page
3.  Select **Embed Code**, then click **Embed HTML** to add an HTML iFrame element to the page

    ![Add HTML iFrame](../../.gitbook/assets/best-practice-add-html-iframe.png)
4. In the `HTML Settings` box, select the `Code` option
5. Paste the iFrame code snippet you obtained from your Dify application
6. Click the **Update** button to save and preview your changes

Here's an example of an iFrame code snippet for embedding a Dify Chatbot:

```bash
<iframe src="https://udify.app/chatbot/ez1pf83HVV3JgWO4" style="width: 100%; height: 100%; min-height: 700px" frameborder="0" allow="microphone"></iframe>
```

![Insert Dify iFrame Code](../../.gitbook/assets/best-practice-insert-dify-iframe-code.png)

> ⚠️ Ensure the address in the iFrame code begins with HTTPS. HTTP addresses will not display correctly.

## 3. Customizing Your Dify Chatbot

You can adjust the Dify Chatbot's button style, position, and other settings.

### 3.1 Customizing Style

Modify the `style` attribute in the iFrame code to customize the Chatbot button's appearance. For example:

```bash
<iframe src="https://udify.app/chatbot/ez1pf83HVV3JgWO4" style="width: 100%; height: 100%; min-height: 700px" frameborder="0" allow="microphone"></iframe>

# Add a 2-pixel wide solid black border: border: 2px solid #000

→

<iframe src="https://udify.app/chatbot/ez1pf83HVV3JgWO4" style="width: 80%; height: 80%; min-height: 500px; border: 2px solid #000;" frameborder="0" allow="microphone"></iframe>
```

This code adds a 2-pixel wide solid black border to the chatbot interface.

### 3.2 Customizing Position

Adjust the button's position by modifying the `position` value in the `style` attribute. For example:

```bash
<iframe src="https://udify.app/chatbot/ez1pf83HVV3JgWO4" style="width: 100%; height: 100%; min-height: 700px" frameborder="0" allow="microphone"></iframe>

# Fix the Chatbot to the bottom right corner of the webpage, 20 pixels from the bottom and right edges.

→

<iframe src="https://udify.app/chatbot/ez1pf83HVV3JgWO4" style="width: 100%; height: 100%; min-height: 700px; position: fixed; bottom: 20px; right: 20px;" frameborder="0" allow="microphone"></iframe>
```

This code fixes the Chatbot to the bottom right corner of the webpage, 20 pixels from the bottom and right edges.

## FAQ

**1. iFrame Content Not Displaying**

* Verify that the URL starts with HTTPS
* Check for typos in the `iframe` code
* Verify the embedded content complies with Wix's security policies

**2. iFrame Content is Cropped**

Modify the `width` and `height` percentage values in the `iframe` code to resolve content truncation issues.
```

## File: en/learn-more/use-cases/README.md
```markdown
# Use Cases
```

## File: en/learn-more/how-to-use-json-schema-in-dify.md
```markdown
# How to Use JSON Schema Output in Dify

JSON Schema is a specification for describing JSON data structures. Developers can define JSON Schema structures to specify that LLM outputs strictly adhere to the defined data or content, such as generating clear document or code structures.

## Models Supporting JSON Schema Functionality

- `gpt-4o-mini-2024-07-18` and later versions
- `gpt-4o-2024-08-06` and later versions

> For more information on the structured output capabilities of OpenAI series models, please refer to [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction).

## Usage of Structured Outputs

1. Connect the LLM to tools, functions, data, and other components within the system. Set `strict: true` in the function definition. When enabled, the Structured Outputs feature ensures that the parameters generated by the LLM for function calls precisely match the JSON schema you provided in the function definition.

2. When the LLM responds to users, it outputs content in a structured format according to the definitions in the JSON Schema.

## Enabling JSON Schema in Dify

Switch the LLM in your application to one of the models supporting JSON Schema output mentioned above. Then, in the settings form, enable `JSON Schema` and fill in the JSON Schema template. Simultaneously, enable the `response_format` column and switch it to the `json_schema` format.

![](../../../img/learn-more-json-schema.png)

The content generated by the LLM supports output in the following format:

- **Text:** Output in text format

## Defining JSON Schema Templates

You can refer to the following JSON Schema format to define your template content:

```json
{
    "name": "template_schema",
    "description": "A generic template for JSON Schema",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "field1": {
                "type": "string",
                "description": "Description of field1"
            },
            "field2": {
                "type": "number",
                "description": "Description of field2"
            },
            "field3": {
                "type": "array",
                "description": "Description of field3",
                "items": {
                    "type": "string"
                }
            },
            "field4": {
                "type": "object",
                "description": "Description of field4",
                "properties": {
                    "subfield1": {
                        "type": "string",
                        "description": "Description of subfield1"
                    }
                },
                "required": ["subfield1"],
                "additionalProperties": false
            }
        },
        "required": ["field1", "field2", "field3", "field4"],
        "additionalProperties": false
    }
}
```


Step-by-step guide:

1. Define basic information:
   - Set `name`: Choose a descriptive name for your schema.
   - Add `description`: Briefly explain the purpose of the schema.
   - Set `strict`: true to ensure strict mode.

2. Create the `schema` object:
   - Set `type: "object"` to specify the root level as an object type.
   - Add a `properties` object to define all fields.

3. Define fields:
   - Create an object for each field, including `type` and `description`.
   - Common types: `string`, `number`, `boolean`, `array`, `object`.
   - For arrays, use `items` to define element types.
   - For objects, recursively define `properties`.

4. Set constraints:
   - Add a `required` array at each level, listing all required fields.
   - Set `additionalProperties: false` at each object level.

5. Handle special fields:
   - Use `enum` to restrict optional values.
   - Use `$ref` to implement recursive structures.

## Example

### 1. Chain of thought（routine）

**JSON Schema Example**

```json
{
    "name": "math_reasoning",
    "description": "Records steps and final answer for mathematical reasoning",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "steps": {
                "type": "array",
                "description": "Array of reasoning steps",
                "items": {
                    "type": "object",
                    "properties": {
                        "explanation": {
                            "type": "string",
                            "description": "Explanation of the reasoning step"
                        },
                        "output": {
                            "type": "string",
                            "description": "Output of the reasoning step"
                        }
                    },
                    "required": ["explanation", "output"],
                    "additionalProperties": false
                }
            },
            "final_answer": {
                "type": "string",
                "description": "The final answer to the mathematical problem"
            }
        },
        "additionalProperties": false,
        "required": ["steps", "final_answer"]
    }
}
```

**Prompts**

```text
You are a helpful math tutor. You will be provided with a math problem,
and your goal will be to output a step by step solution, along with a final answer.
For each step, just provide the output as an equation use the explanation field to detail the reasoning.
```

### UI generation（root recursion mode）

**JSON Schema Example**

```json
{
        "name": "ui",
        "description": "Dynamically generated UI",
        "strict": true,
        "schema": {
            "type": "object",
            "properties": {
                "type": {
                    "type": "string",
                    "description": "The type of the UI component",
                    "enum": ["div", "button", "header", "section", "field", "form"]
                },
                "label": {
                    "type": "string",
                    "description": "The label of the UI component, used for buttons or form fields"
                },
                "children": {
                    "type": "array",
                    "description": "Nested UI components",
                    "items": {
                        "$ref": "#"
                    }
                },
                "attributes": {
                    "type": "array",
                    "description": "Arbitrary attributes for the UI component, suitable for any element",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {
                                "type": "string",
                                "description": "The name of the attribute, for example onClick or className"
                            },
                            "value": {
                                "type": "string",
                                "description": "The value of the attribute"
                            }
                        },
                      "additionalProperties": false,
                      "required": ["name", "value"]
                    }
                }
            },
            "required": ["type", "label", "children", "attributes"],
            "additionalProperties": false
        }
    }
```

**Prompts**

```text
You are a UI generator AI. Convert the user input into a UI.
```

**Example Output:**

![](../../img/best-practice-json-schema-ui-example.png)

## Tips

- Ensure that the application prompt includes instructions on how to handle cases where user input cannot produce a valid response.

- The model will always attempt to follow the provided schema. If the input content is completely unrelated to the specified schema, it may cause the LLM to generate hallucinations.

- If the LLM detects that the input is incompatible with the task, you can include language in the prompt to specify returning empty parameters or specific sentences.

- All fields must be `required`, For details, please refer to [Supported Schemas](https://platform.openai.com/docs/guides/structured-outputs/supported-schemas).

- [additionalProperties: false](https://platform.openai.com/docs/guides/structured-outputs/additionalproperties-false-must-always-be-set-in-objects) must always be set in objects.

- The root level object of the schema must be an object.

## Appendix

- [Introduction to Structured Outputs](https://cookbook.openai.com/examples/structured_outputs_intro)

- [Structured Output](https://platform.openai.com/docs/guides/structured-outputs/json-mode?context=without_parse)
```
