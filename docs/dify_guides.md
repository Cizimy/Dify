This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-20T13:46:01.965Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
en/
  guides/
    annotation/
      annotation-reply.md
      logs.md
      README.md
    application-orchestrate/
      app-toolkits/
        moderation-tool.md
        README.md
      prompt-engineering/
        prompt-engineering-expert-mode.md
        prompt-template.md
        README.md
      agent.md
      conversation-application.md
      creating-an-application.md
      llms-use-faq.md
      overview.md
      README.md
      text-generation-application.md
    application-publishing/
      launch-your-webapp-quickly/
        conversation-application.md
        README.md
        text-generator.md
        web-app-settings.md
      based-on-frontend-templates.md
      developing-with-apis.md
      embedding-in-websites.md
      README.md
    extension/
      api-based-extension/
        cloudflare-workers.md
        external-data-tool.md
        moderation-extension.md
        moderation.md
        README.md
      code-based-extension/
        external-data-tool.md
        moderation.md
        README.md
      README.md
    knowledge-base/
      create-knowledge-and-upload-documents/
        1.-import-text-data/
          1.1-import-data-from-notion.md
          README.md
        2.-choose-a-chunk-mode.md
        3.-select-the-indexing-method-and-retrieval-setting.md
      knowledge-and-documents-maintenance/
        maintain-documents.md
      connect-external-knowledge.md
      create-knowledge-and-upload-documents.md
      external-knowledge-api-documentation.md
      integrate-knowledge-within-application.md
      knowledge-and-documents-maintenance.md
      maintain-dataset-via-api.md
      README.md
      retrieval-test-and-citation.md
      sync-from-website.md
    management/
      app-management.md
      personal-account-management.md
      README.md
      subscription-management.md
      team-members-management.md
    model-configuration/
      customizable-model.md
      interfaces.md
      load-balancing.md
      new-provider.md
      predefined-model.md
      README.md
      schema.md
    monitoring/
      integrate-external-ops-tools/
        integrate-langfuse.md
        integrate-langsmith.md
        README.md
      analysis.md
      README.md
    tools/
      tool-configuration/
        alphavantage.md
        bing.md
        comfyui.md
        dall-e.md
        google.md
        perplexity.md
        README.md
        searchapi.md
        searxng.md
        serper.md
        siliconflow.md
        stable-diffusion.md
        youtube.md
      advanced-tool-integration.md
      quick-tool-integration.md
      README.md
    workflow/
      debug-and-preview/
        checklist.md
        history.md
        log.md
        README.md
        step-run.md
        yu-lan-yu-yun-hang.md
      error-handling/
        error-type.md
        predefined-error-handling-logic.md
        README.md
      node/
        agent.md
        answer.md
        code.md
        doc-extractor.md
        end.md
        http-request.md
        ifelse.md
        iteration.md
        knowledge-retrieval.md
        list-operator.md
        llm.md
        parameter-extractor.md
        question-classifier.md
        README.md
        start.md
        template.md
        tools.md
        variable-aggregator.md
        variable-assigner.md
      additional-features.md
      bulletin.md
      export_import.md
      file-upload.md
      key-concepts.md
      orchestrate-node.md
      publish.md
      README.md
      shortcut-key.md
      variables.md
    workspace/
      app/
        README.md
      app.md
      billing.md
      explore.md
      invite-and-manage-members.md
      README.md
```

# Files

## File: en/guides/annotation/annotation-reply.md
```markdown
# Annotation Reply

The annotated replies feature provides customizable high-quality question-and-answer responses through manual editing and annotation.

Applicable scenarios:

* **Customized Responses for Specific Fields:** In customer service or knowledge base scenarios for enterprises, government, etc., service providers may want to ensure that certain specific questions are answered with definitive results. Therefore, it is necessary to customize the output for specific questions. For example, creating "standard answers" for certain questions or marking some questions as "unanswerable."
* **Rapid Tuning for POC or DEMO Products:** When quickly building prototype products, customized responses achieved through annotated replies can efficiently enhance the expected generation of Q\&A results, thereby improving customer satisfaction.

The annotated replies feature essentially provides another set of retrieval-enhanced systems, allowing you to bypass the LLM generation phase and avoid the hallucination issues of RAG.

### Workflow

1. After enabling the annotated replies feature, you can annotate the responses from LLM conversations. You can add high-quality answers from LLM responses directly as annotations or edit a high-quality answer according to your needs. These edited annotations will be saved persistently.
2. When a user asks a similar question again, the system will vectorize the question and search for similar annotated questions.
3. If a match is found, the corresponding answer from the annotation will be returned directly, bypassing the LLM or RAG process.
4. If no match is found, the question will continue through the regular process (passing to LLM or RAG).
5. Once the annotated replies feature is disabled, the system will no longer match responses from annotations.

<figure><img src="../../.gitbook/assets/image (130).png" alt="" width="563"><figcaption><p>Annotated Replies Workflow</p></figcaption></figure>

### Enabling Annotated Replies in Prompt Orchestration

Enable the annotated replies switch by navigating to **“Orchestrate -> Add Features”**:

<figure><img src="../../.gitbook/assets/annotated-replies.png" alt=""><figcaption><p>Enabling Annotated Replies in Prompt Orchestration</p></figcaption></figure>

When enabling, you need to set the parameters for annotated replies, which include: Score Threshold and Embedding Model.

**Score Threshold:** This sets the similarity threshold for matching annotated replies. Only annotations with scores above this threshold will be recalled.

**Embedding Model:** This is used to vectorize the annotated text. Changing the model will regenerate the embeddings.

Click save and enable, and the settings will take effect immediately. The system will generate embeddings for all saved annotations using the embedding model.

<figure><img src="../../.gitbook/assets/setting-parameters-for-annotated-replies.png" alt=""><figcaption><p>Setting Parameters for Annotated Replies</p></figcaption></figure>

### Adding Annotations in the Conversation Debug Page

You can directly add or edit annotations on the model response information in the debug and preview pages.

<figure><img src="../../.gitbook/assets/add-annotation-reply.png" alt=""><figcaption><p>Adding Annotated Replies</p></figcaption></figure>

Edit the response to the high-quality reply you need and save it.

<figure><img src="../../.gitbook/assets/editing-annotated-replies.png" alt=""><figcaption><p>Editing Annotated Replies</p></figcaption></figure>

Re-enter the same user question, and the system will use the saved annotation to reply to the user's question directly.

<figure><img src="../../.gitbook/assets/annotaiton-reply.png" alt=""><figcaption><p>Replying to User Questions with Saved Annotations</p></figcaption></figure>

### Enabling Annotated Replies in Logs and Annotations

Enable the annotated replies switch by navigating to “Logs & Ann. -> Annotations”:

<figure><img src="../../.gitbook/assets/logs-annotation-switch.png" alt=""><figcaption><p>Enabling Annotated Replies in Logs and Annotations</p></figcaption></figure>

### Setting Parameters for Annotated Replies in the Annotation Backend

The parameters that can be set for annotated replies include: Score Threshold and Embedding Model.

**Score Threshold:** This sets the similarity threshold for matching annotated replies. Only annotations with scores above this threshold will be recalled.

**Embedding Model:** This is used to vectorize the annotated text. Changing the model will regenerate the embeddings.

<figure><img src="../../.gitbook/assets/annotated-replies-initial.png" alt=""><figcaption><p>Setting Parameters for Annotated Replies</p></figcaption></figure>

### Bulk Import of Annotated Q\&A Pairs

In the bulk import feature, you can download the annotation import template, edit the annotated Q\&A pairs according to the template format, and then import them in bulk.

<figure><img src="../../.gitbook/assets/bulk-import-annotated.png" alt=""><figcaption><p>Bulk Import of Annotated Q&#x26;A Pairs</p></figcaption></figure>

### Bulk Export of Annotated Q\&A Pairs

Through the bulk export feature, you can export all saved annotated Q\&A pairs in the system at once.

<figure><img src="../../.gitbook/assets/bulk-export-annotations.png" alt=""><figcaption><p>Bulk Export of Annotated Q&#x26;A Pairs</p></figcaption></figure>

### Viewing Annotation Hit History

In the annotation hit history feature, you can view the edit history of all hits on the annotation, the user's hit questions, the response answers, the source of the hits, the matching similarity scores, the hit time, and other information. You can use this information to continuously improve your annotated content.

<figure><img src="../../.gitbook/assets/view-annotation-hit-history.png" alt=""><figcaption><p>Viewing Annotation Hit History</p></figcaption></figure>
```

## File: en/guides/annotation/logs.md
```markdown
# Logs and Annotation

{% hint style="warning" %}
Please ensure that your application complies with local regulations when collecting user data. The common practice is to publish a privacy policy and obtain user consent.
{% endhint %}

The **Logs** feature is designed to observe and annotate the performance of Dify applications. Dify records logs for all interactions with the application, whether through the WebApp or API. If you are a Prompt Engineer or LLM operator, it will provide you with a visual experience of LLM application operations.

### Using the Logs Console

You can find the Logs in the left navigation of the application. This page typically displays:

* Interaction records between users and AI within the selected timeframe
* The results of user input and AI output, which for conversational applications are usually a series of message flows
* Ratings from users and operators, as well as improvement annotations from operators

The logs currently do not include interaction records from the Prompt debugging process.

> For the Free tier teams, interaction logs are only retained for the last 30 days. To keep interaction history for a longer period, please visit our [pricing page](https://dify.ai/pricing) to upgrade to a higher tier or consider deploying the [Community Edition](https://docs.dify.ai/getting-started/install-self-hosted/docker-compose).

### Improvement Annotations

{% hint style="info" %}
These annotations will be used for model fine-tuning in future versions of Dify to improve model accuracy and response style. The current preview version only supports annotations.
{% endhint %}

<figure><img src="../../.gitbook/assets/app-logs-ann.png" alt=""><figcaption><p>Mark logs to improve your app</p></figcaption></figure>

Clicking on a log entry will open the log details panel on the right side of the interface. In this panel, operators can annotate an interaction:

* Give a thumbs up for well-performing messages
* Give a thumbs down for poorly-performing messages
* Mark improved responses for improvement, which represents the text you expect AI to reply with

Please note that if multiple administrators in the team annotate the same log entry, the last annotation will overwrite the previous ones.
```

## File: en/guides/annotation/README.md
```markdown
# Annotation
```

## File: en/guides/application-orchestrate/app-toolkits/moderation-tool.md
```markdown
# Moderation Tool

In our interactions with AI applications, we often have stringent requirements in terms of content security, user experience, and legal regulations. At this point, we need the "Sensitive Word Review" feature to create a better interactive environment for end-users. On the orchestration page, click "Add Feature" and locate the "Content Review" toolbox at the bottom:

<figure><img src="../../../.gitbook/assets/content-moderation.png" alt=""><figcaption><p>Content moderation</p></figcaption></figure>

## Call the OpenAI Moderation API

OpenAI, along with most companies providing LLMs, includes content moderation features in their models to ensure that outputs do not contain controversial content, such as violence, sexual content, and illegal activities. Additionally, OpenAI has made this content moderation capability available, which you can refer to [OpenAI's Moderation](https://platform.openai.com/docs/guides/moderation/overview).

Now you can also directly call the OpenAI Moderation API on Dify; you can review either input or output content simply by entering the corresponding "preset reply."

<figure><img src="../../../.gitbook/assets/content-moderation-settings-openai.png" alt="" width="563"><figcaption><p>Configuring Load Balancing from Add Model</p></figcaption></figure>

## Keywords

Developers can customize the sensitive words they need to review, such as using "kill" as a keyword to perform an audit action when users input. The preset reply content should be "The content is violating usage policies." It can be anticipated that when a user inputs a text chuck containing "kill" at the terminal, it will trigger the sensitive word review tool and return the preset reply content.

<figure><img src="../../../.gitbook/assets/keywords-content-moderation.png" alt="" width="563"><figcaption><p>Configuring Load Balancing from Add Model</p></figcaption></figure>

## Moderation Extension

Different enterprises often have their own mechanisms for sensitive word moderation. When developing their own AI applications, such as an internal knowledge base ChatBot, enterprises need to moderate the query content input by employees for sensitive words. For this purpose, developers can write an API extension based on their enterprise's internal sensitive word moderation mechanisms, which can then be called on Dify to achieve a high degree of customization and privacy protection for sensitive word review.

<figure><img src="../../../.gitbook/assets/moderation-api-extension.png" alt=""><figcaption><p>Moderation Extension</p></figcaption></figure>
```

## File: en/guides/application-orchestrate/app-toolkits/README.md
```markdown
# Application Toolkits

In **Application Orchestration**, click **Add Feature** to open the application toolbox.

The application toolbox provides various additional features for Dify's [applications](../#application_type):

<figure><img src="../../../.gitbook/assets/content_moderation (1).png" alt=""><figcaption></figcaption></figure>

### Conversation Opening

In conversational applications, the AI will proactively say the first sentence or ask a question. You can edit the content of the opening, including the initial question. Using conversation openings can guide users to ask questions, explain the application background, and lower the barrier for initiating a conversation.

<figure><img src="../../../.gitbook/assets/image (240).png" alt=""><figcaption><p>Conversation Opening</p></figcaption></figure>

### Next Step Question Suggestions

Setting next step question suggestions allows the AI to generate 3 follow-up questions based on the previous conversation, guiding the next round of interaction.

<figure><img src="../../../.gitbook/assets/image (241).png" alt=""><figcaption></figcaption></figure>

### Citation and Attribution

When this feature is enabled, the large language model will cite content from the knowledge base when responding to questions. You can view specific citation details below the response, including the original text segment, segment number, and match score.

For more details, please see [Citation and Attribution](https://docs.dify.ai/guides/knowledge-base/retrieval-test-and-citation#id-2.-citation-and-attribution).

### Content Moderation

During interactions with AI applications, we often have stringent requirements regarding content safety, user experience, and legal regulations. In such cases, we need the "Sensitive Content Moderation" feature to create a better interaction environment for end users.

### Annotated Replies

The annotated replies feature allows for customizable high-quality Q\&A responses through manual editing and annotation.

See [Annotated Replies](../../annotation/annotation-reply.md).
```

## File: en/guides/application-orchestrate/prompt-engineering/prompt-engineering-expert-mode.md
```markdown
# Prompting Expert Mode

Currently, the orchestration for creating apps in Dify is set to **Basic Mode** by default. This is ideal for non-tech-savvy individuals who want to quickly make an app. For example, if you want to create a corporate knowledge-base ChatBot or an article summary Generator, you can use the **Basic Mode** to design `Pre-prompt` words, add `Query`, integrate `Context`, and other straightforward steps to launch a complete app. For more head to 👉 [text-generation-application.md](../../user-guide/creating-dify-apps/prompt-engineering/text-generation-application.md "mention") and [conversation-application.md](../../user-guide/creating-dify-apps/prompt-engineering/conversation-application.md "mention").

💡However, you surely want to design prompts in a more customized manner if you're a developer who has conducted in-depth research on prompts, then you should opt for the **Expert Mode**. In this mode, you are granted permission to customize comprehensive prompts rather than using the pre-packaged prompts from Dify. You can modify the built-in prompts, rearrange the placement of `Context` and `History` , set necessary parameters, and more. If you're familiar with the OpenAI's Playground, you can get up to speed with this mode more quickly.

***

Well, before you try the new mode, you should be aware of some essential elements in it:

*   **Complete**

    When choosing a model, if you see "COMPLETE" on the right side of the model name, it indicates a Text completion model e.g. <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/screenshot-20231017-092613.png" alt="" data-size="line">

    This type of model accepts a freeform text string and generates a text completion, attempting to match any context or pattern you provide. For example, if you write the prompt `As René Descartes said, "I think, therefore"`, it's highly likely that the model will return `"I am."` as the completion.\\
*   **Chat**

    When choosing a model, if you see "CHAT" on the right side of the model name, it indicates a Chat completions model e.g. <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/screenshot-20231017-092957.png" alt="" data-size="line">

    This type of model takes a list of messages as input and returns a message generated by the model as output. It consists of three message types: `SYSTEM`, `USER`, and `ASSISTANT`.

    *   `SYSTEM`

        System messages help guide the behavior of the AI assistant. For example, you can alter the personality of the AI assistant or provide specific instructions on how it should perform throughout the conversation. System messages are optional. Without system messages, the AI assistant might behave like it's using generic messages such as `"you are a helpful assistant."`
    *   `USER`

        User messages provide requests or comments for the AI assistant to respond to.
    *   `ASSISTANT`

        Assistant messages store previous assistant responses, but they can also be written by you to provide examples of desired behavior.\\
*   **Stop\_Sequences**

    Stop\_Sequences refers to specific words, phrases, or characters used to send a signal to LLM to stop generating text.\\
*   **Blocks**

    <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/Context.png" alt="" data-size="line">

    When users input a query, the app processes the query as search criteria for the knowledge. The organized results from the search then replace the variable `Context`, allowing the LLM to reference the content for its response.

    <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/QUERY.png" alt="" data-size="line">

    The query content is only available in the Text completion models of conversational applications. The content entered by the user during the conversation will replace this variable, initiating a new turn of dialogue.

    <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/history (1).png" alt="" data-size="line">

    The conversation history is only available in the Text completion model of conversational applications. When engaging in multiple conversations in dialogue applications, Dify will assemble and concatenate the historical dialogue records according to built-in rules and replace the 'Conversation History' variable. The `Human` and `Assistant` prefixes can be modified by clicking on the `...` after "Conversation History".\\
*   **Prompt Template**

    In this mode, before formal orchestration, an initial template is provided in the prompt box. We can directly modify this template to have more customized requirements for LLM. Different types of applications have variations in different modes.

    For more head to 👉 [prompt-template.md](prompt-template.md "mention")

***

## Comparison of the two modes

<table><thead><tr><th width="333">Comparison Dimension</th><th width="197">Basic Mode</th><th>Expert Mode</th></tr></thead><tbody><tr><td>Visibility of Built-in Prompts</td><td>Invisible</td><td>Visible</td></tr><tr><td>Automatic Design</td><td>Available</td><td>Disabled</td></tr><tr><td>Variable Insertion</td><td>Available</td><td>Available</td></tr><tr><td>Block Validation</td><td>Disabled</td><td>Available</td></tr><tr><td>SYSTEM / USER / ASSISTANT</td><td>Invisible</td><td>Visible</td></tr><tr><td>Context parameter settings</td><td>Available</td><td>Available</td></tr><tr><td>PROMPT LOG</td><td>Available</td><td>Available</td></tr><tr><td>Stop_Sequences</td><td>Disabled</td><td>Available</td></tr></tbody></table>

## Operation Guide

### 1. How to enter the Expert Mode

After creating an application, you can switch to the **Expert Mode** on the prompt design page.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/000.png" alt=""><figcaption><p>Access to the <strong>Expert Mode</strong></p></figcaption></figure>

{% hint style="warning" %}
After modifying the prompts in the **Expert Mode** and publishing the application, you will not be able to revert back to the **Basic Mode**.
{% endhint %}

### 2. Modify Context parameters

In both two modes, you can modify the parameters for the inserting context, which includes **TopK** and **Score Threshold**.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/Context parameters.png" alt=""><figcaption><p>Context parameters</p></figcaption></figure>

{% hint style="warning" %}
Please note that only after uploading the context, the built-in prompts containing <img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/Context.png" alt="" data-size="line"> will be displayed on the prompt design page.
{% endhint %}

**TopK:** The value is an integer from 1 to 10.

It is used to filter the text fragments with the highest similarity to the user's query. The system will also dynamically adjust the number of fragments based on the context window size of the selected model. The default system value is 2. This value is recommended to be set between 2 and 5, because we expect to get answers that match the embedded context more closely.\\

**Score Threshold:** The value is a floating-point number from 0 to 1, with two decimal places.

It is used to set the similarity threshold for text segment selection, i.e., it only recalls text chunks that exceed the set score. By default, the system turns this setting off, meaning there's no filtering based on the similarity value of the recalled text chunks. When activated, the default value is 0.7. We recommend keeping this setting deactivated by default. If you have more stringent reply requirements, you can set a higher value, though it's not advisable to set it excessively high.

### 3. **Stop\_Sequences**

We do not expect the LLM to generate excessive content. Therefore, it's necessary to set specific words, phrases, or characters to signal the LLM to stop generating text. The default setting is `Human:` .For example, if you've written the _Few-Shot_ below:

```
Human1: What color is the sky?

Assistant1: The sky is blue.

Human1: What color is the fire?

Assistant1: The fire is red.

Human1: What color is the soil?

Assistant1: 
```

Then, in the model parameters, you need to locate `Stop_Sequences` and input `Human1:`. Do not forget to press the `Tab` key. In this way, the LLM will only respond with one sentence when replying instead of generating any extra dialogues:

```
The soil is yellow.
```

Because LLM stops generating content before the next `Human1:`.

### 4.Use "/" to insert Variables and Blocks

You can enter "/" in the text editor to quickly bring up Blocks to insert into the prompt.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/shortcut.png" alt=""><figcaption><p>shortcut "/"</p></figcaption></figure>

{% hint style="warning" %}
Except for `Variables`, other Blocks cannot be inserted repeatedly. In different applications and models, the Blocks that can be inserted will vary based on different prompt template structures.
{% endhint %}

### 5. Input Pre-prompt

The system's initial Prompt Template provides the necessary parameters and requirements for LLM's response. For more head to 👉 [prompt-template.md](prompt-template.md "mention").

The core of the early-stage development by developers is the Pre-prompt for the conversation. It requires designing built-in prompts, with the suggested insertion position below:

```
When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

You are a customer service assistant for Apple Inc., and you can provide iPhone consultation services to users.
When answering, you need to list the detailed specifications of the iPhone, and you must output this information in a vertical {{Format}} table. If the list is too long, it should be transposed.
You are allowed to take a long time to think to generate a more reasonable output.
Note: You currently have knowledge of only a subset of iPhone models, not all of them.
```

You can also customize and modify the initial prompt template. For example, if you want LLM's responses to be in English, you can modify the built-in prompts as follows:

```
When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language English.
```

### 6. Check the Prompt Log

During debugging, you can not only check the user's input and LLM's responses but also view the complete prompts by clicking on the icon in the upper-left corner of the send message button. This makes it convenient for developers to confirm whether the input Variable content, Context, Chat History, and Query content meet expectations.

#### 6.1 Access to the Prompt Log

In the debugging preview interface, after engaging in a conversation with the AI, simply move the mouse pointer over any user session, and you will see the "Log" icon button in the upper-left corner. Click on it to view the Prompt Log.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/Access to the Prompt Log (1).png" alt=""><figcaption><p>Access to the Prompt Log</p></figcaption></figure>

In the Prompt Log, we can clearly see:

1. Complete built-in prompts.
2. Relevant text chucks referenced in the current session.
3. Historical session records.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/log1.png" alt=""><figcaption><p>Prompt Log</p></figcaption></figure>

From the log, we can view the complete prompts that have been assembled by the system and sent to LLM, and we can continuously improve prompt input based on debugging results.

#### 6.2 Trace Debugging History

In the initial application's main interface, you can find "Logs & Ann." in the left-side navigation bar. Clicking on it will allow you to view the complete logs. In the "Logs & Ann." main interface, you can click on any conversation log entry. In the right-side dialog box that appears, simply move the mouse pointer over the conversation and then click the "Log" button to check the Prompt Log.

For more head to 👉 [logs.md](../logs.md "mention") .

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/prompt-engineering/33333.png" alt=""><figcaption><p>Logs &#x26; Ann.</p></figcaption></figure>
```

## File: en/guides/application-orchestrate/prompt-engineering/prompt-template.md
```markdown
# Prompt Template

In order to meet the more customized requirements of developers for LLM, Dify has fully opened the built-in complete prompts in the **Expert Mode** and provided initial templates in the composition interface. Below are four initial templates for reference:

### 1. Using Chat models to build Conversational apps

* **SYSTEM**

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.
{{pre_prompt}}
```

* **USER**

```
{{Query}} //Enter the Query variables here.
```

* **ASSISTANT**

```
""
```

#### **Prompt Structure：**

* `Context`
* `Pre-prompt`
* `Query`

### 2. Using Chat models to build Text Generator apps

* **SYSTEM**

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.
{{pre_prompt}}
```

* **USER**

```
{{Query}} //Enter the Query variables here.
```

* **ASSISTANT**

```
""
```

#### **Prompt Structure：**

* `Context`
* `Pre-prompt`
* `Query`

### 3. Using Complete models to build Conversational apps

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

{{pre_prompt}}

Here is the chat histories between human and assistant, inside <histories></histories> XML tags.

<histories>
{{#histories#}}
</histories>


Human: {{#query#}}

Assistant: 
```

**Prompt Structure：**

* `Context`
* `Pre-prompt`
* `History`
* `Query`

### 4. Using Complete models to build Text Generator apps

```
Use the following context as your learned knowledge, inside <context></context> XML tags.

<context>
{{#context#}}
</context>

When answer to user:
- If you don't know, just say that you don't know.
- If you don't know when you are not sure, ask for clarification.
Avoid mentioning that you obtained the information from the context.
And answer according to the language of the user's question.

{{pre_prompt}}
{{query}}
```

#### **Prompt Structure：**

* `Context`
* `Pre-prompt`
* `Query`

{% hint style="warning" %}
Dify has collaborated with some model providers for joint deep optimization of system prompts, and the initial templates for some models may differ from the examples provided above.
{% endhint %}

### **Parameter Definitions**&#x20;

* **Context**: Used to insert related text from the knowledge as context into the complete prompts.&#x20;
* **Pre-prompt**: Pre-prompts arranged in the **Basic Mode** are inserted into the complete prompts.&#x20;
* **History**: When building a chat application using text generation models, the system inserts the user's conversation history as context into the complete prompts. Since some models may respond differently to role prefixes, you can also modify the role prefix name in the conversation history settings, for example, changing the name "Assistant" to "AI".
* **Query**: The query content represents variable values used to insert questions that users input during the chat.
```

## File: en/guides/application-orchestrate/prompt-engineering/README.md
```markdown
---
description: >-
  Master the use of Dify for orchestrating applications and practicing Prompt
  Engineering, and build high-value AI applications with the two built-in
  application types.
---
<!-- TODO -->
# Prompt Engineering

The core concept of Dify is the declarative definition of AI applications. Everything including Prompts, context, plugins, etc. can be described in a YAML file (which is why it is called Dify). It ultimately presents a single API or out-of-the-box WebApp.

At the same time, Dify provides an easy-to-use Prompt orchestration interface where developers can visually orchestrate various application features based on Prompts. Doesn't it sound simple?

For both simple and complex AI applications, good Prompts can effectively improve the quality of model output, reduce error rates, and meet the needs of specific scenarios. Dify currently provides two common application forms: conversational and text generator. This section will guide you through visually orchestrating AI applications.

### Application Orchestration Steps

1. Determine application scenarios and functional requirements
2. Design and test Prompts and model parameters
3. Orchestrate Prompts
4. Publish the application
5. Observe and continuously iterate

### Hands-on Practice

TODO

### The Differences between Application Types

Text generation and conversation applications in Dify have slight differences in prompt orchestration. Conversation applications require incorporating "conversation lifecycle" to meet more complex user scenarios and context management needs.

Prompt Engineering has developed into a field with tremendous potential, worthy of continuous exploration. Please continue reading to learn about the orchestration guidelines for both types of applications.

### Extended Reading

1. [Learn Prompting](https://learnprompting.org/)
2. [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
3. [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
```

## File: en/guides/application-orchestrate/agent.md
```markdown
# Agent

## Definition

An Agent Assistant can leverage the reasoning abilities of large language models (LLMs). It independently sets goals, simplifies complex tasks, operates tools, and refines processes to complete tasks autonomously.

## Usage Instructions

To facilitate quick learning and use, application templates for the Agent Assistant are available in the 'Explore' section. You can integrate these templates into your workspace. The new Dify 'Studio' also allows the creation of a custom Agent Assistant to suit individual requirements. This assistant can assist in analyzing financial reports, composing reports, designing logos, and organizing travel plans.

<figure><img src="../../.gitbook/assets/docs-1.png" alt=""><figcaption><p>Explore-Agent Assistant Application Template</p></figcaption></figure>

The task completion ability of the Agent Assistant depends on the inference capabilities of the model selected. We recommend using a more powerful model series like GPT-4 when employing Agent Assistant to achieve more stable task completion results.

<figure><img src="../../.gitbook/assets/docs-3.png" alt=""><figcaption><p>Selecting the Reasoning Model for Agent Assistant</p></figcaption></figure>

You can write prompts for the Agent Assistant in 'Instructions'. To achieve optimal results, you can clearly define its task objectives, workflow, resources, and limitations in the instructions.

<figure><img src="../../.gitbook/assets/docs-4.png" alt=""><figcaption><p>Orchestrating Prompts for Agent Assistant</p></figcaption></figure>

## Adding Tools for the Agent Assistant

In the "Context" section, you can incorporate knowledge base tools that the Agent Assistant can utilize for information retrieval. This will assist in providing it with external background knowledge.

In the "Tools" section, you are able to add tools that are required for use. These tools can enhance the capabilities of LLMs, such as internet searches, scientific computations, or image creation, thereby enriching the LLM's ability to interact with the real world. Dify offers two types of tools: **built-in tools and custom tools.**

You have the option to directly use built-in tools in Dify, or you can easily import custom API tools (currently supporting OpenAPI/Swagger and OpenAI Plugin standards).

<figure><img src="../../.gitbook/assets/docs-5.png" alt=""><figcaption><p>Adding Tools for the Assistant</p></figcaption></figure>

The **Tools** feature allows you to create more powerful AI applications on Dify. For example, you can orchestrate suitable tools for Agent Assistant, enabling it to complete complex tasks through reasoning, step decomposition, and tool invocation.

Additionally, the tool simplifies the integration of your application with other systems or services, enabling interactions with the external environment, such as executing code or accessing proprietary information sources. Simply mention the name of the tool you want to invoke in the chat box, and it will be automatically activated.

![](../../.gitbook/assets/agent-dalle3.png)

## Agent Settings

On Dify, two inference modes are provided for Agent Assistant: Function Calling and ReAct. Models like GPT-3.5 and GPT-4 that support Function Calling have demonstrated better and more stable performance. For model series that do not support Function Calling, we have implemented the ReAct inference framework to achieve similar effects.

In the Agent settings, you can modify the iteration limit of the Agent.

<figure><img src="../../.gitbook/assets/docs-6.png" alt=""><figcaption><p>Function Calling Mode</p></figcaption></figure>

<figure><img src="../../.gitbook/assets/guides/application_orchestrate/agent/sec-7.png" alt=""><figcaption><p>ReAct Mode</p></figcaption></figure>

## Configuring the Conversation Opener

You can set up a conversation opener and initial questions for your Agent Assistant. The configured conversation opener will be displayed at the beginning of each user's first interaction, showcasing the types of tasks the Agent can perform, along with examples of questions that can be asked.

<figure><img src="../../.gitbook/assets/docs-8.png" alt=""><figcaption><p>Configuring the Conversation Opener and Initial Questions</p></figcaption></figure>

### Uploading Documentation File

Some LLMs now natively support file processing, such as [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) and [Gemini 1.5 Pro](https://ai.google.dev/api/files). You can check the LLMs' websites for details on their file upload capabilities.

Select an LLM that supports file reading and enable the "Documentation" feature. This enables the Chatbot to recognize files without complex configurations.

![](https://assets-docs.dify.ai/2024/11/9f0b7a3c67b58c0bd7926501284cbb7d.png)

## Debugging and Preview

After orchestrating your Agent Assistant, you have the option to debug and preview it before publishing it as an application. This allows you to assess the effectiveness of the agent in completing tasks.

<figure><img src="../../.gitbook/assets/docs-9.png" alt=""><figcaption><p>Debugging and Preview</p></figcaption></figure>

## Application Publish

<figure><img src="../../.gitbook/assets/docs-10.png" alt=""><figcaption><p>Publishing the Application as a Webapp</p></figcaption></figure>
```

## File: en/guides/application-orchestrate/conversation-application.md
```markdown
# Conversation Assistant

Conversation applications use a one-question-one-answer mode to have a continuous conversation with the user.

### Applicable scenarios

Conversation applications can be used in fields such as customer service, online education, healthcare, financial services, etc. These applications can help organizations improve work efficiency, reduce labor costs, and provide a better user experience.

### How to compose

Conversation applications supports: prompts, variables, context, opening remarks, and suggestions for the next question.

Here, we use a interviewer application as an example to introduce the way to compose a conversation applications.

#### Step 1 Create an application

Click the "Create Application" button on the homepage to create an application. Fill in the application name, and select **"Chatbot"**.

![](https://assets-docs.dify.ai/2024/12/8012e6ed06bfb10b239a4b999b1a0787.png)

#### Step 2: Compose the Application

After the application is successfully created, it will automatically redirect to the application overview page. Click on the button on the left menu: **"Orchestrate"** to compose the application.

<figure><img src="../../.gitbook/assets/compose-the-app.png" alt=""><figcaption></figcaption></figure>

**2.1 Fill in Prompts**

Prompt phrases are used to guide AI in providing professional responses, making the replies more precise. You can utilize the built-in prompt generator to craft suitable prompts. Prompts support the insertion of form variables, such as `{{input}}`. The values in the prompt variables will be replaced with the values filled in by the user.

Example:

1. Enter the interview scenario command.
2. The prompt will automatically generate on the right side content box.
3. You can insert custom variables within the prompt to tailor it to specific needs or details.

For a better experience, we will add an opening dialogue: `"Hello, {{name}}. I'm your interviewer, Bob. Are you ready?"`

To add the opening dialogue, click the "Add Feature" button in the upper left corner, and enable the "Conversation remarkers" feature:

<figure><img src="../../.gitbook/assets/conversation-remarkers.png" alt=""><figcaption></figcaption></figure>

And then edit the opening remarks:

![](../../.gitbook/assets/conversation-options.png)

**2.2 Adding Context**

If an application wants to generate content based on private contextual conversations, it can use our [knowledge](../knowledge-base/) feature. Click the "Add" button in the context to add a knowledge base.

![](../../../img/context.png)

**2.3 Uploading Documentation File**

Some LLMs now natively support file processing, such as [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support) and [Gemini 1.5 Pro](https://ai.google.dev/api/files). You can check the LLMs' websites for details on their file upload capabilities.

Select an LLM that supports file reading and enable the "Documentation" feature. This enables the Chatbot to recognize files without complex configurations.

![](https://assets-docs.dify.ai/2024/11/823399d85e8ced5068dc9da4f693170e.png)

**2.4 Debugging**

Enter user inputs on the right side and check the respond content.

![](../../.gitbook/assets/debug.png)

If the results are not satisfactory, you can adjust the prompts and model parameters. Click on the model name in the upper right corner to set the parameters of the model:

![](../../.gitbook/assets/adjust-model-parameters.png)

**Debugging with multiple models:**

If debugging with a single model feels inefficient, you can utilize the **Debug as Multiple Models** feature to batch-test the models’ response effectiveness.

![](../../.gitbook/assets/multiple-models.png)

Supports adding up to 4 LLMs at the same time.

![](../../.gitbook/assets/multiple-models-2.png)

> ⚠️ When using the multi-model debugging feature, if only some large models are visible, it is because other large models’ keys have not been added yet. You can manually add multiple models’ keys in [“Add New Provider”](https://docs.dify.ai/guides/model-configuration/new-provider).

**2.4 Publish App**

After debugging your application, click the **"Publish"** button in the top right corner to create a standalone AI application. In addition to experiencing the application via a public URL, you can also perform secondary development based on APIs, embed it into websites, and more. For details, please refer to [Publishing](https://docs.dify.ai/guides/application-publishing).

If you want to customize the application that you share, you can Fork our open source [WebApp template](https://github.com/langgenius/webapp-conversation). Based on the template, you can modify the application to meet your specific needs and style requirements.
```

## File: en/guides/application-orchestrate/creating-an-application.md
```markdown
# Create Application

You can create applications in Dify's studio in three ways:

* Create based on an application template (recommended for beginners)
* Create a blank application
* Create application via DSL file (Local/Online)

### Creating an Application from a Template

When using Dify for the first time, you might be unfamiliar with creating applications. To help new users quickly understand what types of applications can be built on Dify, the prompt engineers from the Dify team have already created high-quality application templates for multiple scenarios.

You can select "Studio" from the navigation menu, then choose "Create from Template" in the application list.

<figure><img src="../../.gitbook/assets/create-an-app.png" alt=""><figcaption><p>Create an application from a template</p></figcaption></figure>

Select any template and click **Use this template.**

<figure><img src="../../.gitbook/assets/image (169).png" alt=""><figcaption><p>Dify application templates</p></figcaption></figure>

### Creating a New Application

If you need to create a blank application on Dify, you can select "Studio" from the navigation and then choose "Create from Blank" in the application list.

<figure><img src="../../.gitbook/assets/create-from-blank.png" alt=""><figcaption><p>Create a blank application</p></figcaption></figure>

When creating an application for the first time, you might need to first understand the [basic concepts](./#application_type) of the four different types of applications on Dify: Chatbot, Text Generator, Agent, Chatflow and Workflow.

{% embed url="https://www.motionshot.app/walkthrough/6773d589d27e58127b913946/embed?fullscreen=1&hideAsSteps=1&hideCopy=1&hideDownload=1&hideSteps=1" %}

When selecting a specific application type, you can customize it by providing a name, choosing an appropriate icon(or uploading your favorite image as an icon), and writing a clear and concise description of its purpose. These details will help team members easily understand and use the application in the future.

![](https://assets-docs.dify.ai/2024/12/8012e6ed06bfb10b239a4b999b1a0787.png)

### Creating from a DSL File

{% hint style="info" %}
Dify DSL is an AI application engineering file standard defined by Dify.AI. The file format is YML. This standard covers the basic description of the application, model parameters, orchestration configuration, and other information.
{% endhint %}

#### Import local DSL file

If you have obtained a template (DSL file) from the community or others, you can choose "Import DSL File" from the studio. After importing, all configuration information of the original application will be loaded directly.

<figure><img src="../../.gitbook/assets/en-import-dsl-file.png" alt=""><figcaption><p>Create an application by importing a DSL file</p></figcaption></figure>

#### Import DSL file from URL

You can also import DSL files via a URL, using the following link format:

```url
https://example.com/your_dsl.yml
```

<figure><img src="../../.gitbook/assets/en-import-dsl-file-via-url.png" alt=""><figcaption><p>Create an application by importing a DSL file</p></figcaption></figure>

> When importing a DSL file, the version will be checked. Significant discrepancies between DSL versions may lead to compatibility issues. For more details, please refer to [Application Management: Import](https://docs.dify.ai/guides/management/app-management#importing-application).
```

## File: en/guides/application-orchestrate/llms-use-faq.md
```markdown
# FAQ

### 1. How to choose a basic model?

**gpt-3.5-turbo** •gpt-3.5-turbo is an upgraded version of the gpt-3 model series. It is more powerful than gpt-3 and can handle more complex tasks. It has significant improvements in understanding long text and cross-document reasoning. Gpt-3.5 turbo can generate more coherent and persuasive text. It also has great improvements in summarization, translation and creative writing. **Good at: Long text understanding, cross-document reasoning, summary, translation, creative writing**

**gpt-4** •gpt-4 is the latest and most powerful Transformer language model. It has nearly 200 billion pre-trained parameters, making it state-of-the-art on all language tasks, especially those requiring deep understanding and generation of long, complex responses. Gpt-4 can handle all aspects of human language, including understanding abstract concepts and cross-page reasoning. Gpt-4 is the first true general language understanding system that can handle any natural language processing task in the field of artificial intelligence. **Good at: \*All NLP tasks, language understanding, long text generation, cross-document reasoning, understanding abstract concepts\***Please refer to: [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview)

### 2. Why is it recommended to set max\_tokens smaller?

Because in natural language processing, longer text outputs usually require longer computation time and more computing resources. Therefore, limiting the length of the output text can reduce the computational cost and time to some extent. For example, set: max\_tokens=500, which means that only the first 500 tokens of the output text are considered, and the part exceeding this length will be discarded. The purpose of doing so is to ensure that the length of the output text does not exceed the acceptable range of the LLM, while making full use of computing resources to improve the efficiency of the model. On the other hand, more often limiting max\_tokens can increase the length of the prompt, such as the limit of gpt-3.5-turbo is 4097 tokens, if you set max\_tokens=4000, then only 97 tokens are left for the prompt, and an error will be reported if exceeded.

### 3. How to split long text data in the knowledge reasonably?

In some natural language processing applications, text is often split into paragraphs or sentences for better processing and understanding of semantic and structural information in the text. The minimum splitting unit depends on the specific task and technical implementation. For example:

• For text classification tasks, text is usually split into sentences or paragraphs.

• For machine translation tasks, entire sentences or paragraphs need to be used as splitting units.

Finally, experiments and evaluations are still needed to determine the most suitable embedding technology and splitting unit. The performance of different technologies and splitting units can be compared on the test set to select the optimal scheme.

### 4. What distance function did we use when getting knowledge segmentation?

We use [cosine similarity](https://en.wikipedia.org/wiki/Cosine\_similarity). The choice of distance function is usually irrelevant. OpenAI embeddings are normalized to length 1, which means:

•Using the dot product to calculate cosine similarity can be slightly faster

•Cosine similarity and Euclidean distance will lead to the same ranking

After the embedding vectors are normalized to length 1, calculating the cosine similarity between two vectors can be simplified to their dot product. Because the normalized vectors have a length of 1, the result of the dot product is equal to the result of the cosine similarity.

Since the dot product calculation is faster than other similarity metrics (such as Euclidean distance), using normalized vectors for dot product calculation can slightly improve computational efficiency.

### 5. **When filling in the OpenAI key, the error "Validation failed: You exceeded your current quota, please check your plan and billing details" occurs. What is causing this error?**

This error indicates that the OpenAI key account balance has been used up. Please top up the OpenAI account at openai.com. Refer to [OpenAI ](https://openai.com/pricing)for details on their plans and billing.

### 6. When using OpenAI's key for dialogue in the application, there is an error prompt as follows. **What is the cause?**

Error 1：

```JSON
The server encountered an internal error and was unable to complete your request。Either the server is overloaded or there is an error in the application
```

Error 1：

```JSON
Rate limit reached for default-gpt-3.5-turboin organization org-wDrZCxxxxxxxxxissoZb on requestsper min。 Limit: 3 / min. Please try again in 20s. Contact us through our help center   at help.openai.com   if you continue to haveissues. Please add a payment method toyour account to increase your rate limit.Visit https://platform.openai.com/account/billingto add a payment method.
```

Please check if the official interface call rate limit has been reached. Please refer to the [official documentation](https://platform.openai.com/docs/guides/rate-limits) for details.

### 6. After local deployment, Explore-Chat returns an error "Unrecognized request argument supplied: functions". How can this be resolved?

First, please check that the frontend and backend versions are up-to-date and consistent with each other. This error can also occur if an Azure OpenAI key is being used without successfully deploying the model. Verify that the Azure OpenAI resource has a deployed model - the gpt-3.5-turbo model version must be 0613 or later, as earlier versions do not support the function calling capabilities required by Explore-Chat.

### 7. When switching models in the app, the following error is encountered:

```JSON
Anthropic: Error code: 400 - f'error': f'type': "invalid request error, 'message': 'temperature: range: -1 or 0..1)
```

This error occurs because each model has different valid ranges for its parameters. Make sure to configure the parameter value according to the allowed range for the current model.

### 8. How to solve the following error prompt?

```JSON
Query or prefix prompt is too long, you can reduce the preix prompt, or shrink the max token, or switch to a llm with a larger token limit size
```

You can lower the value of "Max token" in the parameter settings of the Prompt Eng.

### 9. What are the default models in Dify, and can open-source LLMs be used?

A: The default models can be configured under **Settings - Model Provider.** Currently supported text generation LLMs include OpenAI, Azure OpenAl, Anthropic, etc. At the same time, open-source LLMs hosted on Hugging Face, Replicate, xinference, etc. can also be integrated.

### 10. The knowledge in Community Edition gets stuck in "Queued" when Q\&A segmentation mode is enabled.

Please check if the rate limit has been reached for the Embedding model API key used.

### 11. The error "Invalid token" appears when using the app.

There are two potential solutions if the error "Invalid token" appears:

* Clear the browser cache (cookies, session storage, and local storage) or the app cache on mobile. Then, revisit the app.
* Regenerate the app URL and access the app again with the new URL. This should resolve the "Invalid token" error.

### 12. What are the size limits for uploading knowledge documents?

The maximum size for a single document upload is currently 15MB. There is also a limit of 100 total documents. These limits can be adjusted if you are using a local deployment. Refer to the [documentation](../../getting-started/install-self-hosted/install-faq.md#11.-how-to-solve-the-size-and-quantity-limitations-for-uploading-dataset-documents-in-the-local-depl) for details on changing the limits.

### 13. Why does Claude still consume OpenAI credits when using the Claude model?

The Claude model does not have its own embedding model. Therefore, the embedding process and other dialog generation like next question suggestions default to using OpenAI keys. This means OpenAI credits are still consumed. You can set different default inference and embedding models under **Settings > Model Provider.**

### 14. Is there any way to control the greater use of knowledge data rather than the model's own generation capabilities?

Whether to use a knowledge base is related to the description of the knowledge. Please write the knowledge description clearly as much as possible. Please refer to the [documentation](https://docs.dify.ai/advanced/datasets) for details.

### 15. How to better segment the uploaded knowledge document in Excel?

Set the header in the first row, and display the content in each subsequent row. Do not have any additional header settings or complex formatted table content.

### 16. I have already purchased ChatGPT plus, why can't I still use GPT4 in Dify?

ChatGPT Plus and OpenAI's GPT-4 model API are two separate products with separate pricing. The model APIs have their own pricing structure, see [OpenAI's pricing documentation](https://openai.com/pricing) for details. To get access to the GPT-4 model API, you need to pay for a billing cycle - simply having a payment method on file and access to GPT-3.5 via ChatGPT Plus is not sufficient. Please refer to [OpenAI's official documentation](https://platform.openai.com/account/billing/overview) for complete details on gaining access to GPT-4.

### 17. How to add other embedding models?

Dify supports using the listed providers as an Embedding model provider, simply select the `Embedding` type in the configuration box.

* Azure
* LocalAI
* MiniMax
* OpenAI
* Replicate
* XInference

### 18. How can I set my own created app as an app template?

The ability to set your own created app as a template is currently not supported. The existing templates are provided by Dify officially for cloud version users' reference. If you are using the cloud version, you can add apps to your workspace or customize them to make your own after modifications. If you are using the community version and need to create more app templates for your team, you may consult our business team to obtain paid technical support: [business@dify.ai](mailto:business@dify.ai)


### 19. 502 Bad Gateway

This is caused by Nginx forwarding the service to the wrong location. First, make sure the container is running, then run the following command with root privileges:
```
docker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
```
Find these two lines in the output:
```
/docker-web-1: 172.19.0.5
/docker-api-1: 172.19.0.7
```
Remember the IP addresses at the end. Then, open the location where you stored the dify source code, open dify/docker/nginx/conf.d, replace http://api:5001 with http://172.19.0.7:5001, and replace http://web:3000 with http://172.19.0.5:3000. Afterward, restart the Nginx container or reload the configuration.  
These IP addresses are ***exemplary***, you must execute the command to obtain your own IP address, do not fill it in directly.  
You may need to reconfigure based on the IP when restarting related containers.
```

## File: en/guides/application-orchestrate/overview.md
```markdown
# Overview

Web applications are for application consumers. When an application developer creates an application in Dify, he will get a corresponding web application. Users of the web application can use it without logging in. The web application is adapted to different sizes of devices: PC, tablet and mobile.

The content of the web application is consistent with the configuration published by the application. When the configuration of the application is modified and the "Publish" button is clicked on the prompt word layout page of the application to publish, the content of the web application will also be updated according to the configuration of the current application.

We can enable and disable access to the web application on the application overview page, and modify the site information of the web application:

* Icon
* Name
* Application Description
* Interface Language
* Copyright Information
* Privacy Policy Link

The functional performance of the web application depends on whether the developer enables this function when compiling the application, for example:

* Conversation remarks
* Variables filled in before the conversation
* Follow-up
* Speech to text
* More answers like this (Text Generation apps)
* ...

In the following chapters, we will introduce the two types of web applications separately:

* Text Generator
* Conversational
```

## File: en/guides/application-orchestrate/README.md
```markdown
# Application Orchestration

In Dify, an "application" refers to a practical scenario application built on large language models like GPT. By creating an application, you can apply intelligent AI technology to specific needs. It encompasses both the engineering paradigm for developing AI applications and the specific deliverables.

In short, an application provides developers with:

* A user-friendly API that can be directly called by backend or frontend applications, authenticated via Token
* A ready-to-use, aesthetically pleasing, and hosted WebApp, which you can further develop using the WebApp template
* An easy-to-use interface that includes prompt engineering, context management, log analysis, and annotation

You can choose **any one** or **all** of these to support your AI application development.

### Application Types <a href="#application_type" id="application_type"></a>

Dify offers four types of applications:

* **Chat Assistant**: A conversational assistant built on LLM
* **Text Generation**: An assistant for text generation tasks such as writing stories, text classification, translation, etc.
* **Agent**: A conversational intelligent assistant capable of task decomposition, reasoning, and tool invocation
* **Workflow**: Defines more flexible LLM workflows based on process orchestration

The differences between Text Generation and Chat Assistant are shown in the table below:

<table><thead><tr><th width="180.33333333333331"></th><th>Text Generation</th><th>Chat Assistant</th></tr></thead><tbody><tr><td>WebApp Interface</td><td>Form + Results</td><td>Chat-based</td></tr><tr><td>WebAPI Endpoint</td><td><code>completion-messages</code></td><td><code>chat-messages</code></td></tr><tr><td>Interaction Mode</td><td>One question, one answer</td><td>Multi-turn conversation</td></tr><tr><td>Streaming Results</td><td>Supported</td><td>Supported</td></tr><tr><td>Context Preservation</td><td>Per session</td><td>Continuous</td></tr><tr><td>User Input Form</td><td>Supported</td><td>Supported</td></tr><tr><td>Datasets and Plugins</td><td>Supported</td><td>Supported</td></tr><tr><td>AI Opening Remarks</td><td>Not supported</td><td>Supported</td></tr><tr><td>Example Scenarios</td><td>Translation, judgment, indexing</td><td>Chatting</td></tr></tbody></table>

###
```

## File: en/guides/application-orchestrate/text-generation-application.md
```markdown
# Text Generator
<!-- TODO Documentation Missing -->
Text generation applications are applications that can automatically generate high-quality text based on prompts provided by users. They can generate various types of text, such as article summaries, translations, etc.

### **Applicable scenarios**

Text generation applications are suitable for scenarios that require a large amount of text creation, such as news media, advertising, SEO, marketing, etc. They can provide efficient and fast text generation services for these industries, reduce labor costs, and improve production efficiency.

### **How to compose**

Text generation applications supports: prefix prompt words, variables, context, and generating more similar content.

Here, we use a translation application as an example to introduce the way to compose a text generation applications.

#### **Step 1: Create the application**

Click the "Create Application" button on the homepage to create an application. Fill in the application name, and select "Text Generator" as the application type.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (28).png" alt=""><figcaption><p>Create Application</p></figcaption></figure>

#### Step 2: Compose the Application

After the application is successfully created, it will automatically redirect to the application overview page. Click on the left-hand menu: “**Prompt Eng.**” to compose the application.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (50).png" alt=""><figcaption></figcaption></figure>

**2.1 Fill in Prefix Prompts**

Prompts are used to give a series of instructions and constraints to the AI response. Form variables can be inserted, such as `{{input}}`. The value of variables in the prompts will be replaced with the value filled in by the user.

The prompt we are filling in here is: `Translate the content to: {{language}}. The content is as follows:`

![](</en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (7) (1) (1).png>)

**2.2 Adding Context**

If the application wants to generate content based on private contextual conversations, our [knowledge](../../../features/datasets/) feature can be used. Click the "Add" button in the context to add a knowledge base.

![](</en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (12) (1) (1).png>)

**2.3 Adding Future: Generate more like this**

Generating more like this allows you to generate multiple texts at once, which you can edit and continue generating from. Click on the "Add Future" button in the upper left corner to enable this feature.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (35).png" alt=""><figcaption></figcaption></figure>

**2.4 Debugging**

We debug on the right side by entering variables and querying content. Click the "Run" button to view the results of the operation.

![](</en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (17).png>)

If the results are not satisfactory, you can adjust the prompts and model parameters. Click on the model name in the upper right corner to set the parameters of the model:

![](</en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (36).png>)

**2.5 Publish**

After debugging the application, click the **"Publish"** button in the upper right corner to save the current settings.

### **Share Application**

You can find the sharing address of the application on the overview page. Click the "Preview" button to preview the shared application. Click the "Share" button to obtain the sharing link address. Click the "Settings" button to set the information of the shared application.

<figure><img src="/en/.gitbook/assets/guides/application_orchestrate/text-generation-application/image (52).png" alt=""><figcaption></figcaption></figure>

If you want to customize the application shared outside, you can Fork our open source [WebApp template](https://github.com/langgenius/webapp-text-generator). Based on the template, you can modify the application to meet your specific situation and style requirements.
```

## File: en/guides/application-publishing/launch-your-webapp-quickly/conversation-application.md
```markdown
# Conversation Application

Conversational applications engage in continuous dialogue with users in a question-and-answer format. These applications support the following features (ensure these functions are enabled during application orchestration):

* Variables filled out before the conversation.
* Creation, pinning, and deletion of conversations.
* Conversation opening statements.
* Next step question suggestions.
* Speech-to-text.
* References and attributions.

### Variables Filled Out Before the Conversation

If you have set variable filling requirements during application orchestration, you will need to fill out the prompted information before entering the conversation window:

<figure><img src="../../../.gitbook/assets/conversation-chatbot.png" alt="" width="375"><figcaption></figcaption></figure>

Fill in the necessary details and click the "Start Conversation" button to begin chatting. Hover over the AI's response to copy the conversation content, and provide "like" or "dislike" feedback.

<figure><img src="../../../.gitbook/assets/conversation-chatbot-2.png" alt=""><figcaption></figcaption></figure>

### Creation, Pinning, and Deletion of Conversations

Click the "New Conversation" button to start a new conversation. Hover over a conversation to pin or delete it.

<figure><img src="../../../.gitbook/assets/pin-delete-chat.png" alt="" width="242"><figcaption></figcaption></figure>

### Conversation Opener

If the "Conversation Opener" feature is enabled on the application orchestration page, the AI application will automatically initiate the first line of dialogue when a new conversation is created.

<figure><img src="../../../.gitbook/assets/conversation-opener.png" alt=""><figcaption></figcaption></figure>

### Follow Up

If the "Follow-up" feature is enabled on the application orchestration page, the system will automatically generate 3 relevant question suggestions after the conversation:

<figure><img src="../../../.gitbook/assets/conversation-follow-up.png" alt=""><figcaption></figcaption></figure>

### Speech-to-Text

If the "Speech-to-Text" feature is enabled during application orchestration, you will see a speech input icon in the input box of the web application. Click the icon to convert speech to text:

_Please ensure that your device environment is authorized to use the microphone._

<figure><img src="../../../.gitbook/assets/image (79) (1).png" alt="" width="375"><figcaption></figcaption></figure>

### References and Attributions

When testing the knowledge base effect within the application, you can go to **Workspace -- Add Function -- Citation and Attribution** to enable the citation attribution feature. For detailed instructions, please refer to [Citation and Attribution](https://docs.dify.ai/guides/knowledge-base/retrieval-test-and-citation#id-2.-citation-and-attribution).
```

## File: en/guides/application-publishing/launch-your-webapp-quickly/README.md
```markdown
# Publish as a Single-page Web App

One of the benefits of creating AI applications with Dify is that you can publish a Single-page AI web app accessible to all users on the internet within minutes.

* If you're using the self-hosted open-source version, the application will run on your server
* If you're using the cloud service, the application will be hosted at [https://udify.app/](https://udify.app/).

### Publishing an AI Website

Toggle the **"In service / Disabled"** switch, your Web App URL will be effective immediately publicly shared on the internet.

<figure><img src="../../../.gitbook/assets/en-public-web-app.png" alt=""><figcaption></figcaption></figure>

We have pre-set Web App UI for the following two types of applications:

* **Text Generation (Preview)**

{% content-ref url="text-generator.md" %}
[text-generator.md](text-generator.md)
{% endcontent-ref %}

* **Conversation (Preview)**

{% content-ref url="conversation-application.md" %}
[conversation-application.md](conversation-application.md)
{% endcontent-ref %}

### Setting Up Your AI Site

You can modify the language, color theme, copyright ownership, privacy policy link, and disclaimer by clicking the "setting" button.

![](../../../.gitbook/assets/en-web-app-settings.png)

Currently, Web App supports multiple languages: English, Simplified Chinese, Traditional Chinese, Portuguese, German, Japanese, Korean, Ukrainian, and Vietnamese. If you want more languages to be supported, you can submit an Issue on GitHub to seek support or submit a PR to contribute code.

{% content-ref url="web-app-settings.md" %}
[web-app-settings.md](web-app-settings.md)
{% endcontent-ref %}

### Embedding Your AI Site

You can also integrate Dify Web App into your own web project, blog, or any other web page. For more details, please take a refer to [Embedding In Websites](https://docs.dify.ai/guides/application-publishing/embedding-in-websites).
```

## File: en/guides/application-publishing/launch-your-webapp-quickly/text-generator.md
```markdown
# Text Generator Application

The text generation application is an application that automatically generates high-quality text according to the prompts provided by the user. It can generate various types of text, such as article summaries, translations, etc.

Text generation applications support the following features:

1. Run it once.
2. Run in batches.
3. Save the run results.
4. Generate more similar results.

Let's introduce them separately.

### Run it once

Enter the query content, click the run button, and the result will be generated on the right, as shown in the following figure:

<figure><img src="../../../.gitbook/assets/text-generator.png" alt=""><figcaption></figcaption></figure>

In the generated results section, click the "Copy" button to copy the content to the clipboard. Click the "Save" button to save the content. You can see the saved content in the "Saved" tab. You can also "like" and "dislike" the generated content.

### Run in batches

Sometimes, we need to run an application many times. For example: There is a web application that can generate articles based on topics. Now we want to generate 100 articles on different topics. Then this task has to be done 100 times, which is very troublesome. Also, you have to wait for one task to complete before starting the next one.

In the above scenario, the batch operation function is used, which is convenient to operate (enter the theme into a `csv` file, only need to be executed once), and also saves the generation time (multiple tasks run at the same time). The usage is as follows:

#### Step 1 Enter the batch run page

Click the "Run Batch" tab to enter the batch run page.

<figure><img src="../../../.gitbook/assets/text-generator-batch.png" alt=""><figcaption></figcaption></figure>

#### Step 2 Download the template and fill in the content

Click the **"Download the template here"** button to obtain the template file. Edit the file and fill in the required content, then save it as a `.csv` file. Finally, upload the completed file back to Dify.

<figure><img src="../../../.gitbook/assets/text-generator-batch-download.png" alt=""><figcaption></figcaption></figure>

#### Step 3 Upload the file and run

<figure><img src="../../../.gitbook/assets/batch-run.png" alt=""><figcaption></figcaption></figure>

If you need to export the generated content, you can click the download "button" in the upper right corner to export as a `csv` file.

**Note:** The encoding of the uploaded `csv` file must be `Unicode` encoding. Otherwise, the result will fail. Solution: When exporting to a `csv` file with Excel, WPS, etc., select `Unicode` for encoding.

### Save run results

Click the "Save" button below the generated results to save the running results. In the "Saved" tab, you can see all saved content.

<figure><img src="../../../.gitbook/assets/text-generator-saved.png" alt=""><figcaption></figcaption></figure>

### Generate more similar results

If the "More like this" function is turned on the App's Orchestrate page,clicking the "More like this" button in the web application generates content similar to the current result. As shown below:

<figure><img src="../../../.gitbook/assets/text-generator-more-like-this.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/application-publishing/launch-your-webapp-quickly/web-app-settings.md
```markdown
# Overview

Web applications are designed for application users. When an application developer creates an application on Dify, a corresponding web application is generated. Users of the web application can use it without logging in. The web application is adapted for various device sizes: PC, tablet, and mobile.

The content of the web application aligns with the configuration of the published application. When the application's configuration is modified and the "Publish" button is clicked on the prompt orchestration page, the web application's content will be updated according to the current configuration of the application.

On the application overview page, you can enable or disable access to the web application and modify the web application's site information, including:

* Icon
* Name
* Application description
* Interface language
* Copyright information
* Privacy policy link

The functionality and performance of the web application depend on whether the developer has enabled these features during application orchestration, such as:

* Conversation opening remarks
* Variables to be filled before the conversation
* Next step suggestions
* Speech-to-text
* References and attributions
* More similar answers (for text-based applications)
* ......

In the following sections, we will introduce the two types of web applications:

* Text Generation
* Conversational
```

## File: en/guides/application-publishing/based-on-frontend-templates.md
```markdown
# Based on WebApp Template

If developers are developing new products from scratch or in the product prototype design phase, you can quickly launch AI sites using Dify. At the same time, Dify hopes that developers can fully freely create different forms of front-end applications. For this reason, we provide:

* **SDK** for quick access to the Dify API in various languages
* **WebApp Template** for WebApp development scaffolding for each type of application

The WebApp Templates are open source under the MIT license. You are free to modify and deploy them to achieve all the capabilities of Dify or as a reference code for implementing your own App.

You can find these Templates on GitHub:

* [Conversational app](https://github.com/langgenius/webapp-conversation)
* [Text generation app](https://github.com/langgenius/webapp-text-generator)

The fastest way to use the WebApp Template is to click "**Use this template**" on GitHub, which is equivalent to forking a new repository. Then you need to configure the Dify App ID and API Key, like this:

```javascript
export const APP_ID = ''
export const API_KEY = ''
```

More config in `config/index.ts`:

```
export const APP_INFO: AppInfo = {
  "title": 'Chat APP',
  "description": '',
  "copyright": '',
  "privacy_policy": '',
  "default_language": 'zh-Hans'
}

export const isShowPrompt = true
export const promptTemplate = ''
```

> The App ID can be obtained from the App's URL, where the long string characters is the unique App ID.

Each WebApp Template provides a README file containing deployment instructions. Usually, WebApp Templates contain a lightweight backend service to ensure that developers' API keys are not directly exposed to users.

These WebApp Templates can help you quickly build prototypes of AI applications and use all the capabilities of Dify. If you develop your own applications or new templates based on them, feel free to share with us.
```

## File: en/guides/application-publishing/developing-with-apis.md
```markdown
# Developing with APIs

Dify offers a "Backend-as-a-Service" API, providing numerous benefits to AI application developers. This approach enables developers to access the powerful capabilities of large language models (LLMs) directly in frontend applications without the complexities of backend architecture and deployment processes.

### Benefits of using Dify API

* Allow frontend apps to securely access LLM capabilities without backend development
* Design applications visually with real-time updates across all clients
* Well-encapsulated original LLM APIs
* Effortlessly switch between LLM providers and centrally manage API keys
* Operate applications visually, including log analysis, annotation, and user activity observation
* Continuously provide more tools, plugins, and knowledge

### How to use

Choose an application, and find the API Access in the left-side navigation of the Apps section. On this page, you can view the API documentation provided by Dify and manage credentials for accessing the API.

<figure><img src="../../.gitbook/assets/guides\application-publishing\launch-your-webapp-quickly/API Access.png" alt=""><figcaption><p>API document</p></figcaption></figure>

You can create multiple access credentials for an application to deliver to different users or developers. This means that API users can use the AI capabilities provided by the application developer, but the underlying Prompt engineering, knowledge, and tool capabilities are encapsulated.

{% hint style="warning" %}
In best practices, API keys should be called through the backend, rather than being directly exposed in plaintext within frontend code or requests. This helps prevent your application from being abused or attacked.
{% endhint %}

For example, if you're a developer in a consulting company, you can offer AI capabilities based on the company's private database to end-users or developers, without exposing your data and AI logic design. This ensures a secure and sustainable service delivery that meets business objectives.

### Text-generation application

These applications are used to generate high-quality text, such as articles, summaries, translations, etc., by calling the completion-messages API and sending user input to obtain generated text results. The model parameters and prompt templates used for generating text depend on the developer's settings in the Dify Prompt Arrangement page.

You can find the API documentation and example requests for this application in **Applications -> Access API**.

For example, here is a sample call an API for text generation:

{% tabs %}
{% tab title="cURL" %}
```
curl --location --request POST 'https://api.dify.ai/v1/completion-messages' \
--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \
--header 'Content-Type: application/json' \
--data-raw '{
    "inputs": {},
    "response_mode": "streaming",
    "user": "abc-123"
}'
```
{% endtab %}

{% tab title="Python" %}
```python
import requests
import json

url = "https://api.dify.ai/v1/completion-messages"

headers = {
    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',
    'Content-Type': 'application/json',
}

data = {
    "inputs": {"text": 'Hello, how are you?'},
    "response_mode": "streaming",
    "user": "abc-123"
}

response = requests.post(url, headers=headers, data=json.dumps(data))

print(response.text)
```
{% endtab %}
{% endtabs %}

### Conversational Applications

Conversational applications facilitate ongoing dialogue with users through a question-and-answer format. To initiate a conversation, you will call the `chat-messages` API. A `conversation_id` is generated for each session and must be included in subsequent API calls to maintain the conversation flow. 

#### Key Considerations for `conversation_id`:

- **Generating the `conversation_id`:** When starting a new conversation, leave the `conversation_id` field empty. The system will generate and return a new `conversation_id`, which you will use in future interactions to continue the dialogue.
- **Handling `conversation_id` in Existing Sessions:** Once a `conversation_id` is generated, future calls to the API should include this `conversation_id` to ensure the conversation continuity with the Dify bot. When a previous `conversation_id` is passed, any new `inputs` will be ignored. Only the `query` is processed for the ongoing conversation.
- **Managing Dynamic Variables:** If there is a need to modify logic or variables during the session, you can use conversation variables (session-specific variables) to adjust the bot's behavior or responses.

You can access the API documentation and example requests for this application in **Applications -> Access API**.

Here is an example of calling the `chat-messages` API:

{% tabs %}
{% tab title="cURL" %}
```
curl --location --request POST 'https://api.dify.ai/v1/chat-messages' \
--header 'Authorization: Bearer ENTER-YOUR-SECRET-KEY' \
--header 'Content-Type: application/json' \
--data-raw '{
    "inputs": {},
    "query": "eh",
    "response_mode": "streaming",
    "conversation_id": "1c7e55fb-1ba2-4e10-81b5-30addcea2276",
    "user": "abc-123"
}'
```
{% endtab %}

{% tab title="Python" %}
```python
import requests
import json

url = 'https://api.dify.ai/v1/chat-messages'
headers = {
    'Authorization': 'Bearer ENTER-YOUR-SECRET-KEY',
    'Content-Type': 'application/json',
}
data = {
    "inputs": {},
    "query": "eh",
    "response_mode": "streaming",
    "conversation_id": "1c7e55fb-1ba2-4e10-81b5-30addcea2276",
    "user": "abc-123"
}

response = requests.post(url, headers=headers, data=json.dumps(data))

print(response.text())
```
{% endtab %}
{% endtabs %}
```

## File: en/guides/application-publishing/embedding-in-websites.md
```markdown
# Embedding In Websites

Dify Apps can be embedded in websites using an iframe. This allows you to integrate your Dify App into your website, blog, or any other web page.

When use Dify Chatbot Bubble Button embed in your website, you can customize the button style, position, and other settings.

## Customizing the Dify Chatbot Bubble Button

The Dify Chatbot Bubble Button can be customized through the following configuration options:

```javascript
window.difyChatbotConfig = {
    // Required, automatically generated by Dify
    token: 'YOUR_TOKEN',
    // Optional, default is false
    isDev: false,
    // Optional, when isDev is true, default is 'https://dev.udify.app', otherwise default is 'https://udify.app'
    baseUrl: 'YOUR_BASE_URL',
    // Optional, It can accept any valid HTMLElement attribute other than `id`, such as `style`, `className`, etc
    containerProps: {},
    // Optional, If or not the button is allowed to be dragged, default is `false`
    draggable: false,
    // Optional, The axis along which the button is allowed to be dragged, default is `both`, can be `x`, `y`, `both`
    dragAxis: 'both',
    // Optional, An object of inputs that set in the dify chatbot
    inputs: {
        // key is the variable name
        // e.g.
        // name: "NAME"
    }
}
```

## Overriding Default Button Styles

You can override the default button style using CSS variables or the `containerProps` option. Apply these methods based on CSS specificity to achieve your desired customizations.

### 1.Modifying CSS Variables

The following CSS variables are supported for customization:

```css
/* Button distance to bottom, default is `1rem` */
--dify-chatbot-bubble-button-bottom

/* Button distance to right, default is `1rem` */
--dify-chatbot-bubble-button-right

/* Button distance to left, default is `unset` */
--dify-chatbot-bubble-button-left

/* Button distance to top, default is `unset` */
--dify-chatbot-bubble-button-top

/* Button background color, default is `#155EEF` */
--dify-chatbot-bubble-button-bg-color

/* Button width, default is `50px` */
--dify-chatbot-bubble-button-width

/* Button height, default is `50px` */
--dify-chatbot-bubble-button-height

/* Button border radius, default is `25px` */
--dify-chatbot-bubble-button-border-radius

/* Button box shadow, default is `rgba(0, 0, 0, 0.2) 0px 4px 8px 0px)` */
--dify-chatbot-bubble-button-box-shadow

/* Button hover transform, default is `scale(1.1)` */
--dify-chatbot-bubble-button-hover-transform
```

To change the background color to #ABCDEF, add this CSS:

```css
#dify-chatbot-bubble-button {
    --dify-chatbot-bubble-button-bg-color: #ABCDEF;
}
```

### 2.Using `containerProps`

Set inline styles using the `style` attribute:

```javascript
window.difyChatbotConfig = {
    // ... other configurations
    containerProps: {
        style: {
            backgroundColor: '#ABCDEF',
            width: '60px',
            height: '60px',
            borderRadius: '30px',
        },
        // For minor style overrides, you can also use a string value for the `style` attribute:
        // style: 'background-color: #ABCDEF; width: 60px;',
    },
}
```

Apply CSS classes using the `className` attribute:

```javascript
window.difyChatbotConfig = {
    // ... other configurations
    containerProps: {
        className: 'dify-chatbot-bubble-button-custom my-custom-class',
    },
}
```

### 3. Passing `inputs`

There are four types of inputs supported:

1. **`text-input`**: Accepts any value. The input string will be truncated if its length exceeds the maximum allowed length.
2. **`paragraph`**: Similar to `text-input`, it accepts any value and truncates the string if it's longer than the maximum length.
3. **`number`**: Accepts a number or a numerical string. If a string is provided, it will be converted to a number using the `Number` function.
4. **`options`**: Accepts any value, provided it matches one of the pre-configured options.

Example configuration:

```javascript
window.difyChatbotConfig = {
    // Other configuration settings...
    inputs: {
        name: 'apple',
    },
}
```

Note: When using the embed.js script to create an iframe, each input value will be processed—compressed using GZIP and encoded in base64—before being appended to the URL.

For example, the URL with processed input values will look like this:
`http://localhost/chatbot/{token}?name=H4sIAKUlmWYA%2FwWAIQ0AAACDsl7gLuiv2PQEUNAuqQUAAAA%3D`
```

## File: en/guides/application-publishing/README.md
```markdown
# Launching Dify Apps

For more detailed information, please refer to the following sections:

- [Publish as a Single-page Webapp](launch-your-webapp-quickly/)
- [Embedding In Websites](embedding-in-websites.md)
- [Developing with APIs](developing-with-apis.md)
- [Based on Frontend Templates](based-on-frontend-templates.md)
```

## File: en/guides/extension/api-based-extension/cloudflare-workers.md
```markdown
# Deploy API Tools with Cloudflare Workers

## Getting Started

Since the Dify API Extension requires a publicly accessible internet address as an API Endpoint, we need to deploy our API extension to a public internet address. Here, we use Cloudflare Workers for deploying our API extension.

We clone the [Example GitHub Repository](https://github.com/crazywoola/dify-extension-workers), which contains a simple API extension. We can modify this as a base.

```bash
git clone https://github.com/crazywoola/dify-extension-workers.git
cp wrangler.toml.example wrangler.toml
```

Open the `wrangler.toml` file, and modify `name` and `compatibility_date` to your application's name and compatibility date.

An important configuration here is the `TOKEN` in `vars`, which you will need to provide when adding the API extension in Dify. For security reasons, it's recommended to use a random string as the Token. You should not write the Token directly in the source code but pass it via environment variables. Thus, do not commit your wrangler.toml to your code repository.

```toml
name = "dify-extension-example"
compatibility_date = "2023-01-01"

[vars]
TOKEN = "bananaiscool"
```

This API extension returns a random Breaking Bad quote. You can modify the logic of this API extension in `src/index.ts`. This example shows how to interact with a third-party API.

```typescript
// ⬇️ implement your logic here ⬇️
// point === "app.external_data_tool.query"
// https://api.breakingbadquotes.xyz/v1/quotes
const count = params?.inputs?.count ?? 1;
const url = `https://api.breakingbadquotes.xyz/v1/quotes/${count}`;
const result = await fetch(url).then(res => res.text())
// ⬆️ implement your logic here ⬆️
```

This repository simplifies all configurations except for business logic. You can directly use `npm` commands to deploy your API extension.

```bash
npm run deploy
```

After successful deployment, you will get a public internet address, which you can add in Dify as an API Endpoint. Please note not to miss the `endpoint` path.

<figure><img src="../../../.gitbook/assets/api_extension_edit (1).png" alt=""><figcaption><p>Adding API Endpoint in Dify</p></figcaption></figure>

<figure><img src="../../../.gitbook/assets/app_tools_edit (1).png" alt=""><figcaption><p>Adding API Tool in the App edit page</p></figcaption></figure>

## Other Logic TL;DR

### About Bearer Auth

```typescript
import { bearerAuth } from "hono/bearer-auth";

(c, next) => {
    const auth = bearerAuth({ token: c.env.TOKEN });
    return auth(c, next);
},
```

Our Bearer authentication logic is as shown above. We use the `hono/bearer-auth` package for Bearer authentication. You can use `c.env.TOKEN` in `src/index.ts` to get the Token.

### About Parameter Validation

```typescript
import { z } from "zod";
import { zValidator } from "@hono/zod-validator";

const schema = z.object({
  point: z.union([
    z.literal("ping"),
    z.literal("app.external_data_tool.query"),
  ]), // Restricts 'point' to two specific values
  params: z
    .object({
      app_id: z.string().optional(),
      tool_variable: z.string().optional(),
      inputs: z.record(z.any()).optional(),
      query: z.any().optional(),  // string or null
    })
    .optional(),
});
```

We use `zod` to define the types of parameters. You can use `zValidator` in `src/index.ts` for parameter validation. Get validated parameters through `const { point, params } = c.req.valid("json");`. Our point has only two values, so we use `z.union` for definition. `params` is an optional parameter, defined with `z.optional`. It includes a `inputs` parameter, a `Record<string, any>` type representing an object with string keys and any values. This type can represent any object. You can get the `count` parameter in `src/index.ts` using `params?.inputs?.count`.

### Accessing Logs of Cloudflare Workers

```bash
wrangler tail
```

## Reference Content

* [Cloudflare Workers](https://workers.cloudflare.com/)
* [Cloudflare Workers CLI](https://developers.cloudflare.com/workers/cli-wrangler/install-update)
* [Example GitHub Repository](https://github.com/crazywoola/dify-extension-workers)
```

## File: en/guides/extension/api-based-extension/external-data-tool.md
```markdown
# External Data Tools

External data tools are used to fetch additional data from external sources after the end user submits data, and then assemble this data into prompts as additional context information for the LLM. Dify provides a default tool for external API calls, check [External Data Tool](https://docs.dify.ai/guides/knowledge-base/external-data-tool) for details.

For developers deploying Dify locally, to meet more customized needs or to avoid developing an additional API Server, you can directly insert custom external data tool logic in the form of a plugin based on the Dify service. After extending custom tools, your custom tool options will be added to the dropdown list of tool types, and team members can use these custom tools to fetch external data.

## Quick Start

Here is an example of extending an external data tool for `Weather Search`, with the following steps:

1. Initialize the directory
2. Add frontend form specifications
3. Add implementation class
4. Preview the frontend interface
5. Debug the extension

### 1. **Initialize the Directory**

To add a custom type `Weather Search`, you need to create the relevant directory and files under `api/core/external_data_tool`.

```python
.
└── api
    └── core
        └── external_data_tool
            └── weather_search
                ├── __init__.py
                ├── weather_search.py
                └── schema.json
```

### 2. **Add Frontend Component Specifications**

* `schema.json`, which defines the frontend component specifications, detailed in [.](./ "mention")

```json
{
    "label": {
        "en-US": "Weather Search",
        "zh-Hans": "天气查询"
    },
    "form_schema": [
        {
            "type": "select",
            "label": {
                "en-US": "Temperature Unit",
                "zh-Hans": "温度单位"
            },
            "variable": "temperature_unit",
            "required": true,
            "options": [
                {
                    "label": {
                        "en-US": "Fahrenheit",
                        "zh-Hans": "华氏度"
                    },
                    "value": "fahrenheit"
                },
                {
                    "label": {
                        "en-US": "Centigrade",
                        "zh-Hans": "摄氏度"
                    },
                    "value": "centigrade"
                }
            ],
            "default": "centigrade",
            "placeholder": "Please select temperature unit"
        }
    ]
}
```

### 3. Add Implementation Class

`weather_search.py` code template, where you can implement the specific business logic.

{% hint style="warning" %}
Note: The class variable `name` must be the custom type name, consistent with the directory and file name, and must be unique.
{% endhint %}

```python
from typing import Optional

from core.external_data_tool.base import ExternalDataTool


class WeatherSearch(ExternalDataTool):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "weather_search"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user save the config.

        Example:
            .. code-block:: python
                config = {
                    "temperature_unit": "centigrade"
                }

        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """

        if not config.get('temperature_unit'):
            raise ValueError('temperature unit is required')

    def query(self, inputs: dict, query: Optional[str] = None) -> str:
        """
        Query the external data tool.

        :param inputs: user inputs
        :param query: the query of chat app
        :return: the tool query result
        """
        city = inputs.get('city')
        temperature_unit = self.config.get('temperature_unit')

        if temperature_unit == 'fahrenheit':
            return f'Weather in {city} is 32°F'
        else:
            return f'Weather in {city} is 0°C'
```

<!-- ### 4. **Preview the Frontend Interface**

Follow the above steps and run the service to see the newly added custom type.

![](todo)-->

### 4. **Debug the Extension**

Now, you can select the custom `Weather Search` external data tool extension type in the Dify application orchestration interface for debugging.

## Implementation Class Template

```python
from typing import Optional

from core.external_data_tool.base import ExternalDataTool


class WeatherSearch(ExternalDataTool):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "weather_search"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user save the config.

        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """

        # implement your own logic here

    def query(self, inputs: dict, query: Optional[str] = None) -> str:
        """
        Query the external data tool.

        :param inputs: user inputs
        :param query: the query of chat app
        :return: the tool query result
        """
       
        # implement your own logic here
        return "your own data."
```

### Detailed Introduction to Implementation Class Development

### def validate_config

`schema.json` form validation method, called when the user clicks "Publish" to save the configuration.

* `config` form parameters
  * `{{variable}}` custom form variables

### def query

User-defined data query implementation, the returned result will be replaced into the specified variable.

* `inputs`: Variables passed by the end user
* `query`: Current conversation input content from the end user, a fixed parameter for conversational applications.
```

## File: en/guides/extension/api-based-extension/moderation-extension.md
```markdown
# Moderation

This module is used to review the content input by end-users and the output from LLMs within the application, divided into two types of extension points.

Please read [.](./ "mention") to complete the development and integration of basic API service capabilities.

## Extension Point

`app.moderation.input`: End-user input content review extension point. It is used to review the content of variables passed in by end-users and the input content of dialogues in conversational applications.

`app.moderation.output`: LLM output content review extension point. It is used to review the content output by LLM. When the LLM output is streaming, the content will be requested by the API in chunks of 100 characters to avoid delays in review when the output content is lengthy.

### `app.moderation.input`

#### Request Body

```json
{
    "point": "app.moderation.input", 
        "app_id": string,  
        "inputs": {  
            "var_1": "value_1",
            "var_2": "value_2",
            ...
        },
        "query": string | null  
    }
}
```

* Example

```json
{
    "point": "app.moderation.input",
    "params": {
        "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
        "inputs": {
            "var_1": "I will kill you.",
            "var_2": "I will fuck you."
        },
        "query": "Happy everydays."
    }
}
```

#### API Response

```json
{
    "flagged": bool,  
    "action": string, 
    "preset_response": string,  
    "inputs": {  
        "var_1": "value_1",
        "var_2": "value_2",
        ...
    },
    "query": string | null  
}
```

* Example

`action=direct_output`

```json
{
    "flagged": true,
    "action": "direct_output",
    "preset_response": "Your content violates our usage policy."
}
```

`action=overridden`

```
{
    "flagged": true,
    "action": "overridden",
    "inputs": {
        "var_1": "I will *** you.",
        "var_2": "I will *** you."
    },
    "query": "Happy everydays."
}
```

### `app.moderation.output`

#### Request Body

```JSON
{
    "point": "app.moderation.output", 
    "params": {
        "app_id": string,  
        "text": string  
    }
}
```

*   Example



    ```JSON
    {
        "point": "app.moderation.output",
        "params": {
            "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
            "text": "I will kill you."
        }
    }
    ```

#### API Response

```JSON
{
    "flagged": bool,  
    "action": string, 
    "preset_response": string,  
    "text": string  
```

* Example

`action=direct_output`

* ```JSON
  {
      "flagged": true,
      "action": "direct_output",
      "preset_response": "Your content violates our usage policy."
  }
  ```

`action=overridden`

* ```JSON
  {
      "flagged": true,
      "action": "overridden",
      "text": "I will *** you."
  }
  ```
```

## File: en/guides/extension/api-based-extension/moderation.md
```markdown
# Sensitive Content Moderation

This module is used to review the content input by end-users and the output content of the LLM within the application. It is divided into two types of extension points.

### Extension Points

* `app.moderation.input` - Extension point for reviewing end-user input content
  * Used to review the variable content passed in by end-users and the input content of conversational applications.
* `app.moderation.output` - Extension point for reviewing LLM output content
  * Used to review the content output by the LLM.
  * When the LLM output is streamed, the content will be segmented into 100-character blocks for API requests to avoid delays in reviewing longer outputs.

### app.moderation.input Extension Point

#### Request Body

```
{
    "point": "app.moderation.input", // Extension point type, fixed as app.moderation.input here
    "params": {
        "app_id": string,  // Application ID
        "inputs": {  // Variable values passed in by end-users, key is the variable name, value is the variable value
            "var_1": "value_1",
            "var_2": "value_2",
            ...
        },
        "query": string | null  // Current dialogue input content from the end-user, fixed parameter for conversational applications.
    }
}
```

* Example
  * ```
    {
        "point": "app.moderation.input",
        "params": {
            "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
            "inputs": {
                "var_1": "I will kill you.",
                "var_2": "I will fuck you."
            },
            "query": "Happy everydays."
        }
    }
    ```

#### API Response

```
{
    "flagged": bool,  // Whether it violates the moderation rules
    "action": string, // Action to take, direct_output for directly outputting a preset response; overridden for overriding the input variable values
    "preset_response": string,  // Preset response (returned only when action=direct_output)
    "inputs": {  // Variable values passed in by end-users, key is the variable name, value is the variable value (returned only when action=overridden)
        "var_1": "value_1",
        "var_2": "value_2",
        ...
    },
    "query": string | null  // Overridden current dialogue input content from the end-user, fixed parameter for conversational applications. (returned only when action=overridden)
}
```

* Example
  * `action=direct_output`
    * ```
      {
          "flagged": true,
          "action": "direct_output",
          "preset_response": "Your content violates our usage policy."
      }
      ```
  * `action=overridden`
    * ```
      {
          "flagged": true,
          "action": "overridden",
          "inputs": {
              "var_1": "I will *** you.",
              "var_2": "I will *** you."
          },
          "query": "Happy everydays."
      }
      ```

### app.moderation.output Extension Point

#### Request Body

```
{
    "point": "app.moderation.output", // Extension point type, fixed as app.moderation.output here
    "params": {
        "app_id": string,  // Application ID
        "text": string  // LLM response content. When the LLM output is streamed, this will be content segmented into 100-character blocks.
    }
}
```

* Example
  * ```
    {
        "point": "app.moderation.output",
        "params": {
            "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
            "text": "I will kill you."
        }
    }
    ```

#### API Response

```
{
    "flagged": bool,  // Whether it violates the moderation rules
    "action": string, // Action to take, direct_output for directly outputting a preset response; overridden for overriding the input variable values
    "preset_response": string,  // Preset response (returned only when action=direct_output)
    "text": string  // Overridden LLM response content (returned only when action=overridden)
}
```

* Example
  * `action=direct_output`
    * ```
      {
          "flagged": true,
          "action": "direct_output",
          "preset_response": "Your content violates our usage policy."
      }
      ```
  * `action=overridden`
    * ```
      {
          "flagged": true,
          "action": "overridden",
          "text": "I will *** you."
      }
      ```
```

## File: en/guides/extension/api-based-extension/README.md
```markdown
# API-Based Extension

Developers can extend module capabilities through the API extension module. Currently supported module extensions include:

* `moderation`
* `external_data_tool`

Before extending module capabilities, prepare an API and an API Key for authentication, which can also be automatically generated by Dify. In addition to developing the corresponding module capabilities, follow the specifications below so that Dify can invoke the API correctly.

<figure><img src="../../../.gitbook/assets/screenshot-20231128-104353 (2).png" alt=""><figcaption><p>Add API Extension</p></figcaption></figure>

## API Specifications

Dify will invoke your API according to the following specifications:

```
POST {Your-API-Endpoint}
```

### Header

| Header          | Value             | Desc                                                                                                                                         |
| --------------- | ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| `Content-Type`  | application/json  | The request content is in JSON format.                                                                                                       |
| `Authorization` | Bearer {api\_key} | The API Key is transmitted as a token. You need to parse the `api_key` and verify if it matches the provided API Key to ensure API security. |

### Request Body

```JSON
{
    "point":  string, // Extension point, different modules may contain multiple extension points
    "params": {
        ...  // Parameters passed to each module's extension point
    }
}
```

### API Response

```JSON
{
    ...  // For the content returned by the API, see the specific module's design specifications for different extension points.
}

```

## Check

When configuring API-based Extension in Dify, Dify will send a request to the API Endpoint to verify the availability of the API. When the API Endpoint receives `point=ping`, the API should return `result=pong`, as follows:

### Header

```JSON
Content-Type: application/json
Authorization: Bearer {api_key}
```

### Request Body

```JSON
{
    "point": "ping"
}
```

### Expected API response

```JSON
{
    "result": "pong"
}
```

\\

## For Example

Here we take the external data tool as an example, where the scenario is to retrieve external weather information based on the region as context.

### API Specifications

`POST https://fake-domain.com/api/dify/receive`

### **Header**

```JSON
Content-Type: application/json
Authorization: Bearer 123456
```

### **Request Body**

```JSON
{
    "point": "app.external_data_tool.query",
    "params": {
        "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
        "tool_variable": "weather_retrieve",
        "inputs": {
            "location": "London"
        },
        "query": "How's the weather today?"
    }
}
```

### **API Response**

```JSON
{
    "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"
}
```

### Code demo

The code is based on the Python FastAPI framework.

#### **Install dependencies.**

<pre><code><strong>pip install 'fastapi[all]' uvicorn
</strong></code></pre>

#### Write code according to the interface specifications.

<pre><code>from fastapi import FastAPI, Body, HTTPException, Header
<strong>from pydantic import BaseModel
</strong>
app = FastAPI()


class InputData(BaseModel):
    point: str
    params: dict


@app.post("/api/dify/receive")
async def dify_receive(data: InputData = Body(...), authorization: str = Header(None)):
    """
    Receive API query data from Dify.
    """
    expected_api_key = "123456"  # TODO Your API key of this API
    auth_scheme, _, api_key = authorization.partition(' ')

    if auth_scheme.lower() != "bearer" or api_key != expected_api_key:
        raise HTTPException(status_code=401, detail="Unauthorized")

    point = data.point

    # for debug
    print(f"point: {point}")

    if point == "ping":
        return {
            "result": "pong"
        }
    if point == "app.external_data_tool.query":
        return handle_app_external_data_tool_query(params=data.params)
    # elif point == "{point name}":
        # TODO other point implementation here

    raise HTTPException(status_code=400, detail="Not implemented")


def handle_app_external_data_tool_query(params: dict):
    app_id = params.get("app_id")
    tool_variable = params.get("tool_variable")
    inputs = params.get("inputs")
    query = params.get("query")

    # for debug
    print(f"app_id: {app_id}")
    print(f"tool_variable: {tool_variable}")
    print(f"inputs: {inputs}")
    print(f"query: {query}")

    # TODO your external data tool query implementation here, 
    #  return must be a dict with key "result", and the value is the query result
    if inputs.get("location") == "London":
        return {
            "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind "
                      "Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"
        }
    else:
        return {"result": "Unknown city"}
</code></pre>

#### Launch the API service.

The default port is 8000. The complete address of the API is: `http://127.0.0.1:8000/api/dify/receive`with the configured API Key '123456'.

<pre><code><strong>uvicorn main:app --reload --host 0.0.0.0
</strong></code></pre>

#### Configure this API in Dify.

<figure><img src="../../../.gitbook/assets/screenshot-20231128-104353 (2).png" alt=""><figcaption></figcaption></figure>

#### Select this API extension in the App.

<figure><img src="../../../.gitbook/assets/screenshot-20231128-104353 (1).png" alt=""><figcaption></figcaption></figure>

When debugging the App, Dify will request the configured API and send the following content (example):

```JSON
{
    "point": "app.external_data_tool.query",
    "params": {
        "app_id": "61248ab4-1125-45be-ae32-0ce91334d021",
        "tool_variable": "weather_retrieve",
        "inputs": {
            "location": "London"
        },
        "query": "How's the weather today?"
    }
}
```

API Response：

```JSON
{
    "result": "City: London\nTemperature: 10°C\nRealFeel®: 8°C\nAir Quality: Poor\nWind Direction: ENE\nWind Speed: 8 km/h\nWind Gusts: 14 km/h\nPrecipitation: Light rain"
}
```

### Local debugging

Since Dify's cloud version can't access internal network API services, you can use Ngrok to expose your local API service endpoint to the public internet for cloud-based debugging of local code. The steps are:

1. Visit the Ngrok official website at [https://ngrok.com](https://ngrok.com/), register, and download the Ngrok file.

<figure><img src="../../../.gitbook/assets/spaces_CdDIVDY6AtAz028MFT4d_uploads_kLpE7vN8jg1KrzeCWZtn_download.webp" alt=""><figcaption></figcaption></figure>

2. After downloading, go to the download directory. Unzip the package and run the initialization script as instructed:

```
$ unzip /path/to/ngrok.zip
$ ./ngrok config add-authtoken 你的Token
```

3. Check the port of your local API service.

<figure><img src="../../../.gitbook/assets/spaces_CdDIVDY6AtAz028MFT4d_uploads_Z1SpULkGZ0xDBnSXOhC1_8000.webp" alt=""><figcaption></figcaption></figure>

Run the following command to start:

```
$ ./ngrok http [port number]
```

Upon successful startup, you'll see something like the following:

<figure><img src="../../../.gitbook/assets/spaces_CdDIVDY6AtAz028MFT4d_uploads_8EgAgdMcArHAaJJFEEWA_ngrock copy.jpg" alt=""><figcaption></figcaption></figure>

4. Find the 'Forwarding' address, like the sample domain `https://177e-159-223-41-52.ngrok-free.app`, and use it as your public domain.

* For example, to expose your locally running service, replace the example URL `http://127.0.0.1:8000/api/dify/receive` with `https://177e-159-223-41-52.ngrok-free.app/api/dify/receive`.

Now, this API endpoint is accessible publicly. You can configure this endpoint in Dify for local debugging. For the configuration steps, consult the appropriate documentation or guide.

### Deploy API extension with Cloudflare Workers

We recommend that you use Cloudflare Workers to deploy your API extension, because Cloudflare Workers can easily provide a public address and can be used for free.

[cloudflare-workers.md](cloudflare-workers.md "mention")
```

## File: en/guides/extension/code-based-extension/external-data-tool.md
```markdown
# External Data Tools

External data tools are used to fetch additional data from external sources after the end user submits data, and then assemble this data into prompts as additional context information for the LLM. Dify provides a default tool for external API calls, check [api-based-extension](../api-based-extension/ "mention") for details.

For developers deploying Dify locally, to meet more customized needs or to avoid developing an additional API Server, you can directly insert custom external data tool logic in the form of a plugin based on the Dify service. After extending custom tools, your custom tool options will be added to the dropdown list of tool types, and team members can use these custom tools to fetch external data.

## Quick Start

Here is an example of extending an external data tool for `Weather Search`, with the following steps:

1. Initialize the directory
2. Add frontend form specifications
3. Add implementation class
4. Preview the frontend interface
5. Debug the extension

### 1. **Initialize the Directory**

To add a custom type `Weather Search`, you need to create the relevant directory and files under `api/core/external_data_tool`.

```python
.
└── api
    └── core
        └── external_data_tool
            └── weather_search
                ├── __init__.py
                ├── weather_search.py
                └── schema.json
```

### 2. **Add Frontend Component Specifications**

* `schema.json`, which defines the frontend component specifications, detailed in [.](./ "mention")

```json
{
    "label": {
        "en-US": "Weather Search",
        "zh-Hans": "天气查询"
    },
    "form_schema": [
        {
            "type": "select",
            "label": {
                "en-US": "Temperature Unit",
                "zh-Hans": "温度单位"
            },
            "variable": "temperature_unit",
            "required": true,
            "options": [
                {
                    "label": {
                        "en-US": "Fahrenheit",
                        "zh-Hans": "华氏度"
                    },
                    "value": "fahrenheit"
                },
                {
                    "label": {
                        "en-US": "Centigrade",
                        "zh-Hans": "摄氏度"
                    },
                    "value": "centigrade"
                }
            ],
            "default": "centigrade",
            "placeholder": "Please select temperature unit"
        }
    ]
}
```

### 3. Add Implementation Class

`weather_search.py` code template, where you can implement the specific business logic.

{% hint style="warning" %}
Note: The class variable `name` must be the custom type name, consistent with the directory and file name, and must be unique.
{% endhint %}

```python
from typing import Optional

from core.external_data_tool.base import ExternalDataTool


class WeatherSearch(ExternalDataTool):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "weather_search"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user save the config.

        Example:
            .. code-block:: python
                config = {
                    "temperature_unit": "centigrade"
                }

        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """

        if not config.get('temperature_unit'):
            raise ValueError('temperature unit is required')

    def query(self, inputs: dict, query: Optional[str] = None) -> str:
        """
        Query the external data tool.

        :param inputs: user inputs
        :param query: the query of chat app
        :return: the tool query result
        """
        city = inputs.get('city')
        temperature_unit = self.config.get('temperature_unit')

        if temperature_unit == 'fahrenheit':
            return f'Weather in {city} is 32°F'
        else:
            return f'Weather in {city} is 0°C'
```

<!-- ### 4. **Preview the Frontend Interface**

Follow the above steps and run the service to see the newly added custom type. -->

<!-- ![](todo) -->

### 4. **Debug the Extension**

Now, you can select the custom `Weather Search` external data tool extension type in the Dify application orchestration interface for debugging.

## Implementation Class Template

```python
from typing import Optional

from core.external_data_tool.base import ExternalDataTool


class WeatherSearch(ExternalDataTool):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "weather_search"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user save the config.

        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """

        # implement your own logic here

    def query(self, inputs: dict, query: Optional[str] = None) -> str:
        """
        Query the external data tool.

        :param inputs: user inputs
        :param query: the query of chat app
        :return: the tool query result
        """
       
        # implement your own logic here
        return "your own data."
```

### Detailed Introduction to Implementation Class Development

### def validate_config

`schema.json` form validation method, called when the user clicks "Publish" to save the configuration.

* `config` form parameters
  * `{{variable}}` custom form variables

### def query

User-defined data query implementation, the returned result will be replaced into the specified variable.

* `inputs`: Variables passed by the end user
* `query`: Current conversation input content from the end user, a fixed parameter for conversational applications.
```

## File: en/guides/extension/code-based-extension/moderation.md
```markdown
# Sensitive Content Moderation

In addition to the system's built-in content moderation types, Dify also supports user-defined content moderation rules. This method is suitable for developers customizing their own private deployments. For instance, in an enterprise internal customer service setup, it may be required that users, while querying or customer service agents while responding, not only avoid entering words related to violence, sex, and illegal activities but also avoid specific terms forbidden by the enterprise or violating internally established moderation logic. Developers can extend custom content moderation rules at the code level in a private deployment of Dify.

## Quick Start

Here is an example of extending a `Cloud Service` content moderation type, with the steps as follows:

1. Initialize the directory
2. Add the frontend component definition file
3. Add the implementation class
4. Preview the frontend interface
5. Debug the extension

### 1. Initialize the Directory

To add a custom type `Cloud Service`, create the relevant directories and files under the `api/core/moderation` directory.

```Plain
.
└── api
    └── core
        └── moderation
            └── cloud_service
                ├── __init__.py
                ├── cloud_service.py
                └── schema.json
```

### 2. Add Frontend Component Specifications

* `schema.json`: This file defines the frontend component specifications. For details, see [.](./ "mention").

```json
{
    "label": {
        "en-US": "Cloud Service",
        "zh-Hans": "云服务"
    },
    "form_schema": [
        {
            "type": "select",
            "label": {
                "en-US": "Cloud Provider",
                "zh-Hans": "云厂商"
            },
            "variable": "cloud_provider",
            "required": true,
            "options": [
                {
                    "label": {
                        "en-US": "AWS",
                        "zh-Hans": "亚马逊"
                    },
                    "value": "AWS"
                },
                {
                    "label": {
                        "en-US": "Google Cloud",
                        "zh-Hans": "谷歌云"
                    },
                    "value": "GoogleCloud"
                },
                {
                    "label": {
                        "en-US": "Azure Cloud",
                        "zh-Hans": "微软云"
                    },
                    "value": "Azure"
                }
            ],
            "default": "GoogleCloud",
            "placeholder": ""
        },
        {
            "type": "text-input",
            "label": {
                "en-US": "API Endpoint",
                "zh-Hans": "API Endpoint"
            },
            "variable": "api_endpoint",
            "required": true,
            "max_length": 100,
            "default": "",
            "placeholder": "https://api.example.com"
        },
        {
            "type": "paragraph",
            "label": {
                "en-US": "API Key",
                "zh-Hans": "API Key"
            },
            "variable": "api_keys",
            "required": true,
            "default": "",
            "placeholder": "Paste your API key here"
        }
    ]
}
```

### 3. Add Implementation Class

`cloud_service.py` code template where you can implement specific business logic.

{% hint style="warning" %}
Note: The class variable name must be the same as the custom type name, matching the directory and file names, and must be unique.
{% endhint %}

```python
from core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult

class CloudServiceModeration(Moderation):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "cloud_service"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user saves the config.

        Example:
            .. code-block:: python
                config = {
                    "cloud_provider": "GoogleCloud",
                    "api_endpoint": "https://api.example.com",
                    "api_keys": "123456",
                    "inputs_config": {
                        "enabled": True,
                        "preset_response": "Your content violates our usage policy. Please revise and try again."
                    },
                    "outputs_config": {
                        "enabled": True,
                        "preset_response": "Your content violates our usage policy. Please revise and try again."
                    }
                }

        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """

        cls._validate_inputs_and_outputs_config(config, True)

        if not config.get("cloud_provider"):
            raise ValueError("cloud_provider is required")

        if not config.get("api_endpoint"):
            raise ValueError("api_endpoint is required")

        if not config.get("api_keys"):
            raise ValueError("api_keys is required")

    def moderation_for_inputs(self, inputs: dict, query: str = "") -> ModerationInputsResult:
        """
        Moderation for inputs.

        :param inputs: user inputs
        :param query: the query of chat app, there is empty if is completion app
        :return: the moderation result
        """
        flagged = False
        preset_response = ""

        if self.config['inputs_config']['enabled']:
            preset_response = self.config['inputs_config']['preset_response']

            if query:
                inputs['query__'] = query
            flagged = self._is_violated(inputs)

        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)
        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)

    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:
        """
        Moderation for outputs.

        :param text: the text of LLM response
        :return: the moderation result
        """
        flagged = False
        preset_response = ""

        if self.config['outputs_config']['enabled']:
            preset_response = self.config['outputs_config']['preset_response']

            flagged = self._is_violated({'text': text})

        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)
        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)

    def _is_violated(self, inputs: dict):
        """
        The main logic of moderation.

        :param inputs:
        :return: the moderation result
        """
        return False
```

<!-- ### 4. Preview Frontend Interface

Following the above steps, run the service to see the newly added custom type. -->

<!-- ![](todo) -->

### 4. Debug the Extension

At this point, you can select the custom `Cloud Service` content moderation extension type for debugging in the Dify application orchestration interface.

## Implementation Class Template

```python
from core.moderation.base import Moderation, ModerationAction, ModerationInputsResult, ModerationOutputsResult

class CloudServiceModeration(Moderation):
    """
    The name of custom type must be unique, keep the same with directory and file name.
    """
    name: str = "cloud_service"

    @classmethod
    def validate_config(cls, tenant_id: str, config: dict) -> None:
        """
        schema.json validation. It will be called when user saves the config.
        
        :param tenant_id: the id of workspace
        :param config: the variables of form config
        :return:
        """
        cls._validate_inputs_and_outputs_config(config, True)
        
        # implement your own logic here

    def moderation_for_inputs(self, inputs: dict, query: str = "") -> ModerationInputsResult:
        """
        Moderation for inputs.

        :param inputs: user inputs
        :param query: the query of chat app, there is empty if is completion app
        :return: the moderation result
        """
        flagged = False
        preset_response = ""
        
        # implement your own logic here
        
        # return ModerationInputsResult(flagged=flagged, action=ModerationAction.overridden, inputs=inputs, query=query)
        return ModerationInputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)

    def moderation_for_outputs(self, text: str) -> ModerationOutputsResult:
        """
        Moderation for outputs.

        :param text: the text of LLM response
        :return: the moderation result
        """
        flagged = False
        preset_response = ""
        
        # implement your own logic here

        # return ModerationOutputsResult(flagged=flagged, action=ModerationAction.overridden, text=text)
        return ModerationOutputsResult(flagged=flagged, action=ModerationAction.DIRECT_OUTPUT, preset_response=preset_response)
```

## Detailed Introduction to Implementation Class Development

### def validate\_config

The `schema.json` form validation method is called when the user clicks "Publish" to save the configuration.

* `config` form parameters
  * `{{variable}}` custom variable of the form
  * `inputs_config` input moderation preset response
    * `enabled` whether it is enabled
    * `preset_response` input preset response
  * `outputs_config` output moderation preset response
    * `enabled` whether it is enabled
    * `preset_response` output preset response

### def moderation\_for\_inputs

Input validation function

* `inputs`: values passed by the end user
* `query`: the current input content of the end user in a conversation, a fixed parameter for conversational applications.
* `ModerationInputsResult`
  * `flagged`: whether it violates the moderation rules
  * `action`: action to be taken
    * `direct_output`: directly output the preset response
    * `overridden`: override the passed variable values
  * `preset_response`: preset response (returned only when action=direct_output)
  * `inputs`: values passed by the end user, with key as the variable name and value as the variable value (returned only when action=overridden)
  * `query`: overridden current input content of the end user in a conversation, a fixed parameter for conversational applications (returned only when action=overridden)

### def moderation\_for\_outputs

Output validation function

* `text`: content output by the model
* `moderation_for_outputs`: output validation function
  * `text`: content of the LLM response. When the LLM output is streamed, this is the content in segments of 100 characters.
  * `ModerationOutputsResult`
    * `flagged`: whether it violates the moderation rules
    * `action`: action to be taken
      * `direct_output`: directly output the preset response
      * `overridden`: override the passed variable values
    * `preset_response`: preset response (returned only when action=direct_output)
    * `text`: overridden content of the LLM response (returned only when action=overridden).
```

## File: en/guides/extension/code-based-extension/README.md
```markdown
# Code Based Extensions

For developers deploying Dify locally, if you want to implement extension capabilities without rewriting an API service, you can use code extensions. This allows you to extend or enhance the functionality of the program in code form (i.e., plugin capability) without disrupting the original code logic of Dify. It follows certain interfaces or specifications to achieve compatibility and plug-and-play capability with the main program. Currently, Dify offers two types of code extensions:

* Adding a new type of external data tool [External Data Tool](https://docs.dify.ai/guides/extension/api-based-extension/external-data-tool)
* Extending sensitive content moderation strategies [Moderation](https://docs.dify.ai/guides/extension/api-based-extension/moderation)

Based on the above functionalities, you can achieve horizontal expansion by following the code-level interface specifications. If you are willing to contribute your extensions to us, we warmly welcome you to submit a PR to Dify.

## Frontend Component Specification Definition

The frontend styles of code extensions are defined through `schema.json`:

* label: Custom type name, supporting system language switching
* form_schema: List of form contents
  * type: Component type
    * select: Dropdown options
    * text-input: Text
    * paragraph: Paragraph
  * label: Component name, supporting system language switching
  * variable: Variable name
  * required: Whether it is required
  * default: Default value
  * placeholder: Component hint content
  * options: Exclusive property for the "select" component, defining the dropdown contents
    * label: Dropdown name, supporting system language switching
    * value: Dropdown option value
  * max_length: Exclusive property for the "text-input" component, maximum length

### Template Example

```json
{
    "label": {
        "en-US": "Cloud Service",
        "zh-Hans": "云服务"
    },
    "form_schema": [
        {
            "type": "select",
            "label": {
                "en-US": "Cloud Provider",
                "zh-Hans": "云厂商"
            },
            "variable": "cloud_provider",
            "required": true,
            "options": [
                {
                    "label": {
                        "en-US": "AWS",
                        "zh-Hans": "亚马逊"
                    },
                    "value": "AWS"
                },
                {
                    "label": {
                        "en-US": "Google Cloud",
                        "zh-Hans": "谷歌云"
                    },
                    "value": "GoogleCloud"
                },
                {
                    "label": {
                        "en-US": "Azure Cloud",
                        "zh-Hans": "微软云"
                    },
                    "value": "Azure"
                }
            ],
            "default": "GoogleCloud",
            "placeholder": ""
        },
        {
            "type": "text-input",
            "label": {
                "en-US": "API Endpoint",
                "zh-Hans": "API Endpoint"
            },
            "variable": "api_endpoint",
            "required": true,
            "max_length": 100,
            "default": "",
            "placeholder": "https://api.example.com"
        },
        {
            "type": "paragraph",
            "label": {
                "en-US": "API Key",
                "zh-Hans": "API Key"
            },
            "variable": "api_keys",
            "required": true,
            "default": "",
            "placeholder": "Paste your API key here"
        }
    ]
}
```
```

## File: en/guides/extension/README.md
```markdown
# Extension

In the process of creating AI applications, developers face constantly changing business needs and complex technical challenges. Effectively leveraging extension capabilities can not only enhance the flexibility and functionality of applications but also ensure the security and compliance of enterprise data. Dify offers the following two methods of extension:

{% content-ref url="api-based-extension/" %}
[api-based-extension](api-based-extension/)
{% endcontent-ref %}

{% content-ref url="code-based-extension/" %}
[code-based-extension](code-based-extension/)
{% endcontent-ref %}
```

## File: en/guides/knowledge-base/create-knowledge-and-upload-documents/1.-import-text-data/1.1-import-data-from-notion.md
```markdown
# 1.1 Import Data from Notion

The Dify knowledge base supports importing data from Notion with subsequent automatic synchronization.

### Authorization Verification

1. When creating a dataset and selecting the data source, click **Sync from Notion Content -- Bind Now** and follow the prompts to complete the authorization verification.
2. Alternatively, you can go to **Settings -- Data Sources -- Add Data Source**, click on the Notion source **Bind**, and complete the authorization verification.

![Binding Notion](https://assets-docs.dify.ai/2024/12/f1d5bcdcfbd57407e0bce1597df4daad.png)

### Importing Notion Data

After completing the authorization verification, go to the create dataset page, click **Sync from Notion Content**, and select the authorized pages you need to import.

<figure><img src="https://assets-docs.dify.ai/2025/01/ac130faeb40a59662c2f63b9680d061e.png" alt=""><figcaption><p>Import from notion</p></figcaption></figure>

### Chunking and Cleaning

Next, choose a [chunking mode](1.1-import-data-from-notion.md#chunking-and-cleaning) and [indexing method](../3.-select-the-indexing-method-and-retrieval-setting.md) for your knowledge base, then save it and wait for the automatically processing. Dify not only supports importing standard Notion pages but can also consolidate and save page attributes from database-type pages.

_**Note: images and files cannot be imported, and data from tables will be converted to text.**_

<figure><img src="https://assets-docs.dify.ai/2024/12/ab1b1aa690adad153cac0a321b6b7585.png" alt=""><figcaption><p>Preview of Chunked Results for Notion Page</p></figcaption></figure>

### Synchronizing Notion Data

If your Notion content has been updated, you can sync the changes by clicking the **Sync** button for the corresponding page in the document list of your knowledge base. Syncing involves an embedding process, which will consume tokens from your embedding model.

![](https://assets-docs.dify.ai/2024/12/af7cabd98c3aac392819d9041cc408de.png)

### Integration Configuration Method for Community Edition Notion

Notion offers two integration options: **internal integration** and **public integration**. For more details on the differences between these two methods, please refer to the [official Notion documentation](https://developers.notion.com/docs/authorization).

#### **1. Using Internal Integration**

First, create an integration in the integration settings page [Create Integration](https://www.notion.so/my-integrations). By default, all integrations start as internal integrations; internal integrations will be associated with the workspace you choose, so you need to be the workspace owner to create an integration.

Specific steps:

Click the **New integration** button. The type is **Internal** by default (cannot be modified). Select the associated space, enter the integration name, upload a logo, and click **Submit** to create the integration successfully.

![](https://assets-docs.dify.ai/2024/12/223a190a2e61e488fb821c5e3f0e9883.png)

After creating the integration, you can update its settings as needed under the Capabilities tab and click the **Show** button under Secrets to copy the secrets.

![](https://assets-docs.dify.ai/2024/12/83c1f1699ec4165b56ae8fea304d35f5.png)

After copying, go back to the Dify source code, and configure the relevant environment variables in the **.env** file. The environment variables are as follows:

```
NOTION_INTEGRATION_TYPE = internal or NOTION_INTEGRATION_TYPE = public
NOTION_INTERNAL_SECRET=you-internal-secret
```

#### **Using Public Integration**

**You need to upgrade the internal integration to a public integration.** Navigate to the Distribution page of the integration, and toggle the switch to make the integration public. When switching to the public setting, you need to fill in additional information in the Organization Information form below, including your company name, website, and redirect URL, then click the **Submit** button.

![](https://assets-docs.dify.ai/2024/12/c37759d54f8e72685e1cacffa23d2e9f.png)

After successfully making the integration public on the integration settings page, you will be able to access the integration key in the Keys tab:

![](https://assets-docs.dify.ai/2024/12/c4af8b95298c6b86d80406bec09c31e7.png)

Go back to the Dify source code, and configure the relevant environment variables in the **.env** file. The environment variables are as follows:

```
NOTION_INTEGRATION_TYPE=public
NOTION_CLIENT_SECRET=your-client-secret
NOTION_CLIENT_ID=your-client-id
```

After configuration, you can operate the Notion data import and synchronization functions in the dataset.
```

## File: en/guides/knowledge-base/create-knowledge-and-upload-documents/1.-import-text-data/README.md
```markdown
# 1. Import Text Data

Click on Knowledge in the main navigation bar of Dify. On this page, you can see your existing knowledge bases. Click **Create Knowledge** to enter the setup wizard. The Knowledge supports the import of the following two online data:

Click **Knowledge** in the top navigation bar of the Dify, then select **Create Knowledge**. You can upload documents to the knowledge or importing online data to it.

### Upload Local Files

Drag and drop or select files to upload. The number of files allowed for **batch upload** depends on your [subscription plan](https://dify.ai/pricing).

**Limitations for uploading documents:**

* The upload size limit for a single document is 15MB;
* Different [subscription plans](https://dify.ai/pricing) for the SaaS version limit **batch upload numbers, total document uploads, and vector storage**;

<figure><img src="https://assets-docs.dify.ai/2025/01/22064cb61356e4c005c4072d5d066cf6.png" alt=""><figcaption><p>Create knowledge</p></figcaption></figure>

### Import From Online Data Source

When creating a **Knowledge**, you can import data from online sources. The knowledge supports the following two types of online data:

{% content-ref url="1.1-import-data-from-notion.md" %}
[1.1-import-data-from-notion.md](1.1-import-data-from-notion.md)
{% endcontent-ref %}

{% content-ref url="../../sync-from-website.md" %}
[sync-from-website.md](../../sync-from-website.md)
{% endcontent-ref %}

If a knowledge base is set up to use online data, you won’t be able to add local documents later or switch it to a local file-based mode. This prevents a single knowledge base from mixing multiple data sources, avoiding management complications.

### Adding Data Later

If you haven’t prepared your documents or other content yet, simply create an empty knowledge first. You can then upload local files or import online data whenever you’re ready.
```

## File: en/guides/knowledge-base/create-knowledge-and-upload-documents/2.-choose-a-chunk-mode.md
```markdown
# 2. Choose a Chunk Mode

After uploading content to the knowledge base, the next step is chunking and data cleaning. **This stage involves content preprocessing and structuring, where long texts are divided into multiple smaller chunks.**

<details>

<summary>What is the Chunking and Cleaning Strategy?</summary>

* **Chunking**

Due to the limited context window of LLMs, it is hard to process and transmit the entire knowledge base content at once. Instead, long texts in documents must be splited into content chunks. Even though some advanced models now support uploading complete documents, studies show that retrieval efficiency remains weaker compared to querying individual content chunks.

The ability of an LLM to accurately answer questions based on the knowledge base depends on the retrieval effectiveness of content chunks. This process is similar to finding key chapters in a manual for quick answers, without the need to analyze the entire document line by line. After chunking, the knowledge base uses a Top-K retrieval method to identify the most relevant content chunks based on user queries, supplementing key information to enhance the accuracy of responses.

The size of the content chunks is critical during semantic matching between queries and chunks. Properly sized chunks enable the model to locate the most relevant content accurately while minimizing noise. Overly large or small chunks can negatively impact retrieval effectiveness.

Dify offers two chunking modes: **General Mode** and **Parent-child Mode**, tailored to different document structures and application scenarios. These modes are designed to meet varying requirements for retrieval efficiency and accuracy in knowledge bases.

* **Cleaning**

To ensure effective text retrieval, it’s essential to clean the data before uploading it to the knowledge base. For instance, meaningless characters or empty lines can affect the quality of query responses and should be removed. Dify provides built-in automatic cleaning strategies. For more information, see [ETL](https://docs.dify.ai/guides/knowledge-base/create-knowledge-and-upload-documents#optional-etl-configuration).

</details>

Whether an LLM can accurately answer knowledge base queries depends on how effectively the system retrieves relevant content chunks. High-relevance chunks are crucial for AI applications to produce precise and comprehensive responses.

In an AI customer chatbot scenario, for example, directing the LLM to the key content chunks in a tool manual is sufficient to quickly answer user questions—no need to repeatedly analyze the entire document. This approach saves tokens during the analysis phase while boosting the overall quality of the AI-generated answers.

### Chunk Mode

The knowledge base supports two chunking modes: **General Mode** and **Parent-child Mode**. If you are creating a knowledge base for the first time, it is recommended to choose Parent-Child Mode.

{% hint style="info" %}
**Please note**: The original **“Automatic Chunking and Cleaning”** mode has been automatically updated to **“General”** mode. No changes are required, and you can continue to use the default setting.&#x20;

Once the chunk mode is selected and the knowledge base is created, it cannot be changed later. Any new documents added to the knowledge base will follow the same chunking strategy.
{% endhint %}

<figure><img src="https://assets-docs.dify.ai/2024/12/b3052a6aae6e4d0e5701dde3a859e326.png" alt=""><figcaption><p>General mode and Parent-child mode</p></figcaption></figure>

#### General Mode

Content will be divided into independent chunks. When a user submits a query, the system automatically calculates the relevance between the chunks and the query keywords. The top-ranked chunks are then retrieved and sent to the LLM for processing the answers.

In this mode, you need to manually define text chunking rules based on different document formats or specific scenario requirements. Refer to the following configuration options for guidance:

*   **Chunk identifier**, The default value is  `\n\n`, which means the text will be chunked by paragraphs. You can customize chunking rules using [regex](https://regexr.com/). The system will automatically execute chunking whenever it detects the specified delimiter. For example,  `\n` means chunk the text by sentences.



    <figure><img src="https://assets-docs.dify.ai/2024/12/2c19c1c1a0446c00e3c07d6f4c8968e4.png" alt=""><figcaption><p>Chunk results of different identifier syntaxes</p></figcaption></figure>
* **Maximum chunk length:** Specifies the maximum number of text characters allowed per chunk. If this limit is exceeded, the system will automatically enforce chunking. The default value is 500 tokens, with a maximum chunk length of 4000 tokens.
* **Overlapping chunk length**: When data is chunked, there is a certain amount of overlap between chunks. This overlap can help to improve the retention of information and the accuracy of analysis, and enhance retrieval effects. It is recommended that the setting be 10-25% of the chunk length Tokens.&#x20;

**Text Preprocessing Rules**, Text preprocessing rules help filter out irrelevant content from the knowledge base. The following options are available:

* Replace consecutive spaces, newline characters, and tabs
* Remove all URLs and email addresses

Once configured, click **“Preview Chunk”** to see the chunking results. You can view the character count for each chunk. If you modify the chunking rules, click the button again to view the latest generated text chunks.

If multiple documents are uploaded in bulk, you can switch between them by clicking the document titles at the top to review the chunk results for other documents.

<figure><img src="https://assets-docs.dify.ai/2024/12/b3ec2ce860550563234ca22967abdd17.png" alt=""><figcaption><p>General mode</p></figcaption></figure>

After setting the chunking rules, the next step is to specify the indexing method. General mode supports **High-Quality Indexing** **Method** and **Economical Indexing Method**. For more details, please refer to [Set up the Indexing Method](3.-select-the-indexing-method-and-retrieval-setting.md).

#### Parent-child Mode

Compared to **General mode**, **Parent-child mode** uses a two-tier data structure that balances precise retrieval with comprehensive context, combining accurate matching and richer contextual information.

In this mode, parent chunks (e.g., paragraphs) serve as larger text units to provide context, while child chunks (e.g., sentences) focus on pinpoint retrieval. The system searches child chunks first to ensure relevance, then fetches the corresponding parent chunk to supply the full context—thereby guaranteeing both accuracy and a complete background in the final response. You can customize how parent and child chunks are split by configuring delimiters and maximum chunk lengths.

For example, in an AI-powered customer chatbot case, a user query can be mapped to a specific sentence within a support document. The paragraph or chapter containing that sentence is then provided to the LLM, filling in the overall context so the answer is more precise.

Its fundamental mechanism includes:

* **Query Matching with Child Chunks:**
  * Small, focused pieces of information, often as concise as a single sentence within a paragraph, are used to match the user's query.
  * These child chunks enable precise and relevant initial retrieval.
* **Contextual Enrichment with Parent Chunks:**
  * Larger, encompassing sections—such as a paragraph, a section, or even an entire document—that include the matched child chunks are then retrieved.
  * These parent chunks provide comprehensive context for the Language Model (LLM).

<figure><img src="https://assets-docs.dify.ai/2024/12/3e6820c10bd7c5f6884930e3a14e7b66.png" alt="" width="375"><figcaption><p>Parent-child mode schematic</p></figcaption></figure>

In this mode, you need to manually configure separate chunking rules for both parent and child chunks based on different document formats or specific scenario requirements.

**Parent Chunk**

The parent chunk settings offer the following options:

*   **Paragraph**

    This mode splits the text in to paragraphs based on delimiters and the maximum chunk length, using the split text as the parent chunk for retrieval. Each paragraph is treated as a parent chunk, suitable for documents with large volumes of text, clear content, and relatively independent paragraphs. The following settings are supported:

    * **Chunk Delimiter**: The default value is `\n\n`, which chunks text by paragraphs. You can customize the chunking rules using [regex](https://regexr.com/). The system automatically chunks the text whenever the specified delimiter appears.
    * **Maximum chunk length:** Specifies the maximum number of text characters allowed per chunk. If this limit is exceeded, the system will automatically enforce chunking. The default value is 500 tokens, with a maximum chunk length of 4000 tokens.
*   **Full Doc**

    Instead of splitting the text into paragraphs, the entire document is used as the parent chunk and retrieved directly. For performance reasons, only the first 10,000 tokens of the text are retained. This setting is ideal for smaller documents where paragraphs are interrelated, requiring full doc retrieval.

<figure><img src="https://assets-docs.dify.ai/2024/12/e3814336710d445a99a9ded3d251622b.png" alt=""><figcaption><p>The difference between p<strong>aragraph and Full doc</strong></p></figcaption></figure>

**Child Chunk**

Child chunks are derived from parent chunks by splitting them based on delimiter rules. They are used to identify and match the most relevant and direct information to the query keywords. When using the default child chunking rules, the segmentation typically results in the following:

* If the parent chunk is a paragraph, child chunks correspond to individual sentences within each paragraph.
* If the parent chunk is the full document, child chunks correspond to the individual sentences within the document.

You can configure the following chunk settings:

* **Chunk Delimiter**: The default value is `\n`, which chunks text by sentences. You can customize the chunking rules using [regex](https://regexr.com/). The system automatically chunks the text whenever the specified delimiter appears.
* **Maximum chunk length:** Specifies the maximum number of text characters allowed per chunk. If this limit is exceeded, the system will automatically enforce chunking. The default value is 200 tokens, with a maximum chunk length of 4000 tokens.

You can also use **text preprocessing rules** to filter out irrelevant content from the knowledge base:

* Replace consecutive spaces, newline characters, and tabs
* Remove all URLs and email addresses

After completing the configuration, click **“Preview Chunks”** to view the results. You can see the total character count of the parent chunk.&#x20;

Once configured, click **“Preview Chunk”** to see the chunking results. You can see the total character count of the parent chunk. Characters highlighted in blue represent child chunks, and the character count for the current child chunk is also displayed for reference.

If you modify the chunking rules, click the button again to view the latest generated text chunks.

If multiple documents are uploaded in bulk, you can switch between them by clicking the document titles at the top to review the chunk results for other documents.

<figure><img src="https://assets-docs.dify.ai/2024/12/af5c9a68f85120a6ea687bf93ecfb80a.png" alt=""><figcaption><p>Parent-child mode</p></figcaption></figure>

To ensure accurate content retrieval, the Parent-child chunk mode only supports the [High-Quality Indexing](3.-select-the-indexing-method-and-retrieval-setting.md#high-quality).

### What's the Difference Between Two Modes？

The difference between the two modes lies in the structure of the content chunks. **General Mode** produces multiple independent content chunks, whereas **Parent-child Mode** uses a two-layer chunking approach. In this way, a single parent chunk (e.g., the entire document or a paragraph) contains multiple child chunks (e.g., sentences).

Different chunking methods influence how effectively the LLM can search the knowledge base. When used on the same document, Parent-child Retrieval provides more comprehensive context while maintaining high precision, making it significantly more effective than the traditional single-layer approach.

<figure><img src="https://assets-docs.dify.ai/2024/12/0b614c6a07c6ea2151fe17d85ce6a1d1.png" alt=""><figcaption><p>The Difference Between General Mode and Parent-child Mode</p></figcaption></figure>

### Reference

After choosing the chunking mode, refer to the following documentation to configure the indexing method and retrieval method and finis the creation of your knowledge base.

{% content-ref url="3.-select-the-indexing-method-and-retrieval-setting.md" %}
[3.-select-the-indexing-method-and-retrieval-setting.md](3.-select-the-indexing-method-and-retrieval-setting.md)
{% endcontent-ref %}
```

## File: en/guides/knowledge-base/create-knowledge-and-upload-documents/3.-select-the-indexing-method-and-retrieval-setting.md
```markdown
# 3. Select the Indexing Method and Retrieval Setting

After selecting the chunking mode, the next step is to define the indexing method for structured content.&#x20;

### Setting the Indexing Method

Similar to the search engines use efficient indexing algorithms to match search results most relevant to user queries, the selected indexing method directly impacts the retrieval efficiency of the LLM and the accuracy of its responses to knowledge base content.

The knowledge base offers two indexing methods: **High-Quality** and **Economical**, each with different retrieval setting options:

{% hint style="info" %}
**Note**: The original **Q\&A mode (Available only in the Community Edition)** is now an optional feature under the High-Quality Indexing Method.
{% endhint %}

{% tabs %}
{% tab title="High-Quality" %}
### High Quality

In High-Quality Mode, the Embedding model converts text chunks into numerical vectors, enabling efficient compression and storage of large volumes of textual information. **This allows for more precise matching between user queries and text.**

Once the text chunks are vectorized and stored in the database, an effective retrieval method is required to fetch the chunks that match user queries. The High-Quality indexing method offers three retrieval settings: vector retrieval, full-text retrieval, and hybrid retrieval. For more details on retrieval settings, please check ["Retrieval Settings"](3.-select-the-indexing-method-and-retrieval-setting.md#id-4-retrieval-settings).

After selecting **High Quality** mode, the indexing method for the knowledge base cannot be downgraded to **Economical** mode later. If you need to switch, it is recommended to create a new knowledge base and select the desired indexing method.

<figure><img src="https://assets-docs.dify.ai/2024/12/51442c8fcd05479616a3dd8279a4853a.png" alt="" width="563"><figcaption><p>High Quality Indexing Mode</p></figcaption></figure>

### Enable Q\&A Mode (Optional, Community Edition Only)

When this mode is enabled, the system segments the uploaded text and automatically generates Q\&A pairs for each segment after summarizing its content.

Compared with the common **Q to P** strategy (user questions matched with text paragraphs), the Q\&A mode uses a **Q to Q** strategy (questions matched with questions).

This approach is particularly effective because the text in FAQ documents **is often written in natural language with complete grammatical structures**.&#x20;

> The **Q to Q** strategy makes the matching between questions and answers clearer and better supports scenarios with high-frequency or highly similar questions.

<figure><img src="https://assets-docs.dify.ai/2024/12/70960a237d4f5eaed2dbf46a2cca2bf7.png" alt=""><figcaption><p>Q&#x26;A Chunk</p></figcaption></figure>

When a user asks a question, the system identifies the most similar question and returns the corresponding chunk as the answer. This approach is more precise, as it directly matches the user’s query, helping them retrieve the exact information they need.

<figure><img src="https://assets-docs.dify.ai/2024/12/8745ccabff56290eae329a9d3592f745.png" alt=""><figcaption><p>Difference between Q to P and Q to Q indexing method</p></figcaption></figure>
{% endtab %}

{% tab title="Economical" %}
### Economical

Using 10 keywords per chunk for retrieval, no tokens are consumed at the expense of reduced retrieval accuracy. For the retrieved blocks, only the inverted index method is provided to select the most relevant blocks. For more details, please refer to [Inverted Index](3.-select-the-indexing-method-and-retrieval-setting.md#inverted-index).

If the performance of the economical indexing method does not meet your expectations, you can upgrade to the High-Quality indexing method in the Knowledge settings page.

<figure><img src="https://assets-docs.dify.ai/2024/12/3b86e6b484da39452c164cb6372a7242.png" alt="" width="375"><figcaption><p>Economical mode</p></figcaption></figure>
{% endtab %}
{% endtabs %}

### Setting the Retrieval Setting

Once the knowledge base receives a user query, it searches existing documents according to preset retrieval methods and extracts highly relevant content chunks. These chunks provide essential contextual for the LLM, ultimately affecting the accuracy and credibility of its answers.

Common retrieval methods include:

1. Semantic Retrieval based on vector similarity—where text chunks and queries are converted into vectors and matched via similarity scoring.
2. Keyword Matching using an inverted index (a standard search engine technique). Both methods are supported in Dify’s knowledge base.&#x20;

Both retrieval methods are supported in Dify’s knowledge base. The specific retrieval options available depend on the chosen indexing method.

{% tabs %}
{% tab title="High-Quality" %}
#### High Quality

In the **High-Quality** Indexing Method, Dify offers three retrieval settings: **Vector Search, Full-Text Search, and Hybrid Search**.

<figure><img src="https://assets-docs.dify.ai/2024/12/9b02fc353324221cc91f185a350775b6.png" alt=""><figcaption><p>Retrieval Settings</p></figcaption></figure>

#### Vector Search

**Definition**: Vectorize the user’s question to generate a query vector, then compare it with the corresponding text vectors in the knowledge base to find the nearest chunks.

<figure><img src="https://assets-docs.dify.ai/2024/12/620044faa47a5037f85b32a27a56fce5.png" alt="" width="375"><figcaption><p>Vector search settings</p></figcaption></figure>

**Vector Search Settings：**

**Rerank Model**: Disabled by default. When enabled, a third-party Rerank model will sort the text chunks returned by Vector Search to optimize results. This helps the LLM access more precise information and improve output quality. Before enabling this option, go to **Settings** → **Model Providers** and configure the Rerank model’s API key.

> Enabling this feature will consume tokens from the Rerank model. For more details, refer to the associated model’s pricing page.

**TopK**: Determines how many text chunks, deemed most similar to the user’s query, are retrieved. It also automatically adjusts the number of chunks based on the chosen model’s context window. The default value is **3**, and higher numbers will recall more text chunks.

**Score Threshold**: Sets the minimum similarity score required for a chunk to be retrieved. Only chunks exceeding this score are retrieved. The default value is **0.5**. Higher thresholds demand greater similarity and thus result in fewer chunks being retrieved.

> The TopK and Score configurations are only effective during the Rerank phase. Therefore, to apply either of these settings, it is necessary to add and enable a Rerank model.

#### Full-Text Search

**Definition:** Indexing all terms in the document, allowing users to query any terms and return text fragments containing those terms.

<figure><img src="https://assets-docs.dify.ai/2024/12/513bff1ca38ec746b3246502b0311b39.png" alt="" width="375"><figcaption><p>Full-Text Search Settings</p></figcaption></figure>

**Rerank Model**: Disabled by default. When enabled, a third-party Rerank model will sort the text chunks returned by Full-Text Search to optimize results. This helps the LLM access more precise information and improve output quality. Before enabling this option, go to **Settings** → **Model Providers** and configure the Rerank model’s API key.

> Enabling this feature will consume tokens from the Rerank model. For more details, refer to the associated model’s pricing page.

**TopK**: Determines how many text chunks, deemed most similar to the user’s query, are retrieved. It also automatically adjusts the number of chunks based on the chosen model’s context window. The default value is **3**, and higher numbers will recall more text chunks.

**Score Threshold**: Sets the minimum similarity score required for a chunk to be retrieved. Only chunks exceeding this score are retrieved. The default value is **0.5**. Higher thresholds demand greater similarity and thus result in fewer chunks being retrieved.

> The TopK and Score configurations are only effective during the Rerank phase. Therefore, to apply either of these settings, it is necessary to add and enable a Rerank model.

#### **Hybrid Search**

**Definition**: This process combines full-text search and vector search, performing both simultaneously. It includes a reordering step to select the best-matching results from both search outcomes based on the user’s query.

<figure><img src="https://assets-docs.dify.ai/2024/12/bd2621bfe8a1a8e21fca0743ec495a9e.png" alt="" width="375"><figcaption><p>Hybrid Retrieval Setting</p></figcaption></figure>

In this mode, you can specify **"Weight settings"** without needing to configure the Rerank model API, or enable **Rerank model** for retrieval.

*   **Weight Settings**

    This feature enables users to set custom weights for semantic priority and keyword priority. Keyword search refers to performing a full-text search within the knowledge base, while semantic search involves vector search within the knowledge base.

    *   **Semantic Value of 1**

        This activates only the semantic search mode. Utilizing embedding models, even if the exact terms from the query do not appear in the knowledge base, the search can delve deeper by calculating vector distances, thus returning relevant content. Additionally, when dealing with multilingual content, semantic search can capture meaning across different languages, providing more accurate cross-language search results.
    *   **Keyword Value of 1**

        This activates only the keyword search mode. It performs a full match against the input text in the knowledge base, suitable for scenarios where the user knows the exact information or terminology. This approach consumes fewer computational resources and is ideal for quick searches within a large document knowledge base.
    *   **Custom Keyword and Semantic Weights**

        In addition to enabling only semantic search or keyword search, we provide flexible custom weight settings. You can continuously adjust the weights of the two methods to identify the optimal weight ratio that suits your business scenario.

***

*   **Rerank Model**

    Disabled by default. When enabled, a third-party Rerank model will sort the text chunks returned by Hybrid Search to optimize results. This helps the LLM access more precise information and improve output quality. Before enabling this option, go to **Settings** → **Model Providers** and configure the Rerank model’s API key.

    > Enabling this feature will consume tokens from the Rerank model. For more details, refer to the associated model’s pricing page.

The **"Weight Settings"** and **"Rerank Model"** settings support the following options:

**TopK**: Determines how many text chunks, deemed most similar to the user’s query, are retrieved. It also automatically adjusts the number of chunks based on the chosen model’s context window. The default value is **3**, and higher numbers will recall more text chunks.

**Score Threshold**: Sets the minimum similarity score required for a chunk to be retrieved. Only chunks exceeding this score are retrieved. The default value is **0.5**. Higher thresholds demand greater similarity and thus result in fewer chunks being retrieved.
{% endtab %}

{% tab title="Economical" %}
#### Economical

In **Economical Indexing** mode, only the inverted index approach is available. An inverted index is a data structure designed for fast keyword retrieval within documents, commonly used in online search engines. Inverted indexing supports only the **TopK** setting.

**TopK:** Determines how many text chunks, deemed most similar to the user’s query, are retrieved. It also automatically adjusts the number of chunks based on the chosen model’s context window. The default value is **3**, and higher numbers will recall more text chunks.

<figure><img src="https://assets-docs.dify.ai/2024/12/b417cd028131d34779993fbcbb8dbdd7.png" alt="" width="375"><figcaption><p>Inverted index</p></figcaption></figure>
{% endtab %}
{% endtabs %}

***

### Reference

After specifying the retrieval settings, you can refer to the following documentation to review how keywords match with content chunks in different scenarios.

{% content-ref url="../retrieval-test-and-citation.md" %}
[retrieval-test-and-citation.md](../retrieval-test-and-citation.md)
{% endcontent-ref %}
```

## File: en/guides/knowledge-base/knowledge-and-documents-maintenance/maintain-documents.md
```markdown
# Maintain Documents

## Manage Documentations in the Knowledge Base

### Adding Documentations

A knowledge base is a collection of documents. Documents can be uploaded by developers or operators, or synchronized from other data sources. Each document in the knowledge base corresponds to a file in its data source—for example, a Notion document or an online webpage.

To upload a new document to an existing knowledge base, go to **Knowledge Base** > **Documents** and click **Add File**.

<figure><img src="https://assets-docs.dify.ai/2024/12/424ab491aaebe09b490a36d26c9fa8da.png" alt=""><figcaption><p>Uploading the new documentation on Knowledge Base</p></figcaption></figure>

### Disable / Archive / Delete document

**Enable**: Documents that are currently in normal status can be edited and retrieved in the knowledge base. If a document has been disabled, you can re-enable it. For archived documents, you must first unarchive them before re-enabling.

**Disable**: If you don’t want a document to be indexed during use, toggle off the blue switch on the right side of the document to disable it. A disabled document can still be edited or modified.

**Archive**: For older documents that are no longer in use but you don’t want to delete, you can archive them. Archived documents can only be viewed or deleted and cannot be edited. You can archive a document from the Knowledge Base’s **Document List** by clicking the **Archive** button, or within the document’s details page. Archiving can be undone.

**Delete**: ⚠️ Dangerous Option. For incorrect documents or clearly ambiguous content, select Delete from the menu on the right side of the document. Deleted content cannot be restored, so proceed with caution.

> The above options all support batch operations after multiple documents are selected.

<figure><img src="https://assets-docs.dify.ai/2024/12/5e0e64859a1ac51602d167ec55ef9350.png" alt=""><figcaption><p>Batch file Operations</p></figcaption></figure>

**Note:**&#x20;

If there are some documents in your knowledge base that haven’t been updated or retrieved for a while, the system will disable inactive documents to ensure optimal performance.&#x20;

* For Sandbox users, the "inactive document disable period" is **after 7 days**.&#x20;
* For Professional and Team users, it is **after 30 days**. You can revert these documents and continue using them at any time by clicking the "Enable" button in the knowledge base.

You can revert these disable documents and continue using them at any time by clicking the "Enable" button in the knowledge base. Paid users are provided with **one-click revert** function.

<figure><img src="https://assets-docs.dify.ai/2024/12/bf6485b17aec716741eb65e307c2274c.png" alt=""><figcaption><p>O<strong>ne-click revert</strong></p></figcaption></figure>

***

## Managing Text Chunks

### Viewing Text Chunks

In the knowledge base, each uploaded document is stored as text chunks. By clicking on the document title, you can view the list of chunks and their specific text content on the details page. Each page displays 10 chunks by default, but you can change the number of chunks shown per page at the bottom of the web.

Only the first two lines of each content chunk are visible in the preview. If you need to see the full text within a chunk, click the “Expand Chunk” button for a complete view.

<figure><img src="https://assets-docs.dify.ai/2024/12/86cc80f17fab1eea75aa73ee681e4663.png" alt=""><figcaption><p>Expand text chunks</p></figcaption></figure>

You can quickly view all enabled or disabled documents using the filter.

<figure><img src="https://assets-docs.dify.ai/2025/01/47ef07319175a102bfd1692dcc6cac9b.png" alt=""><figcaption><p>Filter text chunks</p></figcaption></figure>

Different [chunking modes](../create-knowledge-and-upload-documents/2.-choose-a-chunk-mode.md) correspond to different text chunking preview methods:

{% tabs %}
{% tab title="General Mode" %}
**General Mode**

Chunks of text in [General mode](../create-knowledge-and-upload-documents.md#general) are independent blocks. If you want to view the complete content of a chunk, click the **full-screen** icon.

<figure><img src="https://assets-docs.dify.ai/2024/12/c37a1a247092cda9433a10243543698f.png" alt=""><figcaption><p>Full screen viewing</p></figcaption></figure>

Tap the document title at the top to quickly switch to other documents in the knowledge base.

<figure><img src="https://assets-docs.dify.ai/2024/12/4422286c6d254e13c1ab59b147f0ffbf.png" alt=""><figcaption><p>General mode - text chunking</p></figcaption></figure>
{% endtab %}

{% tab title="Parent-child Mode" %}
**Parent-child Mode**

In[ Parent-child](maintain-documents.md#parent-child-chunking-mode) mode, content is divided into parent chunks and child chunks.

*   **Parent chunks**

    After selecting a document in the knowledge base, you’ll first see the parent chunk content. Parent chunks can be split by **Paragraph** or **Full Doc**, offering a more comprehensive context. The illustration below shows how the text preview differs between these split modes.

<figure><img src="https://assets-docs.dify.ai/2024/12/b3961da2536dc922496ef6646315b9f4.png" alt=""><figcaption><p>Difference in preview between paragraph and full doc</p></figcaption></figure>

*   **Child chunks**

    Child chunks are usually sentences (smaller text blocks) within a paragraph, containing more detailed information. Each chunk displays its character count and the number of times it has been retrieved. Tapping **“Child Chunks”** reveals more details. If you want to see the full content of a chunk, click the full-screen icon in the top-right corner of that chunk to enter full-screen reading mode.

<figure><img src="https://assets-docs.dify.ai/2024/12/c0776f91e155bb1c961ae255bb98f39e.png" alt=""><figcaption><p>Parent-child mode - text chunking</p></figcaption></figure>
{% endtab %}

{% tab title="Q&A Mode (Community Edition Only)" %}
**Q\&A Mode**

In Q\&A Mode, a content chunk consists of a question and an answer. Click on any document title to view the text chunks.

<figure><img src="https://assets-docs.dify.ai/2024/12/98e2486f6c5e06b4ece1b81d078afa08.png" alt=""><figcaption><p><strong>Q&#x26;A Mode - check content chunk</strong></p></figcaption></figure>
{% endtab %}
{% endtabs %}

***

### Checking Chunk Quality

Document chunking significantly influences the Q\&A performance of knowledge-base applications. It’s recommended to perform a manual review of chunking quality before integrating the knowledge base with your application.

Although automated chunk methods based on character length, identifiers, or NLP semantic system can significantly reduce the workload of large-scale text chunk, the quality of chunk is related to the text structure of different document formats and the semantic context. Manual checking and correction can effectively compensate for the shortcomings of machine chunk in semantic recognition.

When checking chunk quality, pay attention to the following situations:

* **Overly short text chunks**, leading to semantic loss;

<figure><img src="https://assets-docs.dify.ai/2024/12/ee081e98c1649aea4a5c2b15b88e11aa.png" alt=""><figcaption><p>Overly short text chunks</p></figcaption></figure>

* **Overly long text chunks**, leading to semantic noise affecting matching accuracy;

<figure><img src="https://assets-docs.dify.ai/2024/12/ac47381ae4be183768dd025c37c049fa.png" alt=""><figcaption><p>Overly long text chunks</p></figcaption></figure>

* **Obvious semantic truncation**, which occurs when using maximum segment length limits, leading to forced semantic truncation and missing content during recall;

<figure><img src="https://assets-docs.dify.ai/2024/12/b8ab7ac84028b0b16c3948f35015e069.png" alt=""><figcaption><p>Obvious semantic truncation</p></figcaption></figure>

***

### Adding Text Chunks

You can add text chunks individually to the knowledge base, and different chunking modes correspond to different ways of adding those chunks.

> Adding text chunks is a paid feature. Please upgrade your account [here](https://dify.ai/pricing) to access this functionality.

{% tabs %}
{% tab title="General Mode" %}
**General Mode**

Click **Add Chunks** in the chunks list page to add one or multiple custom chunks to the document.

<figure><img src="https://assets-docs.dify.ai/2024/12/552ff4ab9e77130ad09aaef878b19cc9.png" alt=""><figcaption><p>General mode - Add chunks</p></figcaption></figure>

When manually adding text chunks, you can choose to add both the main content and keywords. After entering the content, select the **“Add another”** checkbox at the bottom to continue adding more text chunks seamlessly.

<figure><img src="https://assets-docs.dify.ai/2024/12/cd769622bc1d85c037277ef6fa5247c9.png" alt=""><figcaption><p>General mode - Add another text chunk</p></figcaption></figure>

To add chunks in bulk, you need to download the upload template in CSV format first and edit all the chunk contents in Excel according to the template format, then save the CSV file and upload it.

<figure><img src="https://assets-docs.dify.ai/2024/12/5e501dd8efba02ff31d2e739417ce864.png" alt=""><figcaption><p>General mode - Add customize chunks in bulk</p></figcaption></figure>
{% endtab %}

{% tab title="Parent-child Mode" %}
**Parent Child Chunks Mode**

Click Add Chunks in the Chunk list to add one or multiple custom **parent chunks** to the document.

<figure><img src="https://assets-docs.dify.ai/2024/12/ed4be3bf178e3a41d53bcc10255ad3b2.png" alt=""><figcaption><p>Parent-child mode - Add chunks</p></figcaption></figure>

After entering the content, select the **“Add another”** checkbox at the bottom to keep adding more text chunks.

<figure><img src="https://assets-docs.dify.ai/2024/12/ba64232eea364b68f2e38341eb9cf5c1.png" alt=""><figcaption><p>Parent-child mode - Add chunks 2 </p></figcaption></figure>



You can add child chunks individually under a parent chunk. Click “Add” on the right side of the child chunk within the parent chunk to add it.

<figure><img src="https://assets-docs.dify.ai/2024/12/23f68a369eb9c1a2cc9022b99a08341d.png" alt=""><figcaption><p>Parent-child mode - Add child chunks</p></figcaption></figure>
{% endtab %}

{% tab title="Q&A Mode (Community Edition Only)" %}
**Q\&A Mode**

Click the “Add Chunk” button at the top of the chunk list to manually add a single or multiple question-answer pairs chunk to the document.
{% endtab %}
{% endtabs %}

***

### Editing Text Chunks

{% tabs %}
{% tab title="General Mode" %}
**General Mode**

You can directly edit or modify the added chunks content, including modifying the **text content or keywords within the chunks.**

To prevent duplicate edits, an “Edited” tag will appear on the content chunk after it has been modified.

<figure><img src="https://assets-docs.dify.ai/2024/12/92e7788dad008d38f7c8f532fbcb3636.png" alt=""><figcaption><p>Edit text chunks</p></figcaption></figure>
{% endtab %}

{% tab title="Parent-child Mode" %}
**Parent-child Mode**

A parent chunk contains the content of its child chunks, but they remain independent. You can edit the parent chunk or child chunks separately. Below is a diagram explaining the process of modifying parent and child chunks:

<figure><img src="https://assets-docs.dify.ai/2024/12/aacdb2e95b9b7c0265455caaf0f1f55f.png" alt="" width="375"><figcaption><p> Diagram of editing parent-child chunks</p></figcaption></figure>

**To edit a parent chunk:**

1\. Click the Edit button on the right side of the parent chunk.

2\. Enter your changes and then click **Save**—this won’t affect the content of the child chunks.

3\. If you want to regenerate the child chunks after editing, click Save and Re-generate Child Chunks.

To prevent duplicate edits, an “Edited” tag will appear on the content chunk after it has been modified.

<figure><img src="https://assets-docs.dify.ai/2024/12/06354a75368f96b3f8f2afaad4f50b0c.png" alt=""><figcaption><p>Parent-chid chunks mode - Modify parent chunks</p></figcaption></figure>

**Modify child chunks**: select any child chunks and enter edit mode and save it after modification. The modification will not affect the contents of the parent chunks. Child chunks that have been edited or newly added will be marked with a deep blue label, `C-NUMBER-EDITED`.

You can also treat child chunks as tags for the current parent text block.

<figure><img src="https://assets-docs.dify.ai/2024/12/a59563614d8f4661ebfb20f6b646b4ea.png" alt=""><figcaption><p>Parent-child mode - modify child chunks</p></figcaption></figure>
{% endtab %}

{% tab title="Q&A Mode (Community Edition Only)" %}
**Q\&A Mode**

In Q\&A chunking mode, each content chunk consists of a question and an answer. Click on the text chunk you wish to edit to modify the question and answer individually. Additionally, you can edit the keywords for the current chunk.

<figure><img src="https://assets-docs.dify.ai/2024/12/5c69adc0d4ec470d0677e67a4dd894a1.png" alt=""><figcaption><p><strong>Q&#x26;A Mode - modify text chunks</strong></p></figcaption></figure>
{% endtab %}
{% endtabs %}

***

### Metadata Management

In addition to capturing metadata (e.g., title, URL, keywords, or a web page description) from various source documents, metadata is also used as structured fields during the chunk retrieval process for filtering or displaying citation sources.

<figure><img src="https://assets-docs.dify.ai/2024/12/f3b1ff4b559ebc40f18b8980b3719fe8.png" alt=""><figcaption><p>Metadata management</p></figcaption></figure>

***
```

## File: en/guides/knowledge-base/connect-external-knowledge.md
```markdown
# Connect to an External Knowledge Base

> To make a distinction, knowledge bases independent of the Dify platform are collectively referred to as **"external knowledge bases"** in this article.

## Functional Introduction

For developers with advanced content retrieval requirements, **the built-in knowledge base functionality and text retrieval mechanisms of the Dify platform may have limitations, particularly in terms of customizing recall results.**

Due to the requirement of higher accuracy of text retrieval and recall, as well as the need to manage internal materials, some developer teams choose to independently develop RAG algorithms and independently maintain text retrieval systems, or uniformly host content to cloud vendors' knowledge base services (such as [AWS Bedrock](https://aws.amazon.com/bedrock/)).

As a neutral platform for LLM application development, Dify is committed to providing developers with a wider range of options.

The **Connect to External Knowledge Base** feature enables integration between the Dify platform and external knowledge bases. Through API services, AI applications can access a broader range of information sources. This capability offers two key advantages:

* The Dify platform can directly obtain the text content hosted in the cloud service provider's knowledge base, so that developers do not need to repeatedly move the content to the knowledge base in Dify;
* The Dify platform can directly obtain the text content processed by algorithms in the self-built knowledge base. Developers only need to focus on the information retrieval mechanism of the self-built knowledge base and continuously optimize and improve the accuracy of information retrieval.

<figure><img src="../../.gitbook/assets/image (117).png" alt=""><figcaption><p>Principle of external knowledge base connection</p></figcaption></figure>

Here are the detailed steps for connecting to external knowledge:

## 1. Create a Compliant External Knowledge Base API

Create a compliant External Knowledge Base API before setting up the API service, please refer to Dify's [External Knowledge Base API](external-knowledge-api-documentation.md) specifications to ensure successful integration between your external knowledge base and Dify.

## 2. Add External Knowledge API

> Currently, when connecting to external knowledge bases, Dify only supports retrieval permissions and does not support optimization or modification of external knowledge bases. Developers need to maintain external knowledge bases themselves.

Navigate to the **"Knowledge"** page, click **"External Knowledge API"** in the upper right corner, then click **"Add External Knowledge API"**. Follow the page prompts to fill in the following information:

* Name Customizable name to distinguish different external knowledge APIs;
* API Endpoint The URL of the external knowledge base API endpoint, e.g., api-endpoint/retrieval; refer to the [External Knowledge API](external-knowledge-api-documentation.md) for detailed instructions;
* API Key Connection key for the external knowledge, refer to the [External Knowledge API](external-knowledge-api-documentation.md) for detailed instructions.

<figure><img src="../../../img/connect-kb-1-en.png" alt=""><figcaption><p>Associate External Knowledge API</p></figcaption></figure>

## 3. Connect to the External Knowledge Base

Go to the **"Knowledge"** page, click **"Connect to an External Knowledge Base"** under the Add Knowledge Base card to direct to the parameter configuration page.

<figure><img src="../../../img/connect-kb-2-en.png" alt=""><figcaption><p>Connect to the external knowledge base</p></figcaption></figure>

Fill in the following parameters:

* **Knowledge base name & description**
*   **External Knowledge API**

    Select the external knowledge base API associated in step 2; Dify will call the text content stored in the external knowledge base through the API connection method.
*   **External knowledge ID**

    Specify the particular external knowledge base ID to be associated, refer to the external knowledge base API definition for detailed instructions.
*   **Retrieval Setting**

    **Top K:** When a user sends a question, it will request the external knowledge API to obtain highly relevant content chunks. This parameter is used to filter text chunks with high similarity to the user's question. The default value is 3; the higher the value, the more text chunks with relevant similarities will be retrieval.

    **Score Threshold:** The similarity threshold for text chunk filtering, only retrievaling text chunks that exceed the set score. The default value is 0.5. A higher value indicates a higher requirement for similarity between the text and the question, expecting fewer retrievaled text chunks, and the results will be relatively more precise.

<figure><img src="../../../img/connect-kb-3-en.webp" alt=""><figcaption></figcaption></figure>

## 4. Test External Knowledge Base and Retrieval Results

After connected with the external knowledge base, developers can simulate possible question keywords in the **"Retrieval Testing"** to preview text chunks that might be retrieval. If you are unsatisfied with the retrieval results, try modifying the **External Knowledge Base Settings** or adjusting the retrieval strategy of the external knowledge base.

<figure><img src="../../../img/connect-kb-4-en.png" alt=""><figcaption><p>Test external knowledge base connection and retrieval</p></figcaption></figure>

## 5. Integrating External Knowledge base in Applications

*   **Chatbot / Agent** type application

    Associate the external knowledge base in the orchestration page within Chatbot / Agent type applications.

<figure><img src="../../../img/connect-kb-5-en.png" alt=""><figcaption><p>Chatbot / Agent</p></figcaption></figure>

*   **Chatflow / Workflow** type application

    Add a **"Knowledge Retrieval"** node and select the external knowledge base.

<figure><img src="../../../img/connect-kb-6-en.png" alt=""><figcaption><p>Chatflow / Workflow</p></figcaption></figure>

## 6. Manage External Knowledge

Navigate to the **"Knowledge"** page, external knowledge base cards will list an **EXTERNAL** label in the upper right corner. Select the knowledge base needs to be modified, click **"Settings"** to modify the following information:

* **Knowledge base name and description**
*   **Permissions**

    Provide **"Only me"**, **"All team members"**, and **"Partial team members"** permission scope. Those without permission will not be able to access the knowledge base. If you choose to make the knowledge base public to other members, it means that other members also have the rights to view, edit, and delete the knowledge base.
*   **Retrieval Setting**

    **Top K:** When a user sends a question, it will request the external knowledge API to obtain highly relevant content segments. This parameter is used to filter text chunks with high similarity to the user's question. The default value is 3; the higher the value, the more text chunks with relevant similarities will be retrievaled.

    **Score threshold:** The similarity threshold for text chunk filtering, only retrievaling text chunks that exceed the set score. The default value is 0.5. A higher value indicates a higher requirement for similarity between the text and the question, expecting fewer retrievaled text chunks, and the results will be relatively more precise.

The **"External Knowledge API"** and **"External Knowledge ID"** associated with the external knowledge base do not support modification. If modification is needed, please associate a new **"External Knowledge API"** and reset it.

<figure><img src="../../../img/connect-kb-7-en.webp" alt=""><figcaption></figcaption></figure>

### Connection Example

[how-to-connect-aws-bedrock.md](../../learn-more/use-cases/how-to-connect-aws-bedrock.md "mention")

## FAQ

**How to Fix the Errors Occurring When Connecting to External Knowledge API?**

Solutions corresponding to each error code in the return information:

| Error Code | Result                              | Solutions                                                   |
| ---------- | ----------------------------------- | ----------------------------------------------------------- |
| 1001       | Invalid Authorization header format | Please check the Authorization header format of the request |
| 1002       | Authorization failed                | Please check whether the API Key you entered is correct.    |
| 2001       | The knowledge is not exist          | Please check the external repository                        |
```

## File: en/guides/knowledge-base/create-knowledge-and-upload-documents.md
```markdown
# Create Knowledge

Steps to upload documents to create a knowledge base:

1. Create a knowledge base and import either local document file or online data.

{% content-ref url="create-knowledge-and-upload-documents/1.-import-text-data/" %}
[1.-import-text-data](create-knowledge-and-upload-documents/1.-import-text-data/)
{% endcontent-ref %}

2. Choose a chunking mode and preview the spliting results. This stage involves content preprocessing and structuring, where long texts are divided into multiple smaller chunks.

{% content-ref url="create-knowledge-and-upload-documents/2.-choose-a-chunk-mode.md" %}
[2.-choose-a-chunk-mode.md](create-knowledge-and-upload-documents/2.-choose-a-chunk-mode.md)
{% endcontent-ref %}

3. Configure the indexing method and retrieval setting. Once the knowledge base receives a user query, it searches existing documents according to preset retrieval methods and extracts highly relevant content chunks.&#x20;

{% content-ref url="create-knowledge-and-upload-documents/3.-select-the-indexing-method-and-retrieval-setting.md" %}
[3.-select-the-indexing-method-and-retrieval-setting.md](create-knowledge-and-upload-documents/3.-select-the-indexing-method-and-retrieval-setting.md)
{% endcontent-ref %}

4. Wait for the chunk embeddings to complete.
5. Once finished, link the knowledge base to your application and start using it. You can then [integrate it into your application](integrate-knowledge-within-application.md) to build an LLM that are capable of Q\&A based on knowledge-bases. If you want to modify and manage the knowledge base further, take refer to [Knowledge Base and Document Maintenance](knowledge-and-documents-maintenance.md).

<figure><img src="https://assets-docs.dify.ai/2024/12/a3362a1cd384cb2b539c9858de555518.png" alt=""><figcaption><p>Complete the creation of the knowledge base</p></figcaption></figure>

***

### Reference

#### ETL

In production-level applications of RAG, to achieve better data retrieval, multi-source data needs to be preprocessed and cleaned, i.e., ETL (extract, transform, load). To enhance the preprocessing capabilities of unstructured/semi-structured data, Dify supports optional ETL solutions: **Dify ETL** and [**Unstructured ETL**](https://unstructured.io/).

> Unstructured can efficiently extract and transform your data into clean data for subsequent steps.

ETL solution choices in different versions of Dify:

* The SaaS version defaults to using Unstructured ETL and cannot be changed;
* The community version defaults to using Dify ETL but can enable Unstructured ETL through [environment variables](../../getting-started/install-self-hosted/environments.md#zhi-shi-ku-pei-zhi);

Differences in supported file formats for parsing:

| DIFY ETL                                                | Unstructured ETL                                                                        |
| ------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| txt, markdown, md, pdf, html, htm, xlsx, xls, docx, csv | txt, markdown, md, pdf, html, htm, xlsx, xls, docx, csv, eml, msg, pptx, ppt, xml, epub |

{% hint style="info" %}
Different ETL solutions may have differences in file extraction effects. For more information on Unstructured ETL’s data processing methods, please refer to the [official documentation](https://docs.unstructured.io/open-source/core-functionality/partitioning).
{% endhint %}

#### Embedding

**Embedding** transforms discrete variables (words, sentences, documents) into continuous vector representations, mapping high-dimensional data to lower-dimensional spaces. This technique preserves crucial semantic information while reducing dimensionality, enhancing content retrieval efficiency.

**Embedding models**, specialized large language models, excel at converting text into dense numerical vectors, effectively capturing semantic nuances for improved data processing and analysis.
```

## File: en/guides/knowledge-base/external-knowledge-api-documentation.md
```markdown
---
description: 'Editor: Allen. Dify Technical Writer'
---

# External Knowledge API

## Endpoint

```
POST <your-endpoint>/retrieval
```

## Header

This API is used to connect to a knowledge base that is independent of the Dify and maintained by developers. For more details, please refer to [Connecting to an External Knowledge Base](https://docs.dify.ai/guides/knowledge-base/connect-external-knowledge-base). You can use `API-Key` in the `Authorization` HTTP Header to verify permissions. The authentication logic is defined by you in the retrieval API, as shown below:

```
Authorization: Bearer {API_KEY}
```

## Request Body Elements

The request accepts the following data in JSON format.

| Property | Required | Type | Description | Example value |
|----------|----------|------|-------------|---------------|
| knowledge_id | TRUE | string | Your knowledge's unique ID | AAA-BBB-CCC |
| query | TRUE | string | User's query | What is Dify? |
| retrieval_setting | TRUE | object | Knowledge's retrieval parameters | See below |

The `retrieval_setting` property is an object containing the following keys:

| Property | Required | Type | Description | Example value |
|----------|----------|------|-------------|---------------|
| top_k | TRUE | int | Maximum number of retrieved results | 5 |
| score_threshold | TRUE | float | The score limit of relevance of the result to the query, scope: 0~1 | 0.5 |

## Request Syntax

```json
POST <your-endpoint>/retrieval HTTP/1.1
-- header
Content-Type: application/json
Authorization: Bearer your-api-key
-- data
{
    "knowledge_id": "your-knowledge-id",
    "query": "your question",
    "retrieval_setting":{
        "top_k": 2,
        "score_threshold": 0.5
    }
}
```

## Response Elements

If the action is successful, the service sends back an HTTP 200 response.

The following data is returned in JSON format by the service.

| Property | Required | Type | Description | Example value |
|----------|----------|------|-------------|---------------|
| records | TRUE | List[Object] | A list of records from querying the knowledge base. | See below |

The `records` property is a list object containing the following keys:

| Property | Required | Type | Description | Example value |
|----------|----------|------|-------------|---------------|
| content | TRUE | string | Contains a chunk of text from a data source in the knowledge base. | Dify:The Innovation Engine for GenAI Applications |
| score | TRUE | float | The score of relevance of the result to the query, scope: 0~1 | 0.5 |
| title | TRUE | string | Document title | Dify Introduction |
| metadata | FALSE | json | Contains metadata attributes and their values for the document in the data source. | See example |

## Response Syntax

```json
HTTP/1.1 200
Content-type: application/json
{
    "records": [{
                    "metadata": {
                            "path": "s3://dify/knowledge.txt",
                            "description": "dify knowledge document"
                    },
                    "score": 0.98,
                    "title": "knowledge.txt",
                    "content": "This is the document for external knowledge."
            },
            {
                    "metadata": {
                            "path": "s3://dify/introduce.txt",
                            "description": "dify introduce"
                    },
                    "score": 0.66,
                    "title": "introduce.txt",
                    "content": "The Innovation Engine for GenAI Applications"
            }
    ]
}
```

## Errors

If the action fails, the service sends back the following error information in JSON format:

| Property | Required | Type | Description | Example value |
|----------|----------|------|-------------|---------------|
| error_code | TRUE | int | Error code | 1001 |
| error_msg | TRUE | string | The description of API exception | Invalid Authorization header format. Expected 'Bearer <api-key>' format. |

The `error_code` property has the following types:

| Code | Description |
|------|-------------|
| 1001 | Invalid Authorization header format. |
| 1002 | Authorization failed |
| 2001 | The knowledge does not exist |

### HTTP Status Codes

**AccessDeniedException**
The request is denied because of missing access permissions. Check your permissions and retry your request.
HTTP Status Code: 403

**InternalServerException**
An internal server error occurred. Retry your request.
HTTP Status Code: 500
```

## File: en/guides/knowledge-base/integrate-knowledge-within-application.md
```markdown
# Integrate Knowledge Base within Application

### Creating an Application Integrated with Knowledge Base

A **"Knowledge Base"** can be used as an external information source to provide precise answers to user questions via LLM. You can associate an existing knowledge base with any [application type](https://docs.dify.ai/guides/application-orchestrate#application_type) in Dify.

Taking a chat assistant as an example, the process is as follows:

1. Go to **Knowledge -- Create Knowledge -- Upload file**
2. Go to **Studio -- Create Application -- Select Chatbot**
3. Enter **Context**, click **Add**, and select one of the knowledge base created
4. In **Context Settings -- Retrieval Setting**, configure the **Retrieval Setting**
5. Enable **Citation and Attribution** in **Add Features**
6. In **Debug and Preview**, input user questions related to the knowledge base for debugging
7. After debugging, click **Publish** button to make an AI application based on your own knowledge!

***

### Connecting Knowledge and Setting Retrieval Mode

In applications that utilize multiple knowledge bases, it is essential to configure the retrieval mode to enhance the precision of retrieved content. To set the retrieval mode for the knowledge bases, navigate to **Context -- Retrieval Settings -- Rerank Setting**.

#### Retrieval Setting

The retriever scans all knowledge bases linked to the application for text content relevant to the user's question. The results are then consolidated. Below is the technical flowchart for the Multi-path Retrieval mode:

<figure><img src="../../.gitbook/assets/rerank-flow-chart.png" alt=""><figcaption></figcaption></figure>

This method simultaneously queries all knowledge bases connected in **"Context"**, seeking relevant text chucks across multiple knowledge bases, collecting all content that aligns with the user's question, and ultimately applying the Rerank strategy to identify the most appropriate content to respond to the user. This retrieval approach offers more comprehensive and accurate results by leveraging multiple knowledge bases simultaneously.

<figure><img src="https://assets-docs.dify.ai/2024/12/fca4f030e71a857e15a753f508e1b042.png" alt=""><figcaption></figcaption></figure>

For instance, in application A, with three knowledge bases K1, K2, and K3. When a user send a question, multiple relevant pieces of content will be retrieved and combined from these knowledge bases. To ensure the most pertinent content is identified, the Rerank strategy is employed to find the content that best relates to the user's query, enhancing the precision and reliability of the results.

In practical Q\&A scenarios, the sources of content and retrieval methods for each knowledge base may differ. To manage the mixed content returned from retrieval, the [Rerank strategy](https://docs.dify.ai/learn-more/extended-reading/retrieval-augment/rerank) acts as a refined sorting mechanism. It ensures that the candidate content aligns well with the user's question, optimizing the ranking of results across multiple knowledge bases to identify the most suitable content, thereby improving answer quality and overall user experience.

Considering the costs associated with using Rerank and the needs of the business, the multi-path retrieval mode provides two Rerank settings:

**Weighted Score**

This setting uses internal scoring mechanisms and does not require an external Rerank model, thus **avoiding any additional processing costs**. You can select the most appropriate content matching strategy by adjusting the weight ratio sliders for semantics or keywords.

*   **Semantic Value of 1**

    This mode activates semantic retrieval only. By utilizing the Embedding model, the search depth can be enhanced even if the exact words from the query do not appear in the knowledge base, as it calculates vector distances to return the relevant content. Furthermore, when dealing with multilingual content, semantic retrieval can capture meanings across different languages, yielding more accurate cross-language search results.
*   **Keyword Value of 1**

    This mode activates keyword retrieval only. It matches the user's input text against the full text of the knowledge base, making it ideal for scenarios where the user knows the exact information or terminology. This method is resource-efficient, making it suitable for quickly retrieving information from large document repositories.
*   **Custom Keyword and Semantic Weights**

    In addition to enabling only semantic or keyword retrieval modes, we offer flexible custom Weight Score. You can determine the best weight ratio for your business scenario by continuously adjusting the weights of both.

**Rerank Model**

The Rerank model is an external scoring system that calculates the relevance score between the user's question and each candidate document provided, improving the results of semantic ranking and returning a list of documents sorted by relevance from high to low.

While this method incurs some additional costs, it is more adept at handling complex knowledge base content, such as content that combines semantic queries and keyword matches, or cases involving multilingual returned content.

> Click here to learn more about the [Re-ranking](https://docs.dify.ai/learn-more/extended-reading/retrieval-augment/rerank).

Dify currently supports multiple Rerank models. To use external Rerank models, you'll need to provide an API Key. Enter the API Key for the Rerank model (such as Cohere, Jina AI, etc.) on the "Model Provider" page.

<figure><img src="../../.gitbook/assets/en-rerank-model-api.png" alt=""><figcaption><p>Configuring the Rerank model in the Model Provider</p></figcaption></figure>

**Adjustable Parameters**

* **TopK**: Determines how many text chunks, deemed most similar to the user’s query, are retrieved. It also automatically adjusts the number of chunks based on the chosen model’s context window. The default value is **3**, and higher numbers will recall more text chunks.
* **Score Threshold**: Sets the minimum similarity score required for a chunk to be retrieved. Only chunks exceeding this score are retrieved. The default value is **0.5**. Higher thresholds demand greater similarity and thus result in fewer chunks being retrieved.

### View Linked Applications in the Knowledge Base

On the left side of the knowledge base, you can see all linked Apps. Hover over the circular icon to view the list of all linked apps. Click the jump button on the right to quickly browser them.

<figure><img src="https://assets-docs.dify.ai/2024/12/28899b9b0eba8996f364fb74e5b94c7f.png" alt=""><figcaption><p>Viewing the linked Apps</p></figcaption></figure>

### Frequently Asked Questions

1. **How should I choose Rerank settings in multi-recall mode?**

If users know the exact information or terminology, you can use keyword search for precise matching. In that case, set **“Keywords” to 1** under Weight Settings.

If the knowledge base doesn’t contain the exact terms or if a cross-lingual query is involved, we recommend setting **“Semantic” to 1** under Weight Settings.

If you are familiar with real user queries and want to adjust the ratio of semantics to keywords, they can manually tweak the ratio under **Weight Settings**.

If the knowledge base is complex, making simple semantic or keyword matches insufficient—and you need highly accurate answers and are willing to pay more—consider using a **Rerank Model** for content retrieval.

2. **What should I do if I encounter issues finding the “Weight Score” or the requirement to configure a Rerank model?**

Here's how the knowledge base retrieval method affects Multi-path Retrieval:

<figure><img src="../../.gitbook/assets/image (101).png" alt=""><figcaption></figcaption></figure>

3. **What should I do if I cannot adjust the “Weight Score” when referencing multiple knowledge bases and an error message appears?**

This issue occurs because the embedding models used in the multiple referenced knowledge bases are inconsistent, prompting this notification to avoid conflicts in retrieval content. It is advisable to set and enable the Rerank model in the "Model Provider" or unify the retrieval settings of the knowledge bases.

4. **Why can't I find the “Weight Score” option in multi-recall mode, and only see the Rerank model?**

Please check whether your knowledge base is using the “Economical” index mode. If so, switch it to the “High Quality” index mode.
```

## File: en/guides/knowledge-base/knowledge-and-documents-maintenance.md
```markdown
# Manage Knowledge

> The knowledge page is accessible only to the team owner, team administrators, and users with editor permissions.

Click the **Knowledge** button at the top of the Dify platform and select the knowledge you want to manage. Navigate to Settings in the left sidebar to configure it.

Here, you can modify the knowledge base’s name, description, permissions, indexing method, embedding model and retrieval settings.

<figure><img src="https://assets-docs.dify.ai/2024/12/20fc93428f8f20f7acfce665c4ed4ddf.png" alt=""><figcaption><p>Knowledge base settings</p></figcaption></figure>

* **Knowledge Name**: Used to distinguish among different knowledge bases.
* **Knowledge Description**: Used to describe the information represented by the documents in the knowledge base.
* **Permission**: Defines access control for the knowledge base with three levels:
  * **"Only Me"**: Restricts access to the knowledge base owner.
  * **"All team members"**: Grants access to every member of the team.
  *   **"Partial team members"**: Allows selective access to specific team members.

      Users without appropriate permissions cannot access the knowledge base. When granting access to team members (Options 2 or 3), authorized users are granted full permissions, including the ability to view, edit, and delete knowledge base content.
* **Indexing Mode**: For detailed explanations, please refer to the [documentation](create-knowledge-and-upload-documents/3.-select-the-indexing-method-and-retrieval-setting.md).
* **Embedding Model**: Allows you to modify the embedding model for the knowledge base. Changing the embedding model will re-embed all documents in the knowledge base, and the original embeddings will be deleted.
* **Retrieval Settings**: For detailed explanations, please refer to the [documentation](../../learn-more/extended-reading/retrieval-augment/retrieval.md).

***

#### View Linked Applications in the Knowledge Base

On the left side of the knowledge base, you can see all linked Apps. Hover over the circular icon to view the list of all linked apps. Click the jump button on the right to quickly browser them.

<figure><img src="https://assets-docs.dify.ai/2024/12/28899b9b0eba8996f364fb74e5b94c7f.png" alt=""><figcaption><p>Viewing the Linked Apps</p></figcaption></figure>

You can manage your knowledge base documents either through a web interface or via an API.

#### Maintain Knowledge Documents

You can administer all documents and their corresponding chunks directly in the knowledge base. For more details, refer to the following documentation:

{% content-ref url="knowledge-and-documents-maintenance/maintain-documents.md" %}
[maintain-documents.md](knowledge-and-documents-maintenance/maintain-documents.md)
{% endcontent-ref %}

#### Maintain Knowledge Base Via API

Dify Knowledge Base provides a comprehensive set of standard APIs. Developers can use these APIs to perform routine management and maintenance tasks, such as adding, deleting, updating, and retrieving documents and chunks. For more details, refer to the following documentaiton:

{% content-ref url="maintain-dataset-via-api.md" %}
[maintain-dataset-via-api.md](maintain-dataset-via-api.md)
{% endcontent-ref %}

<figure><img src="../../.gitbook/assets/knowledge-base-api.png" alt=""><figcaption><p>Knowledge base API management</p></figcaption></figure>
```

## File: en/guides/knowledge-base/maintain-dataset-via-api.md
```markdown
# Maintain Knowledge via API

> The authentication and invocation methods for the Knowledge Base API are consistent with the Application Service API. However, a single Knowledge Base API token generated has the authority to operate on all visible knowledge bases under the same account. Please pay attention to data security.

### Advantages of Utilizing Knowledge Base API

Leveraging the API for knowledge base maintenance significantly enhances data processing efficiency. It enables seamless data synchronization via command-line interfaces, facilitating automated operations instead of manipulating the user interface.

Key advantages include:

* Automated Synchronization: Enables seamless integration between data systems and the Dify knowledge base, fostering efficient workflow construction.
* Comprehensive Management: Offers functionalities such as knowledge base listing, document enumeration, and detailed querying, facilitating the development of custom data management interfaces.
* Flexible Content Ingestion: Accommodates both plain text and file upload methodologies, supporting batch operations for addition and modification of content chunks.
* Enhanced Productivity: Minimizes manual data handling requirements, thereby optimizing the overall user experience on the Dify platform.

### How to Use

Navigate to the knowledge base page, and you can switch to the **API ACCESS** page from the left navigation. On this page, you can view the dataset API documentation provided by Dify and manage the credentials for accessing the dataset API in **API Keys**.

<figure><img src="../../.gitbook/assets/knowledge-base-api-token.png" alt=""><figcaption><p>Knowledge API Document</p></figcaption></figure>

### API Requesting Examples

#### **Create a document from text**

This api is based on an existing Knowledge and creates a new document through text based on this Knowledge.

Request example:

```json
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create_by_text' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json' \
--data-raw '{"name": "text","text": "text","indexing_technique": "high_quality","process_rule": {"mode": "automatic"}}'
```

Response example:

```json
{
  "document": {
    "id": "",
    "position": 1,
    "data_source_type": "upload_file",
    "data_source_info": {
        "upload_file_id": ""
    },
    "dataset_process_rule_id": "",
    "name": "text.txt",
    "created_from": "api",
    "created_by": "",
    "created_at": 1695690280,
    "tokens": 0,
    "indexing_status": "waiting",
    "error": null,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "archived": false,
    "display_status": "queuing",
    "word_count": 0,
    "hit_count": 0,
    "doc_form": "text_model"
  },
  "batch": ""
}
```

#### Create a Document from a file

This API is based on an existing knowledge and creates a new document through a file based on this knowledge.

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-file' \
--header 'Authorization: Bearer {api_key}' \
--form 'data="{"indexing_technique":"high_quality","process_rule":{"rules":{"pre_processing_rules":[{"id":"remove_extra_spaces","enabled":true},{"id":"remove_urls_emails","enabled":true}],"segmentation":{"separator":"###","max_tokens":500}},"mode":"custom"}}";type=text/plain' \
--form 'file=@"/path/to/file"'
```

```bash
{
  "document": {
    "id": "",
    "position": 1,
    "data_source_type": "upload_file",
    "data_source_info": {
      "upload_file_id": ""
    },
    "dataset_process_rule_id": "",
    "name": "Dify.txt",
    "created_from": "api",
    "created_by": "",
    "created_at": 1695308667,
    "tokens": 0,
    "indexing_status": "waiting",
    "error": null,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "archived": false,
    "display_status": "queuing",
    "word_count": 0,
    "hit_count": 0,
    "doc_form": "text_model"
  },
  "batch": ""
}

```

#### Create Documents from a File

This api is based on an existing Knowledge and creates a new document through a file based on this Knowledge.

Request example:

```json
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/document/create-by-file' \
--header 'Authorization: Bearer {api_key}' \
--form 'data="{"indexing_technique":"high_quality","process_rule":{"rules":{"pre_processing_rules":[{"id":"remove_extra_spaces","enabled":true},{"id":"remove_urls_emails","enabled":true}],"segmentation":{"separator":"###","max_tokens":500}},"mode":"custom"}}";type=text/plain' \
--form 'file=@"/path/to/file"'
```

Response example:

```json
{
  "document": {
    "id": "",
    "position": 1,
    "data_source_type": "upload_file",
    "data_source_info": {
      "upload_file_id": ""
    },
    "dataset_process_rule_id": "",
    "name": "Dify.txt",
    "created_from": "api",
    "created_by": "",
    "created_at": 1695308667,
    "tokens": 0,
    "indexing_status": "waiting",
    "error": null,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "archived": false,
    "display_status": "queuing",
    "word_count": 0,
    "hit_count": 0,
    "doc_form": "text_model"
  },
  "batch": ""
}

```

#### Create an Empty Knowledge Base

{% hint style="warning" %}
Only used to create an empty knowledge base.
{% endhint %}

Request example:

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json' \
--data-raw '{"name": "name", "permission": "only_me"}'
```

Response example:

```json
{
  "id": "",
  "name": "name",
  "description": null,
  "provider": "vendor",
  "permission": "only_me",
  "data_source_type": null,
  "indexing_technique": null,
  "app_count": 0,
  "document_count": 0,
  "word_count": 0,
  "created_by": "",
  "created_at": 1695636173,
  "updated_by": "",
  "updated_at": 1695636173,
  "embedding_model": null,
  "embedding_model_provider": null,
  "embedding_available": null
}
```

#### Get Knowledge Base List

Request example:

```bash
curl --location --request GET 'https://api.dify.ai/v1/datasets?page=1&limit=20' \
--header 'Authorization: Bearer {api_key}'
```

Response example:

```json
{
  "data": [
    {
      "id": "",
      "name": "name",
      "description": "desc",
      "permission": "only_me",
      "data_source_type": "upload_file",
      "indexing_technique": "",
      "app_count": 2,
      "document_count": 10,
      "word_count": 1200,
      "created_by": "",
      "created_at": "",
      "updated_by": "",
      "updated_at": ""
    },
    ...
  ],
  "has_more": true,
  "limit": 20,
  "total": 50,
  "page": 1
}
```

#### Delete a Knowledge Base

Request example:

```json
curl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}' \
--header 'Authorization: Bearer {api_key}'
```

Response example:

```json
204 No Content
```

#### Update a Document with Text

This api is based on an existing Knowledge and updates the document through text based on this Knowledge

Request example:

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update_by_text' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json' \
--data-raw '{"name": "name","text": "text"}'
```

Response example:

```json
{
  "document": {
    "id": "",
    "position": 1,
    "data_source_type": "upload_file",
    "data_source_info": {
      "upload_file_id": ""
    },
    "dataset_process_rule_id": "",
    "name": "name.txt",
    "created_from": "api",
    "created_by": "",
    "created_at": 1695308667,
    "tokens": 0,
    "indexing_status": "waiting",
    "error": null,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "archived": false,
    "display_status": "queuing",
    "word_count": 0,
    "hit_count": 0,
    "doc_form": "text_model"
  },
  "batch": ""
}
```

#### Update a document with a file

This api is based on an existing Knowledge, and updates documents through files based on this Knowledge.

Request example:

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/update-by-file' \
--header 'Authorization: Bearer {api_key}' \
--form 'data="{"name":"Dify","indexing_technique":"high_quality","process_rule":{"rules":{"pre_processing_rules":[{"id":"remove_extra_spaces","enabled":true},{"id":"remove_urls_emails","enabled":true}],"segmentation":{"separator":"###","max_tokens":500}},"mode":"custom"}}";type=text/plain' \
--form 'file=@"/path/to/file"'
```

Response example:

```json
{
  "document": {
    "id": "",
    "position": 1,
    "data_source_type": "upload_file",
    "data_source_info": {
      "upload_file_id": ""
    },
    "dataset_process_rule_id": "",
    "name": "Dify.txt",
    "created_from": "api",
    "created_by": "",
    "created_at": 1695308667,
    "tokens": 0,
    "indexing_status": "waiting",
    "error": null,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "archived": false,
    "display_status": "queuing",
    "word_count": 0,
    "hit_count": 0,
    "doc_form": "text_model"
  },
  "batch": "20230921150427533684"
}
```

#### Get Document Embedding Status (Progress)

Request example:

```bash
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{batch}/indexing-status' \
--header 'Authorization: Bearer {api_key}'
```

Response example:

```json
{
  "data":[{
    "id": "",
    "indexing_status": "indexing",
    "processing_started_at": 1681623462.0,
    "parsing_completed_at": 1681623462.0,
    "cleaning_completed_at": 1681623462.0,
    "splitting_completed_at": 1681623462.0,
    "completed_at": null,
    "paused_at": null,
    "error": null,
    "stopped_at": null,
    "completed_segments": 24,
    "total_segments": 100
  }]
}
```

#### Delete a Document

Request example:

```bash
curl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}' \
--header 'Authorization: Bearer {api_key}'
```

Response example:

```bash
{
  "result": "success"
}
```

#### Get the Document List of a Knowledge Base

Request example:

```bash
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents' \
--header 'Authorization: Bearer {api_key}'
```

Response example:

```json
{
  "data": [
    {
      "id": "",
      "position": 1,
      "data_source_type": "file_upload",
      "data_source_info": null,
      "dataset_process_rule_id": null,
      "name": "dify",
      "created_from": "",
      "created_by": "",
      "created_at": 1681623639,
      "tokens": 0,
      "indexing_status": "waiting",
      "error": null,
      "enabled": true,
      "disabled_at": null,
      "disabled_by": null,
      "archived": false
    },
  ],
  "has_more": false,
  "limit": 20,
  "total": 9,
  "page": 1
}
```

#### Add Chunks to a Document

Request example:

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json' \
--data-raw '{"segments": [{"content": "1","answer": "1","keywords": ["a"]}]}'
```

Response example:

```json
{
  "data": [{
    "id": "",
    "position": 1,
    "document_id": "",
    "content": "1",
    "answer": "1",
    "word_count": 25,
    "tokens": 0,
    "keywords": [
        "a"
    ],
    "index_node_id": "",
    "index_node_hash": "",
    "hit_count": 0,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "status": "completed",
    "created_by": "",
    "created_at": 1695312007,
    "indexing_at": 1695312007,
    "completed_at": 1695312007,
    "error": null,
    "stopped_at": null
  }],
  "doc_form": "text_model"
}

```

#### Get Chunks from a Document

Request example:

```bash
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json'
```

Response example:

```bash
{
  "data": [{
    "id": "",
    "position": 1,
    "document_id": "",
    "content": "1",
    "answer": "1",
    "word_count": 25,
    "tokens": 0,
    "keywords": [
        "a"
    ],
    "index_node_id": "",
    "index_node_hash": "",
    "hit_count": 0,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "status": "completed",
    "created_by": "",
    "created_at": 1695312007,
    "indexing_at": 1695312007,
    "completed_at": 1695312007,
    "error": null,
    "stopped_at": null
  }],
  "doc_form": "text_model"
}
```

#### Delete a Chunk in a Document

Request example:

```bash
curl --location --request DELETE 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json'
```

Response example:

```bash
{
  "result": "success"
}
```

#### Update a Chunk in a Document

Request example:

```bash
curl --location --request POST 'https://api.dify.ai/v1/datasets/{dataset_id}/documents/{document_id}/segments/{segment_id}' \
--header 'Authorization: Bearer {api_key}' \
--header 'Content-Type: application/json'\
--data-raw '{"segment": {"content": "1","answer": "1", "keywords": ["a"], "enabled": false}}'
```

Response example:

```bash
{
  "data": [{
    "id": "",
    "position": 1,
    "document_id": "",
    "content": "1",
    "answer": "1",
    "word_count": 25,
    "tokens": 0,
    "keywords": [
        "a"
    ],
    "index_node_id": "",
    "index_node_hash": "",
    "hit_count": 0,
    "enabled": true,
    "disabled_at": null,
    "disabled_by": null,
    "status": "completed",
    "created_by": "",
    "created_at": 1695312007,
    "indexing_at": 1695312007,
    "completed_at": 1695312007,
    "error": null,
    "stopped_at": null
  }],
  "doc_form": "text_model"
}
```

#### Retrieve Chunks from a Knowledge Base

Request example:

```bash
curl --location --request GET 'https://api.dify.ai/v1/datasets/{dataset_id}/retrieve' \
--header 'Authorization: Bearer {api_key}'\
--header 'Content-Type: application/json'\
--data-raw '{
    "query": "test",
    "retrieval_model": {
        "search_method": "keyword_search",
        "reranking_enable": false,
        "reranking_mode": null,
        "reranking_model": {
            "reranking_provider_name": "",
            "reranking_model_name": ""
        },
        "weights": null,
        "top_k": 1,
        "score_threshold_enabled": false,
        "score_threshold": null
    }
}'
```

Response example:

```bash
{
  "query": {
    "content": "test"
  },
  "records": [
    {
      "segment": {
        "id": "7fa6f24f-8679-48b3-bc9d-bdf28d73f218",
        "position": 1,
        "document_id": "a8c6c36f-9f5d-4d7a-8472-f5d7b75d71d2",
        "content": "Operation guide",
        "answer": null,
        "word_count": 847,
        "tokens": 280,
        "keywords": [
          "install",
          "java",
          "base",
          "scripts",
          "jdk",
          "manual",
          "internal",
          "opens",
          "add",
          "vmoptions"
        ],
        "index_node_id": "39dd8443-d960-45a8-bb46-7275ad7fbc8e",
        "index_node_hash": "0189157697b3c6a418ccf8264a09699f25858975578f3467c76d6bfc94df1d73",
        "hit_count": 0,
        "enabled": true,
        "disabled_at": null,
        "disabled_by": null,
        "status": "completed",
        "created_by": "dbcb1ab5-90c8-41a7-8b78-73b235eb6f6f",
        "created_at": 1728734540,
        "indexing_at": 1728734552,
        "completed_at": 1728734584,
        "error": null,
        "stopped_at": null,
        "document": {
          "id": "a8c6c36f-9f5d-4d7a-8472-f5d7b75d71d2",
          "data_source_type": "upload_file",
          "name": "readme.txt",
          "doc_type": null
        }
      },
      "score": 3.730463140527718e-05,
      "tsne_position": null
    }
  ]
}
```

### Error message

Response example:

```bash
  {
    "code": "no_file_uploaded",
    "message": "Please upload your file.",
    "status": 400
  }

```

| code                          | status | message                                                                                      |
| ----------------------------- | ------ | -------------------------------------------------------------------------------------------- |
| no\_file\_uploaded            | 400    | Please upload your file.                                                                     |
| too\_many\_files              | 400    | Only one file is allowed.                                                                    |
| file\_too\_large              | 413    | File size exceeded.                                                                          |
| unsupported\_file\_type       | 415    | File type not allowed. Supported format: txt, markdown, md, pdf, html, html, xlsx, docx, csv |
| high\_quality\_dataset\_only  | 400    | Current operation only supports 'high-quality' datasets.                                     |
| dataset\_not\_initialized     | 400    | The dataset is still being initialized or indexing. Please wait a moment.                    |
| archived\_document\_immutable | 403    | The archived document is not editable.                                                       |
| dataset\_name\_duplicate      | 409    | The dataset name already exists. Please modify your dataset name.                            |
| invalid\_action               | 400    | Invalid action.                                                                              |
| document\_already\_finished   | 400    | The document has been processed. Please refresh the page or go to the document details.      |
| document\_indexing            | 400    | The document is being processed and cannot be edited.                                        |
| invalid\_metadata             | 400    | The metadata content is incorrect. Please check and verify.                                  |
```

## File: en/guides/knowledge-base/README.md
```markdown
# Knowledge

Dify’s Knowledge feature visualizes each stage of the RAG pipeline, providing a friendly UI for application builders to easily manage personal or team knowledge. It also allows for seamless integration into AI applications.

Developers can upload internal company documents, FAQs, and standard working guides, then process them into structured data that large language models (LLMs) can query.&#x20;

Compared with the static pre-trained datasets built into AI models, the content in a knowledge base can be updated in real time, ensuring LLMs always have access to the latest information and helping avoid problems caused by outdated or missing data.

When an LLM receives a user query, it first uses keywords to search within the knowledge base. Based on those keywords, the knowledge base returns content chunks with high relevance rankings, giving the LLM crucial context to generate more precise answers.

This approach ensures LLMs don’t rely solely on pre-trained knowledge. Instead, they can also draw from real-time documents and databases, enhancing both the accuracy and relevance of responses.

**Key Advantages**

**• Real-Time Updates**: The knowledge base can be updated anytime, ensuring the model always has the latest information.

• **Precision**: By retrieving relevant documents, the LLM can ground its answers in actual information, minimizing hallucinations.

• **Flexibility**: Developers can customize the knowledge base content to match specific needs, defining the scope of knowledge as required.

***

You only need to prepare text content, such as:

* Long text content (TXT, Markdown, DOCX, HTML, JSONL, or even PDF files)
* Structured data (CSV, Excel, etc.)
* Online data source(Web pages, Notion, etc.)

By simply uploading files to the **Knowledge Base**, data processing is handled automatically.

> If your team already has an independent knowledge base, you can use the [“Connect to an External Knowledge Base”](connect-external-knowledge.md) feature to establish its connection with Dify.

<figure><img src="https://assets-docs.dify.ai/2024/12/effc826d2584d5f2983cdcd746099bb6.png" alt=""><figcaption><p>Create a knowledge base</p></figcaption></figure>

### **Use Case**

If you want to create an AI customer support assistant based on your existing knowledge base and product documentation, you can simply upload those files to the Knowledge Base in Dify, then set up a conversational application.&#x20;

Traditionally, going from raw text training to a fully developed AI customer support chatbot could take weeks, plus it’s challenging to maintain and iterate effectively.&#x20;

In Dify, the entire process takes just three minutes, after which you can immediately begin gathering user feedback.

### Knowledge Base and Documents

In Dify, a Knowledge Base is a collection of Documents, each of which can include multiple Chunks of content. You can integrate an entire knowledge base into an application to serve as a retrieval context, drawing from uploaded files or data synchronized from other sources.

If your team already has an independent, external knowledge that is separate from the Dify platform, you can link it using the [External Knowledge Base](external-knowledge-api-documentation.md) feature. This way, you don’t need to re-upload all your content to Dify. Your AI app can directly access and process information in real time from your team’s existing knowledge.
```

## File: en/guides/knowledge-base/retrieval-test-and-citation.md
```markdown
# Retrieval Test / Citation and Attributions

### 1. Retrieval Testing

Dify’s knowledge base provides a text retrieval testing feature, allowing you to simulate user queries and retrieve knowledge base content blocks. The retrieval chunks are sorted by score and then sent to the LLM. Generally, the higher the match between the question and the content chunks, the more closely the LLM’s answer will align with the source document, leading to better “training results.”

You can test with different **retrieval methods and parameter configurations** to evaluate the quality and effectiveness of the retrieved text chunks. Different **chunking modes** use different retrieved testing methods.

{% tabs %}
{% tab title="General" %}
**General**

Enter common user questions into the **Source Text field** and click **Test** to see the **Retrieved Chunks** results on the right.

In **General Mode**, each text chunk stands independently. The score shown in the top-right corner of a chunk represents how closely it matches the query keywords. A higher score indicates a stronger alignment between the chunk and the keywords.

<figure><img src="https://assets-docs.dify.ai/2024/12/806967bb36e74fc744b34887cd3ebe52.png" alt=""><figcaption><p>General mode - retrieval text chunks</p></figcaption></figure>

Tap a content chunk to see the details of the referenced content. Each chunk shows its source document information at the bottom, letting you verify whether the text  chunk is appropriate.

<figure><img src="https://assets-docs.dify.ai/2024/12/419ac78ad21ea198b08f89c4f5fde485.png" alt=""><figcaption><p>Review the details of text chunks</p></figcaption></figure>
{% endtab %}

{% tab title="Parent-child" %}
**Parent-child**

Enter typical user questions into the **Source Text** field and click **Test** to view the **Retrieved Chunks** on the right. In parent-child chunking mode, keywords are matched against child chunks for more precise results, and the score in the upper-right corner indicates how closely a child chunk matches the keyword.

You can click a child chunk to preview its exact content. After the match, the entire parent chunk is recalled to provide more comprehensive information.

<figure><img src="https://assets-docs.dify.ai/2024/12/6f0b99f97b138805bf4665d0c5c16f26.png" alt=""><figcaption><p>Retrieval test - Parent-child mode</p></figcaption></figure>

Each chunk displays its source document at the bottom—usually a specific paragraph or sentence. Tap the “Open” button on the right side of the source to view the entire referenced content chunk. Since multiple child chunks can be relevant, this allows you to assess whether the current chunk is appropriate.

<figure><img src="https://assets-docs.dify.ai/2024/12/22103227f8a25069d147160254f69512.png" alt=""><figcaption><p>Check the details of retrieval chunks</p></figcaption></figure>
{% endtab %}
{% endtabs %}

In **Records**, you can check the past query records. If the knowledge base is linked to an application, any queries triggered within the application will also appear here.

#### Modify Text Retrieval Setting

Click the icon in the upper-right corner of the Source Text field to change the current knowledge base’s retrieval method and related parameters. These changes only take effect during the current retrieval test session for debugging, you can compare the retrieval performance of different retrieval settings.&#x20;

If you want to permanently modify the retrieval method for the knowledge base, go to **“Knowledge Base Settings”** > **“Retrieval Settings”** to make changes.

<figure><img src="https://assets-docs.dify.ai/2024/12/86b78cb114a843c9dedcba1fe12e3b02.png" alt=""><figcaption><p>Retrieval settings</p></figcaption></figure>

**Suggested Steps for Retrieval Testing:**

1. Design and organize test cases/test question sets covering common user questions.
2. Choose an appropriate retrieval strategy: vector search/full-text search/hybrid search. For the pros and cons of different retrieval methods, please refer to the extended reading [Retrieval-Augmented Generation (RAG)](../../learn-more/extended-reading/retrieval-augment/).
3. Debug the number of retrieval segments (TopK) and the recall score threshold (Score). Choose appropriate parameter combinations based on the application scenario, including the quality of the documents themselves.

**How to Configure TopK Value and Retrieval Threshold (Score)**

* **TopK represents the maximum number of retrieval chunks when sorted in descending order of similarity scores.** A smaller TopK value will recall fewer segments, which may result in incomplete recall of relevant texts; a larger TopK value will recall more segments, which may result in recalling segments with lower semantic relevance, reducing the quality of LLM responses.
* **The retrieval threshold (Score) represents the minimum similarity score allowed for recall segments.** A smaller recall score will retrieval more segments, which may result in recalling less relevant segments; a larger recall score threshold will recall fewer segments, and if too large, may result in missing relevant segments.

***

### 2. Citation and Attribution

When testing the knowledge base effect within the application, you can go to **Workspace -- Add Feature -- Citation and Attribution** to enable the citation attribution feature.

<figure><img src="../../.gitbook/assets/citation-and-attribution.png" alt=""><figcaption><p>Enable citation and attribution feature</p></figcaption></figure>

After enabling the feature, when the large language model responds to a question by citing content from the knowledge base, you can view specific citation paragraph information below the response content, including **original segment text, segment number, matching degree**, etc. Clicking **Link to Knowledge** above the cited segment allows quick access to the segment list in the knowledge base, facilitating developers in debugging and editing.

<figure><img src="../../../img/view-citation-information.png" alt=""><figcaption><p>View citation information in response content</p></figcaption></figure>

#### View Linked Applications in the Knowledge Base

On the left side of the knowledge base, you can see all linked Apps. Hover over the circular icon to view the list of all linked apps. Click the jump button on the right to quickly browser them.

<figure><img src="https://assets-docs.dify.ai/2024/12/28899b9b0eba8996f364fb74e5b94c7f.png" alt=""><figcaption><p>Viewing the linked Apps</p></figcaption></figure>
```

## File: en/guides/knowledge-base/sync-from-website.md
```markdown
# 1.2 Import Data from Website

The knowledge base supports crawling content from public web pages using third-party tools such as [Jina Reader](https://jina.ai/reader/) and [Firecrawl](https://www.firecrawl.dev/), parsing it into Markdown content, and importing it into the knowledge base.

{% hint style="info" %}
​[Firecrawl](https://www.firecrawl.dev/) and [Jina Reader](https://jina.ai/reader/) are both open-source web parsing tools that can convert web pages into clean Markdown format text that is easy for LLMs to recognize, while providing easy-to-use API services.
{% endhint %}

The following sections will introduce the usage methods for Firecrawl and Jina Reader respectively.

### Firecrawl <a href="#how-to-configure" id="how-to-configure"></a>

#### **1. Configure Firecrawl API credentials**

Click on the avatar in the upper right corner, then go to the **DataSource** page, and click the **Configure** button next to Firecrawl.

<figure><img src="https://assets-docs.dify.ai/2024/12/d468cf996f591b4b2bd0ffb5de62bad4.png" alt=""><figcaption><p>Configuring Firecrawl credentials</p></figcaption></figure>

Log in to the [Firecrawl website](https://www.firecrawl.dev/) to complete registration, get your API Key, and then enter and save it in Dify.

<figure><img src="https://files.gitbook.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FRncMhlfeYTrpujwzDIqw%2Fuploads%2FtAwcLoAYT1A2v12pfJC3%2Fimage.png?alt=media&#x26;token=3b5b784f-2808-431f-8595-2638d038c190" alt=""><figcaption><p>Get the API Key and save it in Dify</p></figcaption></figure>

#### 2. Scrape target webpage

On the knowledge base creation page, select **Sync from website**, choose Firecrawl as the provider, and enter the target URL to be crawled.

The configuration options include: Whether to crawl sub-pages, Page crawling limit, Page scraping max depth, Excluded paths, Include only paths, and Content extraction scope. After completing the configuration, click **Run** to preview the parsed pages.

<figure><img src="https://assets-docs.dify.ai/2024/12/3e63b4ced9770e21d5132c3aa8e5d2de.png" alt=""><figcaption><p>Execute scraping</p></figcaption></figure>

#### 3. Review import results

After importing the parsed text from the webpage, it is stored in the knowledge base documents. View the import results and click **Add URL** to continue importing new web pages.

***

### Jina Reader

#### 1. Configuring Jina Reader Credentials&#x20;

Click on the avatar in the upper right corner, then go to the **DataSource** page, and click the **Configure** button next to Jina Reader.

<figure><img src="https://assets-docs.dify.ai/2024/12/28b37f9b36fe808b2d3302c48fce5ea3.png" alt=""><figcaption><p>Configuring Jina Reader</p></figcaption></figure>

Log in to the [Jina Reader website](https://jina.ai/reader/), complete registration, obtain the API Key, then fill it in and save.

#### 2. Using Jina Reader to Crawl Web Content&#x20;

On the knowledge base creation page, select **Sync from website**, choose Jina Reader as the provider, and enter the target URL to be crawled.

<figure><img src="https://assets-docs.dify.ai/2024/12/f9170b2a2ab1be94bc85ff3ed3c3e723.png" alt=""><figcaption><p>Web crawling configuration </p></figcaption></figure>

Configuration options include: whether to crawl subpages, maximum number of pages to crawl, and whether to use sitemap for crawling. After completing the configuration, click the **Run** button to preview the page links to be crawled.

<figure><img src="https://assets-docs.dify.ai/2024/12/a875f21a751551c03109c76308c577ee.png" alt=""><figcaption><p>Executing the crawl process</p></figcaption></figure>

After importing the parsed text from web pages into the knowledge base, you can review the imported results in the documents section. To add more web pages, click the **Add URL** button on the right to continue importing new pages.

<figure><img src="https://assets-docs.dify.ai/2024/12/03494dc3c882ac1c74b464ea931e2533.png" alt=""><figcaption><p>Importing parsed web text into the knowledge base</p></figcaption></figure>

After crawling is complete, the content from the web pages will be incorporated into the knowledge base.
```

## File: en/guides/management/app-management.md
```markdown
# App Management

### Editing Application Information

After creating an application, if you want to modify the application name or description, you can click "Edit info" in the upper left corner of the application to revise the application's icon, name, or description.

<figure><img src="../../.gitbook/assets/image (92).png" alt=""><figcaption><p>Edit App Info</p></figcaption></figure>

### Duplicating Application

All applications support copying. Click "Duplicate" in the upper left corner of the application.

### Exporting Application

Applications created in Dify support export in DSL format files, allowing you to import the configuration files into other Dify teams freely. You can export DSL files using either of the following two methods:

* Click "Export DSL" in the application menu button on the "Studio" page
* After entering the application's orchestration page, click "Export DSL" in the upper left corner

![](../../.gitbook/assets/export-dsl.png)

The DSL file does not include authorization information already filled in [Tool](../workflow/node/tools.md) nodes, such as API keys for third-party services.&#x20;

If the environment variables contain variables of the `Secret` type, a prompt will appear during file export asking whether to allow the export of this sensitive information.

![](../../.gitbook/assets/export-dsl-secret.png)

{% hint style="info" %}
Dify DSL is an AI application engineering file standard defined by Dify.AI in v0.6 and later. The file format is YML. This standard covers the basic description of the application, model parameters, orchestration configuration, and other information.
{% endhint %}

### Importing Application

To import a Dify application, upload the DSL file to the Dify platform. A version check will be conducted during the import process, and a warning will be issued if a lower version of the DSL file is detected.

- For SaaS users, the DSL file exported from the SaaS platform will always be the latest version.
- For Community users, it is recommended to consult [Upgrade Dify](https://docs.dify.ai/getting-started/install-self-hosted/docker-compose#upgrade-dify) to update the Community Edition and export an updated version of the DSL file, thus avoiding potential compatibility issues.

![](https://assets-docs.dify.ai/2024/11/487d2c1cc8b86666feb35ea8a346c053.png)

### Deleting Application

If you want to remove an application, you can click "Delete" in the upper left corner of the application.

{% hint style="info" %}
⚠️ The deletion of an application cannot be undone. All users will be unable to access your application, and all prompts, orchestration configurations, and logs within the application will be deleted.
{% endhint %}
```

## File: en/guides/management/personal-account-management.md
```markdown
# Personal Account Management

## Login Methods

The login methods supported by different versions of Dify are as follows:

<table><thead><tr><th width="166">Version</th><th>Login Method</th></tr></thead><tbody><tr><td>Community</td><td>Email and password</td></tr><tr><td>Cloud</td><td>GitHub account authorization, Google account authorization, email and verification code</td></tr></tbody></table>

> Note: For Dify Cloud Service, if the email associated with a GitHub or Google account is the same as the email used to log in with a verification code, the system will automatically link them as the same account, avoiding the need for manual binding and preventing duplicate registrations.

<figure><img src="../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

## Modifying Personal Information

To update your personal account information:

1. Navigate to the Dify team homepage
2. Click on your avatar in the upper right corner
3. Select **"My Account"**

You can modify the following details:

* Avatar
* Username
* Email
* Password

> Note: The password reset feature is only available in the Community Version.

<figure><img src="../../.gitbook/assets/image (1) (3).png" alt=""><figcaption></figcaption></figure>

### Login Methods

Supports 3 login methods: email + verification code, Google authentication, and GitHub authentication. The same Dify account can log in directly using email + verification code or through Google/GitHub authentication linked to the same email, without the need for additional binding.

### Changing Display Language

To change the display language, click on your avatar in the upper right corner of the Dify team homepage, then click **"Language"**. Dify supports the following languages:

* English
* Simplified Chinese
* Traditional Chinese
* Portuguese (Brazil)
* French (France)
* Japanese (Japan)
* Korean (South Korea)
* Russian (Russia)
* Italian (Italy)
* Thai (Thailand)
* Indonesian
* Ukrainian (Ukraine)

Dify welcomes community volunteers to contribute additional language versions. Visit the [GitHub repository](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md) to contribute!

### View Apps Linked to Your Account

You can view the apps currently linked to your account on the **Account** page.

### Delete Personal Account

⚠️ Dangerous Operation. Please proceed with caution.

To confirm the deletion of your Dify SaaS account, click on your avatar in the top right corner, select **“Account”** from the dropdown menu, and then click the **“Delete Account”** button.

Deleting your account is irreversible, and the same email address cannot be re-registered within 30 days. All workspaces owned by the account will also be deleted, and it will be automatically removed from all shared workspaces.

Enter the email address you want to delete and the confirmation verification code. Afterward, the system will permanently delete all information related to the account.

<figure><img src="https://assets-docs.dify.ai/2024/12/ded326f27886b5884969c220ead998d7.png" alt=""><figcaption><p>Delete Personal Account</p></figcaption></figure>

### FAQ

**1. Can I revert the account deletion if I accidentally delete my account?**\
Account deletion is irreversible. If there are exceptional circumstances, please contact us at `support@dify.ai` within 20 days of the deletion and provide a detailed explanation.

**2. What happens to my roles and data in the team after I delete my account?**\
After account deletion:

* If you were the **team owner**, the workspace(s) you created will be dissolved, and all data within those workspaces will be deleted. Team members will lose access to the workspace.
* If you were a **team member or admin**, the workspaces you joined will retain their data, including the applications created by your account. Your account will be removed from the member list of those workspaces.

**3. Can I re-register a new account with the same email after deleting my account?**\
You cannot re-register a new account using the same email within 30 days of account deletion.

**4. Will my authorizations with third-party services (e.g., Google, GitHub) be revoked after deleting my account?**\
Yes, all authorizations with third-party services (e.g., Google, GitHub) will be automatically revoked after account deletion.

**5. Will my Dify subscription be canceled and refunded after deleting my account?**\
Your Dify subscription will be automatically canceled upon account deletion. However, the subscription fee is non-refundable, and no future charges will be made.
```

## File: en/guides/management/README.md
```markdown
# Management
```

## File: en/guides/management/subscription-management.md
```markdown
# Subscription Management

### Upgrading Dify Team Subscription

Team owners and administrators can upgrade the team subscription plan. Click the **"Upgrade"** button in the upper right corner of the Dify team homepage, select an appropriate package, and complete the payment to upgrade the team's subscription.

### Managing Dify Team Subscription

After subscribing to Dify's paid services (Professional or Team plan), team owners and administrators can navigate to **"Settings"** → **"Billing"** to manage the team's billing and subscription details.

On the billing page, you can view the usage statistics for various team resources.

<figure><img src="../../.gitbook/assets/subscription-management-01.png" alt=""><figcaption><p>Team billing management</p></figcaption></figure>

### Frequently Asked Questions

#### 1. How to upgrade/downgrade the team plan or cancel a subscription?

Team owners and administrators can navigate to **Settings** → **Billing**, then click on **Manage billing and subscription** to change the subscription plan.

* Upgrading from Professional to Team plan requires paying the difference for the current month and takes effect immediately.
* Downgrading from Team to Professional plan takes effect immediately.

<figure><img src="../../.gitbook/assets/subscription-management-02.jpeg" alt=""><figcaption><p>Changing the paid plan</p></figcaption></figure>

Upon cancellation of the subscription plan, **the team will automatically transition to the Sandbox/Free plan at the end of the current billing cycle**. Subsequently, any team members and resources exceeding the Sandbox/Free plan limitations will become inaccessible.

#### 2. What changes will occur to the team's available resources after upgrading the subscription plan?

| Resource                                                                     | Free      | Professional   | Team            |
| ---------------------------------------------------------------------------- | --------- | -------------- | --------------- |
| Team member limit                                                            | 1         | 3              | Unlimited       |
| Application limit                                                            | 10        | 50             | Unlimited       |
| Vector space capacity                                                        | 5MB       | 200MB          | 1GB             |
| [Marked replies](https://docs.dify.ai/guides/biao-zhu/logs) for applications | 10        | 2000           | 5000            |
| Document uploads for knowledge base                                          | 50        | 500            | 1000            |
| OpenAI conversation quota                                                    | 200 total | 5000 per month | 10000 per month |

Note:

* When upgrading from Free to Professional, all resources are increased as shown in the table.
* When upgrading from Professional to Team, resources are further expanded, with some becoming unlimited.

After upgrading the subscription plan:

* The OpenAI conversation quota will be reset to the new limit for the current billing cycle.
* Previously used computational resources (e.g., vector space usage, document uploads) will not be reset or removed.

#### 3. What if I forget to renew subscription on time?

If you forget to renew your subscription, the team will automatically downgrade to the Sandbox/Free version. Except for the team owner, others will not be able to continue accessing the team. Excess computational resources within the team (such as documents, vector space, etc.) will also be locked.

#### 4. Will deleting the team owner's account affect the team?

A team needs to be bound to one team owner. If the team ownership is not transferred to another team member in time, all data of the current team will be deleted along with the owner's account.

#### 5. What are the differences between the subscription versions?

For a detailed feature comparison, please refer to the [Dify pricing](https://dify.ai/pricing).
```

## File: en/guides/management/team-members-management.md
```markdown
# Team Members Management

This guide explains how to manage members within a Dify team. The team member limits for different Dify versions are below.

| Sandbox / Free | Professional | Team      | Community | Enterprise |
| -------------- | ------------ | --------- | --------- | ---------- |
| 1              | 3            | Unlimited | Unlimited | Unlimited  |

### Adding Members

{% hint style="info" %}
Only team owners have permission to invite team members.
{% endhint %}

To add a member, the team owner can click on the avatar in the upper right corner, then select **"Members"** → **"Add"**. Enter the email address and assign member permissions to complete the process.

<figure><img src="../../.gitbook/assets/team-members-management-01.png" alt=""><figcaption><p>Assigning permissions to team members</p></figcaption></figure>

> For Community Edition, enabling email functionality requires the team owner to configure and activate the email service via system [environment variables](https://docs.dify.ai/getting-started/install-self-hosted/environments).

- If the invited member has not registered with Dify, they will receive an invitation email. They can complete registration by clicking the link in the email.
- If the invited member is already registered with Dify, permissions will be automatically assigned and **no invitation email will be sent**. The invited member can switch to the new workspace via the menu in the top right corner.

![](../../../img/switch-workspace.png)

### Member Permissions

Team members are divided into owners, administrators, editors, and members.

* **Owner**
  * Role description: The first member of the team, with the highest level of permissions, responsible for the operation and management of the entire team.
  * Permission overview: Has permissions to manage team members, adjust member permissions, set model providers, create and delete applications, create knowledge bases, set tool libraries, etc.
* **Administrator**
  * Role description: Team administrator, responsible for managing team members and model providers.
  * Permission overview: Cannot adjust member permissions; has permissions to add or remove team members, set model providers, create, edit and delete applications, create knowledge bases, set tool libraries, etc.
* **Editor**
  * Role description: Regular team member, responsible for collaboratively creating and editing applications.
  * Permission overview: Cannot manage team members, set model providers, or set tool libraries; has permissions to create, edit and delete applications, create knowledge bases.
* **Member**
  * Role description: Regular team member, only allowed to view and use applications created within the team.
  * Permission overview: Only has permissions to use applications within the team and use tools.

### Removing Members

{% hint style="info" %}
Only team owners have permission to remove team members.
{% endhint %}

To remove a member, click on the avatar in the upper right corner of the Dify team homepage, navigate to **"Settings"** → **"Members"**, select the member to be removed, and click **"Remove from team"**.

<figure><img src="../../.gitbook/assets/team-members-management-02.png" alt=""><figcaption><p>Removing a member</p></figcaption></figure>

### Frequently Asked Questions

#### 1. How can I transfer team ownership?

Team owners have the highest level of permissions. To maintain the stability of the team structure, team ownership cannot be manually transferred once established.

#### 2. How can I delete a team?

For team data security reasons, team owners cannot delete their teams on their own.

#### 3. How can I delete a team member's account?

Neither team owners nor administrators can delete a team member's account. Account deletion requires the account owner to actively request it, and cannot be performed by others. As an alternative to account deletion, removing a member from the team will revoke that user's access to the team.
```

## File: en/guides/model-configuration/customizable-model.md
```markdown
# Custom Model Integration

### Introduction

After completing vendor integration, the next step is to integrate models under the vendor. To help understand the entire integration process, we will use `Xinference` as an example to gradually complete a full vendor integration.

It is important to note that for custom models, each model integration requires a complete vendor credential.

Unlike predefined models, custom vendor integration will always have the following two parameters, which do not need to be defined in the vendor YAML file.

![](../../.gitbook/assets/customizable-model.png)

In the previous section, we have learned that vendors do not need to implement `validate_provider_credential`. The Runtime will automatically call the corresponding model layer's `validate_credentials` based on the model type and model name selected by the user for validation.

#### Writing Vendor YAML

First, we need to determine what types of models the vendor supports.

Currently supported model types are as follows:

* `llm` Text Generation Model
* `text_embedding` Text Embedding Model
* `rerank` Rerank Model
* `speech2text` Speech to Text
* `tts` Text to Speech
* `moderation` Moderation

`Xinference` supports `LLM`, `Text Embedding`, and `Rerank`, so we will start writing `xinference.yaml`.

```yaml
provider: xinference # Specify vendor identifier
label: # Vendor display name, can be set in en_US (English) and zh_Hans (Simplified Chinese). If zh_Hans is not set, en_US will be used by default.
  en_US: Xorbits Inference
icon_small: # Small icon, refer to other vendors' icons, stored in the _assets directory under the corresponding vendor implementation directory. Language strategy is the same as label.
  en_US: icon_s_en.svg
icon_large: # Large icon
  en_US: icon_l_en.svg
help: # Help
  title:
    en_US: How to deploy Xinference
    zh_Hans: 如何部署 Xinference
  url:
    en_US: https://github.com/xorbitsai/inference
supported_model_types: # Supported model types. Xinference supports LLM/Text Embedding/Rerank
- llm
- text-embedding
- rerank
configurate_methods: # Since Xinference is a locally deployed vendor and does not have predefined models, you need to deploy the required models according to Xinference's documentation. Therefore, only custom models are supported here.
- customizable-model
provider_credential_schema:
  credential_form_schemas:
```

Next, we need to consider what credentials are required to define a model in Xinference.

* It supports three different types of models, so we need `model_type` to specify the type of the model. It has three types, so we write it as follows:

```yaml
provider_credential_schema:
  credential_form_schemas:
  - variable: model_type
    type: select
    label:
      en_US: Model type
      zh_Hans: 模型类型
    required: true
    options:
    - value: text-generation
      label:
        en_US: Language Model
        zh_Hans: 语言模型
    - value: embeddings
      label:
        en_US: Text Embedding
    - value: reranking
      label:
        en_US: Rerank
```

* Each model has its own name `model_name`, so we need to define it here.

```yaml
  - variable: model_name
    type: text-input
    label:
      en_US: Model name
      zh_Hans: 模型名称
    required: true
    placeholder:
      zh_Hans: 填写模型名称
      en_US: Input model name
```

* Provide the address for the local deployment of Xinference.

```yaml
  - variable: server_url
    label:
      zh_Hans: 服务器URL
      en_US: Server url
    type: text-input
    required: true
    placeholder:
      zh_Hans: 在此输入Xinference的服务器地址，如 https://example.com/xxx
      en_US: Enter the url of your Xinference, for example https://example.com/xxx
```

* Each model has a unique `model_uid`, so we need to define it here.

```yaml
  - variable: model_uid
    label:
      zh_Hans: 模型 UID
      en_US: Model uid
    type: text-input
    required: true
    placeholder:
      zh_Hans: 在此输入你的 Model UID
      en_US: Enter the model uid
```

Now, we have completed the basic definition of the vendor.

#### Writing Model Code

Next, we will take the `llm` type as an example and write `xinference.llm.llm.py`.

In `llm.py`, create a Xinference LLM class, which we will name `XinferenceAILargeLanguageModel` (arbitrary name), inheriting from the `__base.large_language_model.LargeLanguageModel` base class. Implement the following methods:

*   LLM Invocation

    Implement the core method for LLM invocation, which can support both streaming and synchronous returns.

    ```python
    def _invoke(self, model: str, credentials: dict,
                prompt_messages: list[PromptMessage], model_parameters: dict,
                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,
                stream: bool = True, user: Optional[str] = None) \
            -> Union[LLMResult, Generator]:
        """
        Invoke large language model

        :param model: model name
        :param credentials: model credentials
        :param prompt_messages: prompt messages
        :param model_parameters: model parameters
        :param tools: tools for tool calling
        :param stop: stop words
        :param stream: is stream response
        :param user: unique user id
        :return: full response or stream response chunk generator result
        """
    ```

    When implementing, note that you need to use two functions to return data, one for handling synchronous returns and one for streaming returns. This is because Python identifies functions containing the `yield` keyword as generator functions, and the return data type is fixed as `Generator`. Therefore, synchronous and streaming returns need to be implemented separately, as shown below (note that the example uses simplified parameters; the actual implementation should follow the parameter list above):

    ```python
    def _invoke(self, stream: bool, **kwargs) \
            -> Union[LLMResult, Generator]:
        if stream:
              return self._handle_stream_response(**kwargs)
        return self._handle_sync_response(**kwargs)

    def _handle_stream_response(self, **kwargs) -> Generator:
        for chunk in response:
              yield chunk
    def _handle_sync_response(self, **kwargs) -> LLMResult:
        return LLMResult(**response)
    ```
*   Precompute Input Tokens

    If the model does not provide a precompute tokens interface, it can directly return 0.

    ```python
    def get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],
                     tools: Optional[list[PromptMessageTool]] = None) -> int:
      """
      Get number of tokens for given prompt messages

      :param model: model name
      :param credentials: model credentials
      :param prompt_messages: prompt messages
      :param tools: tools for tool calling
      :return:
      """
    ```

    Sometimes, you may not want to directly return 0, so you can use `self._get_num_tokens_by_gpt2(text: str)` to get precomputed tokens. This method is located in the `AIModel` base class and uses GPT2's Tokenizer for calculation. However, it can only be used as an alternative method and is not completely accurate.
*   Model Credential Validation

    Similar to vendor credential validation, this is for validating individual model credentials.

    ```python
    def validate_credentials(self, model: str, credentials: dict) -> None:
        """
        Validate model credentials

        :param model: model name
        :param credentials: model credentials
        :return:
        """
    ```
*   Model Parameter Schema

    Unlike custom types, since a model's supported parameters are not defined in the YAML file, we need to dynamically generate the model parameter schema.

    For example, Xinference supports the `max_tokens`, `temperature`, and `top_p` parameters.

    However, some vendors support different parameters depending on the model. For instance, the vendor `OpenLLM` supports `top_k`, but not all models provided by this vendor support `top_k`. Here, we illustrate that Model A supports `top_k`, while Model B does not. Therefore, we need to dynamically generate the model parameter schema, as shown below:

    ```python
    def get_customizable_model_schema(self, model: str, credentials: dict) -> AIModelEntity | None:
        """
            Used to define customizable model schema
        """
        rules = [
            ParameterRule(
                name='temperature', type=ParameterType.FLOAT,
                use_template='temperature',
                label=I18nObject(
                    zh_Hans='温度', en_US='Temperature'
                )
            ),
            ParameterRule(
                name='top_p', type=ParameterType.FLOAT,
                use_template='top_p',
                label=I18nObject(
                    zh_Hans='Top P', en_US='Top P'
                )
            ),
            ParameterRule(
                name='max_tokens', type=ParameterType.INT,
                use_template='max_tokens',
                min=1,
                default=512,
                label=I18nObject(
                    zh_Hans='最大生成长度', en_US='Max Tokens'
                )
            )
        ]

        # if model is A, add top_k to rules
        if model == 'A':
            rules.append(
                ParameterRule(
                    name='top_k', type=ParameterType.INT,
                    use_template='top_k',
                    min=1,
                    default=50,
                    label=I18nObject(
                        zh_Hans='Top K', en_US='Top K'
                    )
                )
            )

        """
            some NOT IMPORTANT code here
        """

        entity = AIModelEntity(
            model=model,
            label=I18nObject(
                en_US=model
            ),
            fetch_from=FetchFrom.CUSTOMIZABLE_MODEL,
            model_type=model_type,
            model_properties={ 
                ModelPropertyKey.MODE:  ModelType.LLM,
            },
            parameter_rules=rules
        )

        return entity
    ```
*   Invocation Error Mapping Table

    When a model invocation error occurs, it needs to be mapped to the Runtime-specified `InvokeError` type to facilitate Dify's different subsequent processing for different errors.

    Runtime Errors:

    * `InvokeConnectionError` Invocation connection error
    * `InvokeServerUnavailableError` Invocation server unavailable
    * `InvokeRateLimitError` Invocation rate limit reached
    * `InvokeAuthorizationError` Invocation authorization failed
    * `InvokeBadRequestError` Invocation parameter error

    ```python
    @property
    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:
        """
        Map model invoke error to unified error
        The key is the error type thrown to the caller
        The value is the error type thrown by the model,
        which needs to be converted into a unified error type for the caller.

        :return: Invoke error mapping
        """
    ```

For an explanation of interface methods, see: [Interfaces](https://github.com/langgenius/dify/blob/main/api/core/model\_runtime/docs/en\_US/interfaces.md). For specific implementations, refer to: [llm.py](https://github.com/langgenius/dify-runtime/blob/main/lib/model\_providers/anthropic/llm/llm.py).
```

## File: en/guides/model-configuration/interfaces.md
```markdown
# Interface Methods

This section describes the interface methods and parameter explanations that need to be implemented by providers and various model types.

## Provider

Inherit the `__base.model_provider.ModelProvider` base class and implement the following interfaces:

```python
def validate_provider_credentials(self, credentials: dict) -> None:
    """
    Validate provider credentials
    You can choose any validate_credentials method of model type or implement validate method by yourself,
    such as: get model list api

    if validate failed, raise exception

    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.
    """
```

- `credentials` (object) Credential information

  The parameters of credential information are defined by the `provider_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

If verification fails, throw the `errors.validate.CredentialsValidateFailedError` error.

## Model

Models are divided into 5 different types, each inheriting from different base classes and requiring the implementation of different methods.

All models need to uniformly implement the following 2 methods:

- Model Credential Verification

  Similar to provider credential verification, this step involves verification for an individual model.


  ```python
  def validate_credentials(self, model: str, credentials: dict) -> None:
      """
      Validate model credentials
  
      :param model: model name
      :param credentials: model credentials
      :return:
      """
  ```

  Parameters:

  - `model` (string) Model name

  - `credentials` (object) Credential information

    The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

  If verification fails, throw the `errors.validate.CredentialsValidateFailedError` error.

- Invocation Error Mapping Table

  When there is an exception in model invocation, it needs to be mapped to the `InvokeError` type specified by Runtime. This facilitates Dify's ability to handle different errors with appropriate follow-up actions.

  Runtime Errors:

  - `InvokeConnectionError` Invocation connection error
  - `InvokeServerUnavailableError` Invocation service provider unavailable
  - `InvokeRateLimitError` Invocation reached rate limit
  - `InvokeAuthorizationError` Invocation authorization failure
  - `InvokeBadRequestError` Invocation parameter error

  ```python
  @property
  def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:
      """
      Map model invoke error to unified error
      The key is the error type thrown to the caller
      The value is the error type thrown by the model,
      which needs to be converted into a unified error type for the caller.
  
      :return: Invoke error mapping
      """
  ```

​	You can refer to OpenAI's `_invoke_error_mapping` for an example.

### LLM

Inherit the `__base.large_language_model.LargeLanguageModel` base class and implement the following interfaces:

- LLM Invocation

  Implement the core method for LLM invocation, which can support both streaming and synchronous returns.


  ```python
  def _invoke(self, model: str, credentials: dict,
              prompt_messages: list[PromptMessage], model_parameters: dict,
              tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,
              stream: bool = True, user: Optional[str] = None) \
          -> Union[LLMResult, Generator]:
      """
      Invoke large language model
  
      :param model: model name
      :param credentials: model credentials
      :param prompt_messages: prompt messages
      :param model_parameters: model parameters
      :param tools: tools for tool calling
      :param stop: stop words
      :param stream: is stream response
      :param user: unique user id
      :return: full response or stream response chunk generator result
      """
  ```

  - Parameters:

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `prompt_messages` (array[[PromptMessage](#PromptMessage)]) List of prompts

      If the model is of the `Completion` type, the list only needs to include one [UserPromptMessage](#UserPromptMessage) element;

      If the model is of the `Chat` type, it requires a list of elements such as [SystemPromptMessage](#SystemPromptMessage), [UserPromptMessage](#UserPromptMessage), [AssistantPromptMessage](#AssistantPromptMessage), [ToolPromptMessage](#ToolPromptMessage) depending on the message.

    - `model_parameters` (object) Model parameters

      The model parameters are defined by the `parameter_rules` in the model's YAML configuration.

    - `tools` (array[[PromptMessageTool](#PromptMessageTool)]) [optional] List of tools, equivalent to the `function` in `function calling`.

      That is, the tool list for tool calling.

    - `stop` (array[string]) [optional] Stop sequences

      The model output will stop before the string defined by the stop sequence.

    - `stream` (bool) Whether to output in a streaming manner, default is True

      Streaming output returns Generator[[LLMResultChunk](#LLMResultChunk)], non-streaming output returns [LLMResult](#LLMResult).

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns

    Streaming output returns Generator[[LLMResultChunk](#LLMResultChunk)], non-streaming output returns [LLMResult](#LLMResult).

- Pre-calculating Input Tokens

  If the model does not provide a pre-calculated tokens interface, you can directly return 0.

  ```python
  def get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],
                     tools: Optional[list[PromptMessageTool]] = None) -> int:
      """
      Get number of tokens for given prompt messages

      :param model: model name
      :param credentials: model credentials
      :param prompt_messages: prompt messages
      :param tools: tools for tool calling
      :return:
      """
  ```

  For parameter explanations, refer to the above section on `LLM Invocation`.

- Fetch Custom Model Schema [Optional]

  ```python
  def get_customizable_model_schema(self, model: str, credentials: dict) -> Optional[AIModelEntity]:
      """
      Get customizable model schema

      :param model: model name
      :param credentials: model credentials
      :return: model schema
      """
  ```

  When the provider supports adding custom LLMs, this method can be implemented to allow custom models to fetch model schema. The default return null.


### TextEmbedding

Inherit the `__base.text_embedding_model.TextEmbeddingModel` base class and implement the following interfaces:

- Embedding Invocation

  ```python
  def _invoke(self, model: str, credentials: dict,
              texts: list[str], user: Optional[str] = None) \
          -> TextEmbeddingResult:
      """
      Invoke large language model
  
      :param model: model name
      :param credentials: model credentials
      :param texts: texts to embed
      :param user: unique user id
      :return: embeddings result
      """
  ```

  - Parameters:

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `texts` (array[string]) List of texts, capable of batch processing

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns:

    [TextEmbeddingResult](#TextEmbeddingResult) entity.

- Pre-calculating Tokens

  ```python
  def get_num_tokens(self, model: str, credentials: dict, texts: list[str]) -> int:
      """
      Get number of tokens for given prompt messages

      :param model: model name
      :param credentials: model credentials
      :param texts: texts to embed
      :return:
      """
  ```

  For parameter explanations, refer to the above section on `Embedding Invocation`.

### Rerank

Inherit the `__base.rerank_model.RerankModel` base class and implement the following interfaces:

- Rerank Invocation

  ```python
  def _invoke(self, model: str, credentials: dict,
              query: str, docs: list[str], score_threshold: Optional[float] = None, top_n: Optional[int] = None,
              user: Optional[str] = None) \
          -> RerankResult:
      """
      Invoke rerank model
  
      :param model: model name
      :param credentials: model credentials
      :param query: search query
      :param docs: docs for reranking
      :param score_threshold: score threshold
      :param top_n: top n
      :param user: unique user id
      :return: rerank result
      """
  ```

  - Parameters:

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `query` (string) Query request content

    - `docs` (array[string]) List of segments to be reranked

    - `score_threshold` (float) [optional] Score threshold

    - `top_n` (int) [optional] Select the top n segments

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns:

    [RerankResult](#RerankResult) entity.

### Speech2text

Inherit the `__base.speech2text_model.Speech2TextModel` base class and implement the following interfaces:

- Invoke Invocation

  ```python
  def _invoke(self, model: str, credentials: dict, file: IO[bytes], user: Optional[str] = None) -> str:
      """
      Invoke large language model
  
      :param model: model name
      :param credentials: model credentials
      :param file: audio file
      :param user: unique user id
      :return: text for given audio file
      """	
  ```

  - Parameters:

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `file` (File) File stream

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns:

    The string after speech-to-text conversion.

### Text2speech

Inherit the `__base.text2speech_model.Text2SpeechModel` base class and implement the following interfaces:

- Invoke Invocation

  ```python
  def _invoke(self, model: str, credentials: dict, content_text: str, streaming: bool, user: Optional[str] = None):
      """
      Invoke large language model
  
      :param model: model name
      :param credentials: model credentials
      :param content_text: text content to be translated
      :param streaming: output is streaming
      :param user: unique user id
      :return: translated audio file
      """	
  ```

  - Parameters：

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `content_text` (string) The text content that needs to be converted

    - `streaming` (bool) Whether to stream output

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns：

    Text converted speech stream。

### Moderation

Inherit the `__base.moderation_model.ModerationModel` base class and implement the following interfaces:

- Invoke Invocation

  ```python
  def _invoke(self, model: str, credentials: dict,
              text: str, user: Optional[str] = None) \
          -> bool:
      """
      Invoke large language model
  
      :param model: model name
      :param credentials: model credentials
      :param text: text to moderate
      :param user: unique user id
      :return: false if text is safe, true otherwise
      """
  ```

  - Parameters:

    - `model` (string) Model name

    - `credentials` (object) Credential information

      The parameters of credential information are defined by either the `provider_credential_schema` or `model_credential_schema` in the provider's YAML configuration file. Inputs such as `api_key` are included.

    - `text` (string) Text content

    - `user` (string) [optional] Unique identifier of the user

      This can help the provider monitor and detect abusive behavior.

  - Returns:

    False indicates that the input text is safe, True indicates otherwise.



## Entities

### PromptMessageRole 

Message role

```python
class PromptMessageRole(Enum):
    """
    Enum class for prompt message.
    """
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"
```

### PromptMessageContentType

Message content types, divided into text and image.

```python
class PromptMessageContentType(Enum):
    """
    Enum class for prompt message content type.
    """
    TEXT = 'text'
    IMAGE = 'image'
```

### PromptMessageContent

Message content base class, used only for parameter declaration and cannot be initialized.

```python
class PromptMessageContent(BaseModel):
    """
    Model class for prompt message content.
    """
    type: PromptMessageContentType
    data: str
```

Currently, two types are supported: text and image. It's possible to simultaneously input text and multiple images.

You need to initialize `TextPromptMessageContent` and `ImagePromptMessageContent` separately for input.

### TextPromptMessageContent

```python
class TextPromptMessageContent(PromptMessageContent):
    """
    Model class for text prompt message content.
    """
    type: PromptMessageContentType = PromptMessageContentType.TEXT
```

If inputting a combination of text and images, the text needs to be constructed into this entity as part of the `content` list.

### ImagePromptMessageContent

```python
class ImagePromptMessageContent(PromptMessageContent):
    """
    Model class for image prompt message content.
    """
    class DETAIL(Enum):
        LOW = 'low'
        HIGH = 'high'

    type: PromptMessageContentType = PromptMessageContentType.IMAGE
    detail: DETAIL = DETAIL.LOW  # Resolution
```

If inputting a combination of text and images, the images need to be constructed into this entity as part of the `content` list.

`data` can be either a `url` or a `base64` encoded string of the image.

### PromptMessage

The base class for all Role message bodies, used only for parameter declaration and cannot be initialized.

```python
class PromptMessage(ABC, BaseModel):
    """
    Model class for prompt message.
    """
    role: PromptMessageRole
    content: Optional[str | list[PromptMessageContent]] = None  # Supports two types: string and content list. The content list is designed to meet the needs of multimodal inputs. For more details, see the PromptMessageContent explanation.
    name: Optional[str] = None
```

### UserPromptMessage

UserMessage message body, representing a user's message.

```python
class UserPromptMessage(PromptMessage):
    """
    Model class for user prompt message.
    """
    role: PromptMessageRole = PromptMessageRole.USER
```

### AssistantPromptMessage

Represents a message returned by the model, typically used for `few-shots` or inputting chat history.

```python
class AssistantPromptMessage(PromptMessage):
    """
    Model class for assistant prompt message.
    """
    class ToolCall(BaseModel):
        """
        Model class for assistant prompt message tool call.
        """
        class ToolCallFunction(BaseModel):
            """
            Model class for assistant prompt message tool call function.
            """
            name: str  # tool name
            arguments: str  # tool arguments

        id: str  # Tool ID, effective only in OpenAI tool calls. It's the unique ID for tool invocation and the same tool can be called multiple times.
        type: str  # default: function
        function: ToolCallFunction  # tool call information

    role: PromptMessageRole = PromptMessageRole.ASSISTANT
    tool_calls: list[ToolCall] = []  # The result of tool invocation in response from the model (returned only when tools are input and the model deems it necessary to invoke a tool).
```

Where `tool_calls` are the list of `tool calls` returned by the model after invoking the model with the `tools` input.

### SystemPromptMessage

Represents system messages, usually used for setting system commands given to the model.

```python
class SystemPromptMessage(PromptMessage):
    """
    Model class for system prompt message.
    """
    role: PromptMessageRole = PromptMessageRole.SYSTEM
```

### ToolPromptMessage

Represents tool messages, used for conveying the results of a tool execution to the model for the next step of processing.

```python
class ToolPromptMessage(PromptMessage):
    """
    Model class for tool prompt message.
    """
    role: PromptMessageRole = PromptMessageRole.TOOL
    tool_call_id: str  # Tool invocation ID. If OpenAI tool call is not supported, the name of the tool can also be inputted.
```

The base class's `content` takes in the results of tool execution.

### PromptMessageTool

```python
class PromptMessageTool(BaseModel):
    """
    Model class for prompt message tool.
    """
    name: str
    description: str
    parameters: dict
```

---

### LLMResult

```python
class LLMResult(BaseModel):
    """
    Model class for llm result.
    """
    model: str  # Actual used modele
    prompt_messages: list[PromptMessage]  # prompt messages
    message: AssistantPromptMessage  # response message
    usage: LLMUsage  # usage info
    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition
```

### LLMResultChunkDelta

In streaming returns, each iteration contains the `delta` entity.

```python
class LLMResultChunkDelta(BaseModel):
    """
    Model class for llm result chunk delta.
    """
    index: int
    message: AssistantPromptMessage  # response message
    usage: Optional[LLMUsage] = None  # usage info
    finish_reason: Optional[str] = None  # finish reason, only the last one returns
```

### LLMResultChunk

Each iteration entity in streaming returns.

```python
class LLMResultChunk(BaseModel):
    """
    Model class for llm result chunk.
    """
    model: str  # Actual used modele
    prompt_messages: list[PromptMessage]  # prompt messages
    system_fingerprint: Optional[str] = None  # request fingerprint, refer to OpenAI definition
    delta: LLMResultChunkDelta
```

### LLMUsage

```python
class LLMUsage(ModelUsage):
    """
    Model class for LLM usage.
    """
    prompt_tokens: int  # Tokens used for prompt
    prompt_unit_price: Decimal  # Unit price for prompt
    prompt_price_unit: Decimal  # Price unit for prompt, i.e., the unit price based on how many tokens
    prompt_price: Decimal  # Cost for prompt
    completion_tokens: int  # Tokens used for response
    completion_unit_price: Decimal  # Unit price for response
    completion_price_unit: Decimal  # Price unit for response, i.e., the unit price based on how many tokens
    completion_price: Decimal  # Cost for response
    total_tokens: int  # Total number of tokens used
    total_price: Decimal  # Total cost
    currency: str  # Currency unit
    latency: float  # Request latency (s)
```

---

### TextEmbeddingResult

```python
class TextEmbeddingResult(BaseModel):
    """
    Model class for text embedding result.
    """
    model: str  # Actual model used
    embeddings: list[list[float]]  # List of embedding vectors, corresponding to the input texts list
    usage: EmbeddingUsage  # Usage information
```

### EmbeddingUsage

```python
class EmbeddingUsage(ModelUsage):
    """
    Model class for embedding usage.
    """
    tokens: int  # Number of tokens used
    total_tokens: int  # Total number of tokens used
    unit_price: Decimal  # Unit price
    price_unit: Decimal  # Price unit, i.e., the unit price based on how many tokens
    total_price: Decimal  # Total cost
    currency: str  # Currency unit
    latency: float  # Request latency (s)
```

---

### RerankResult

```python
class RerankResult(BaseModel):
    """
    Model class for rerank result.
    """
    model: str  # Actual model used
    docs: list[RerankDocument]  # Reranked document list	
```

### RerankDocument

```python
class RerankDocument(BaseModel):
    """
    Model class for rerank document.
    """
    index: int  # original index
    text: str
    score: float
```
```

## File: en/guides/model-configuration/load-balancing.md
```markdown
# Load Balancing

Model rate limits are restrictions imposed by model providers on the number of times users or customers can access API services within a specified time frame. These limits help prevent API abuse or misuse, ensure fair access for all users, and control the overall load on the infrastructure.

In enterprise-level large-scale model API calls, high concurrent requests can exceed rate limits and affect user access. Load balancing can distribute API requests across multiple API endpoints, ensuring all users receive the fastest response and the highest model invocation throughput, thereby ensuring stable business operations.

You can enable this feature by navigating to **Model Provider -- Model List -- Configure Model Load Balancing** and adding multiple credentials (API keys) for the same model.

<figure><img src="../../.gitbook/assets/load-balancing.png" alt="" width="563"><figcaption><p>Model Load Balancing</p></figcaption></figure>

{% hint style="info" %}
Model load balancing is a paid feature. You can enable it by [subscribing to SaaS paid services](../../getting-started/cloud.md#subscription-plan) or purchasing the enterprise edition.
{% endhint %}

The default API key is the credential added when initially configuring the model provider. You need to click **Add Configuration** to add different API keys for the same model to use the load balancing feature properly.

<figure><img src="../../.gitbook/assets/load-balancing-1.png" alt="" width="563"><figcaption><p>Configuring Load Balancing</p></figcaption></figure>

**At least one additional model credential** must be added to save and enable load balancing.

You can also **temporarily disable** or **delete** configured credentials.

<figure><img src="../../.gitbook/assets/load-balancing-2.png" alt="" width="563"><figcaption></figcaption></figure>

Once configured, all models with load balancing enabled will be displayed in the model list.

<figure><img src="../../.gitbook/assets/load-balancing-3.png" alt="" width="563"><figcaption><p>Enabling Load Balancing</p></figcaption></figure>

{% hint style="info" %}
By default, load balancing uses the Round-robin strategy. If the rate limit is triggered, a 1-minute cooldown period will be applied.
{% endhint %}

You can also configure load balancing from **Add Model**, following the same process as above.

<figure><img src="../../.gitbook/assets/load-balancing-4.png" alt="" width="563"><figcaption><p>Configuring Load Balancing from Add Model</p></figcaption></figure>
```

## File: en/guides/model-configuration/new-provider.md
```markdown
# Adding a New Provider

### Provider Configuration Methods

Providers support three configuration models:

**Predefined Model**

This indicates that users only need to configure unified provider credentials to use the predefined models under the provider.

**Customizable Model**

Users need to add credentials configuration for each model. For example, Xinference supports both LLM and Text Embedding, but each model has a unique **model_uid**. If you want to connect both, you need to configure a **model_uid** for each model.

**Fetch from Remote**

Similar to the `predefined-model` configuration method, users only need to configure unified provider credentials, and the models are fetched from the provider using the credential information.

For instance, with OpenAI, we can fine-tune multiple models based on gpt-turbo-3.5, all under the same **api_key**. When configured as `fetch-from-remote`, developers only need to configure a unified **api_key** to allow Dify Runtime to fetch all the developer's fine-tuned models and connect to Dify.

These three configuration methods **can coexist**, meaning a provider can support `predefined-model` + `customizable-model` or `predefined-model` + `fetch-from-remote`, etc. This allows using predefined models and models fetched from remote with unified provider credentials, and additional custom models can be used if added.

### Configuration Instructions

**Terminology**

* `module`: A `module` is a Python Package, or more colloquially, a folder containing an `__init__.py` file and other `.py` files.

**Steps**

Adding a new provider mainly involves several steps. Here is a brief outline to give you an overall understanding. Detailed steps will be introduced below.

* Create a provider YAML file and write it according to the [Provider Schema](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/docs/en_US/schema.md).
* Create provider code and implement a `class`.
* Create corresponding model type `modules` under the provider `module`, such as `llm` or `text_embedding`.
* Create same-named code files under the corresponding model `module`, such as `llm.py`, and implement a `class`.
* If there are predefined models, create same-named YAML files under the model `module`, such as `claude-2.1.yaml`, and write them according to the [AI Model Entity](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/docs/en_US/schema.md#aimodelentity).
* Write test code to ensure functionality is available.

#### Let's Get Started

To add a new provider, first determine the provider's English identifier, such as `anthropic`, and create a `module` named after it in `model_providers`.

Under this `module`, we need to prepare the provider's YAML configuration first.

**Preparing Provider YAML**

Taking `Anthropic` as an example, preset the basic information of the provider, supported model types, configuration methods, and credential rules.

```YAML
provider: anthropic  # Provider identifier
label:  # Provider display name, can be set in en_US English and zh_Hans Chinese. If zh_Hans is not set, en_US will be used by default.
  en_US: Anthropic
icon_small:  # Small icon of the provider, stored in the _assets directory under the corresponding provider implementation directory, same language strategy as label
  en_US: icon_s_en.png
icon_large:  # Large icon of the provider, stored in the _assets directory under the corresponding provider implementation directory, same language strategy as label
  en_US: icon_l_en.png
supported_model_types:  # Supported model types, Anthropic only supports LLM
- llm
configurate_methods:  # Supported configuration methods, Anthropic only supports predefined models
- predefined-model
provider_credential_schema:  # Provider credential rules, since Anthropic only supports predefined models, unified provider credential rules need to be defined
  credential_form_schemas:  # Credential form item list
  - variable: anthropic_api_key  # Credential parameter variable name
    label:  # Display name
      en_US: API Key
    type: secret-input  # Form type, secret-input here represents an encrypted information input box, only displaying masked information when editing.
    required: true  # Whether it is required
    placeholder:  # PlaceHolder information
      zh_Hans: 在此输入你的 API Key
      en_US: Enter your API Key
  - variable: anthropic_api_url
    label:
      en_US: API URL
    type: text-input  # Form type, text-input here represents a text input box
    required: false
    placeholder:
      zh_Hans: 在此输入你的 API URL
      en_US: Enter your API URL
```

If the connected provider offers customizable models, such as `OpenAI` which provides fine-tuned models, we need to add [`model_credential_schema`](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/docs/en_US/schema.md). Taking `OpenAI` as an example:

```yaml
model_credential_schema:
  model: # Fine-tuned model name
    label:
      en_US: Model Name
      zh_Hans: 模型名称
    placeholder:
      en_US: Enter your model name
      zh_Hans: 输入模型名称
  credential_form_schemas:
  - variable: openai_api_key
    label:
      en_US: API Key
    type: secret-input
    required: true
    placeholder:
      zh_Hans: 在此输入你的 API Key
      en_US: Enter your API Key
  - variable: openai_organization
    label:
        zh_Hans: 组织 ID
        en_US: Organization
    type: text-input
    required: false
    placeholder:
      zh_Hans: 在此输入你的组织 ID
      en_US: Enter your Organization ID
  - variable: openai_api_base
    label:
      zh_Hans: API Base
      en_US: API Base
    type: text-input
    required: false
    placeholder:
      zh_Hans: 在此输入你的 API Base
      en_US: Enter your API Base
```

You can also refer to the [YAML configuration information](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/docs/en_US/schema.md) in the directories of other providers under the `model_providers` directory.

**Implement Provider Code**

We need to create a Python file with the same name under `model_providers`, such as `anthropic.py`, and implement a `class` that inherits from the `__base.provider.Provider` base class, such as `AnthropicProvider`.

**Custom Model Providers**

For providers like Xinference that offer custom models, this step can be skipped. Just create an empty `XinferenceProvider` class and implement an empty `validate_provider_credentials` method. This method will not actually be used and is only to avoid abstract class instantiation errors.

```python
class XinferenceProvider(Provider):
    def validate_provider_credentials(self, credentials: dict) -> None:
        pass
```

**Predefined Model Providers**

Providers need to inherit from the `__base.model_provider.ModelProvider` base class and implement the `validate_provider_credentials` method to validate the provider's unified credentials. You can refer to [AnthropicProvider](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/model_providers/anthropic/anthropic.py).

```python
def validate_provider_credentials(self, credentials: dict) -> None:
    """
    Validate provider credentials
    You can choose any validate_credentials method of model type or implement validate method by yourself,
    such as: get model list api

    if validate failed, raise exception

    :param credentials: provider credentials, credentials form defined in `provider_credential_schema`.
    """
```

You can also reserve the `validate_provider_credentials` implementation first and directly reuse it after implementing the model credential validation method.

**Adding Models**

[**Adding Predefined Models**](https://docs.dify.ai/v/zh-hans/guides/model-configuration/predefined-model)**👈🏻**

For predefined models, we can connect them by simply defining a YAML file and implementing the calling code.

[**Adding Custom Models**](https://docs.dify.ai/v/zh-hans/guides/model-configuration/customizable-model) **👈🏻**

For custom models, we only need to implement the calling code to connect them, but the parameters they handle may be more complex.

***

#### Testing

To ensure the availability of the connected provider/model, each method written needs to have corresponding integration test code written in the `tests` directory.

Taking `Anthropic` as an example.

Before writing test code, you need to add the credential environment variables required for testing the provider in `.env.example`, such as: `ANTHROPIC_API_KEY`.

Before executing, copy `.env.example` to `.env` and then execute.

**Writing Test Code**

Create a `module` with the same name as the provider under the `tests` directory: `anthropic`, and continue to create `test_provider.py` and corresponding model type test py files in this module, as shown below:

```shell
.
├── __init__.py
├── anthropic
│   ├── __init__.py
│   ├── test_llm.py       # LLM Test
│   └── test_provider.py  # Provider Test
```

Write test code for various situations of the implemented code above, and after passing the tests, submit the code.
```

## File: en/guides/model-configuration/predefined-model.md
```markdown
# Predefined Model Integration

After completing the supplier integration, the next step is to integrate the models under the supplier.

First, we need to determine the type of model to be integrated and create the corresponding model type `module` in the directory of the respective supplier.

The currently supported model types are as follows:

* `llm` Text Generation Model
* `text_embedding` Text Embedding Model
* `rerank` Rerank Model
* `speech2text` Speech to Text
* `tts` Text to Speech
* `moderation` Moderation

Taking `Anthropic` as an example, `Anthropic` only supports LLM, so we create a `module` named `llm` in `model_providers.anthropic`.

For predefined models, we first need to create a YAML file named after the model under the `llm` `module`, such as: `claude-2.1.yaml`.

#### Preparing the Model YAML

```yaml
model: claude-2.1  # Model identifier
# Model display name, can be set in en_US English and zh_Hans Chinese. If zh_Hans is not set, it will default to en_US.
# You can also not set a label, in which case the model identifier will be used.
label:
  en_US: claude-2.1
model_type: llm  # Model type, claude-2.1 is an LLM
features:  # Supported features, agent-thought supports Agent reasoning, vision supports image understanding
- agent-thought
model_properties:  # Model properties
  mode: chat  # LLM mode, complete for text completion model, chat for dialogue model
  context_size: 200000  # Maximum context size supported
parameter_rules:  # Model invocation parameter rules, only LLM needs to provide
- name: temperature  # Invocation parameter variable name
  # There are 5 preset variable content configuration templates: temperature/top_p/max_tokens/presence_penalty/frequency_penalty
  # You can set the template variable name directly in use_template, and it will use the default configuration in entities.defaults.PARAMETER_RULE_TEMPLATE
  # If additional configuration parameters are set, they will override the default configuration
  use_template: temperature
- name: top_p
  use_template: top_p
- name: top_k
  label:  # Invocation parameter display name
    zh_Hans: 取样数量
    en_US: Top k
  type: int  # Parameter type, supports float/int/string/boolean
  help:  # Help information, describes the parameter's function
    zh_Hans: 仅从每个后续标记的前 K 个选项中采样。
    en_US: Only sample from the top K options for each subsequent token.
  required: false  # Whether it is required, can be omitted
- name: max_tokens_to_sample
  use_template: max_tokens
  default: 4096  # Default parameter value
  min: 1  # Minimum parameter value, only applicable to float/int
  max: 4096  # Maximum parameter value, only applicable to float/int
pricing:  # Pricing information
  input: '8.00'  # Input unit price, i.e., Prompt unit price
  output: '24.00'  # Output unit price, i.e., return content unit price
  unit: '0.000001'  # Price unit, the above price is per 100K
  currency: USD  # Price currency
```

It is recommended to prepare all model configurations before starting the implementation of the model code.

Similarly, you can refer to the YAML configuration information in the directories of other suppliers under the `model_providers` directory. The complete YAML rules can be found in: Schema[^1].

#### Implementing Model Invocation Code

Next, create a Python file with the same name `llm.py` under the `llm` `module` to write the implementation code.

Create an Anthropic LLM class in `llm.py`, which we will name `AnthropicLargeLanguageModel` (name can be arbitrary), inheriting from the `__base.large_language_model.LargeLanguageModel` base class, and implement the following methods:

*   LLM Invocation

    Implement the core method for LLM invocation, supporting both streaming and synchronous responses.

    ```python
    def _invoke(self, model: str, credentials: dict,
                prompt_messages: list[PromptMessage], model_parameters: dict,
                tools: Optional[list[PromptMessageTool]] = None, stop: Optional[List[str]] = None,
                stream: bool = True, user: Optional[str] = None) \
            -> Union[LLMResult, Generator]:
        """
        Invoke large language model

        :param model: model name
        :param credentials: model credentials
        :param prompt_messages: prompt messages
        :param model_parameters: model parameters
        :param tools: tools for tool calling
        :param stop: stop words
        :param stream: is stream response
        :param user: unique user id
        :return: full response or stream response chunk generator result
        """
    ```

    When implementing, note to use two functions to return data, one for handling synchronous responses and one for streaming responses. Since Python recognizes functions containing the `yield` keyword as generator functions, returning a fixed data type of `Generator`, synchronous and streaming responses need to be implemented separately, like this (note the example below uses simplified parameters, actual implementation should follow the parameter list above):

    ```python
    def _invoke(self, stream: bool, **kwargs) \
            -> Union[LLMResult, Generator]:
        if stream:
              return self._handle_stream_response(**kwargs)
        return self._handle_sync_response(**kwargs)

    def _handle_stream_response(self, **kwargs) -> Generator:
        for chunk in response:
              yield chunk
    def _handle_sync_response(self, **kwargs) -> LLMResult:
        return LLMResult(**response)
    ```
*   Precompute Input Tokens

    If the model does not provide a precompute tokens interface, return 0 directly.

    ```python
    def get_num_tokens(self, model: str, credentials: dict, prompt_messages: list[PromptMessage],
                       tools: Optional[list[PromptMessageTool]] = None) -> int:
        """
        Get number of tokens for given prompt messages

        :param model: model name
        :param credentials: model credentials
        :param prompt_messages: prompt messages
        :param tools: tools for tool calling
        :return:
        """
    ```
*   Model Credentials Validation

    Similar to supplier credentials validation, this validates the credentials for a single model.

    ```python
    def validate_credentials(self, model: str, credentials: dict) -> None:
        """
        Validate model credentials

        :param model: model name
        :param credentials: model credentials
        :return:
        """
    ```
*   Invocation Error Mapping Table

    When a model invocation error occurs, it needs to be mapped to the `InvokeError` type specified by Runtime, facilitating Dify to handle different errors differently.

    Runtime Errors:

    * `InvokeConnectionError` Invocation connection error
    * `InvokeServerUnavailableError` Invocation service unavailable
    * `InvokeRateLimitError` Invocation rate limit reached
    * `InvokeAuthorizationError` Invocation authorization failed
    * `InvokeBadRequestError` Invocation parameter error

    ```python
    @property
    def _invoke_error_mapping(self) -> dict[type[InvokeError], list[type[Exception]]]:
        """
        Map model invoke error to unified error
        The key is the error type thrown to the caller
        The value is the error type thrown by the model,
        which needs to be converted into a unified error type for the caller.

        :return: Invoke error mapping
        """
    ```

For interface method descriptions, see: [Interfaces](https://github.com/langgenius/dify/blob/main/api/core/model_runtime/docs/en_US/interfaces.md), and for specific implementation, refer to: [llm.py](https://github.com/langgenius/dify-runtime/blob/main/lib/model_providers/anthropic/llm/llm.py).

[^1]: #### Provider

    * `provider` (string) Supplier identifier, e.g., `openai`
    * `label` (object) Supplier display name, i18n, can be set in `en_US` English and `zh_Hans` Chinese
      * `zh_Hans` (string) [optional] Chinese label name, if `zh_Hans` is not set, it will default to `en_US`.
      * `en_US` (string) English label name
    * `description` (object) [optional] Supplier description, i18n
      * `zh_Hans` (string) [optional] Chinese description
      * `en_US` (string) English description
    * `icon_small` (string) [optional] Supplier small icon, stored in the `_assets` directory under the respective supplier implementation directory, follows the same language strategy as `label`
      * `zh_Hans` (string) [optional] Chinese icon
      * `en_US` (string) English icon
    * `icon_large` (string) [optional] Supplier large icon, stored in the `_assets` directory under the respective supplier implementation directory, follows the same language strategy as `label`
      * `zh_Hans` (string) [optional] Chinese icon
      * `en_US` (string) English icon
    * `background` (string) [optional] Background color value, e.g., #FFFFFF, if empty, the default color value will be displayed on the front end.
    * `help` (object) [optional] Help information
      * `title` (object) Help title, i18n
        * `zh_Hans` (string) [optional] Chinese title
        * `en_US` (string) English title
      * `url` (object) Help link, i18n
        * `zh_Hans` (string) [optional] Chinese link
        * `en_US` (string) English link
    * `supported_model_types` (array[ModelType]) Supported model types
    * `configurate_methods` (array[ConfigurateMethod]) Configuration methods
    * `provider_credential_schema` (ProviderCredentialSchema) Supplier credential schema
    * `model_credential_schema` (ModelCredentialSchema) Model credential schema
```

## File: en/guides/model-configuration/README.md
```markdown
---
description: Learn about the Different Models Supported by Dify.
---

# Model

Dify is a development platform for AI application based on LLM Apps, when you are using Dify for the first time, you need to go to **Settings --> Model Providers** to add and configure the LLM you are going to use.

<figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption><p>Settings - Model Provider</p></figcaption></figure>

Dify supports major model providers like OpenAI's GPT series and Anthropic's Claude series. Each model's capabilities and parameters differ, so select a model provider that suits your application's needs. **Obtain the API key from the model provider's official website before using it in Dify.**

## Model Types in Dify

Dify classifies models into 4 types, each for different uses:

1.  **System Inference Models:** Used in applications for tasks like chat, name generation, and suggesting follow-up questions.

    > Providers include [OpenAI](https://platform.openai.com/account/api-keys)、[Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service/)、[Anthropic](https://console.anthropic.com/account/keys)、Hugging Face Hub、Replicate、Xinference、OpenLLM、[iFLYTEK SPARK](https://www.xfyun.cn/solutions/xinghuoAPI)、[WENXINYIYAN](https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application)、[TONGYI](https://dashscope.console.aliyun.com/api-key\_management?spm=a2c4g.11186623.0.0.3bbc424dxZms9k)、[Minimax](https://api.minimax.chat/user-center/basic-information/interface-key)、ZHIPU(ChatGLM)、[Ollama](https://docs.dify.ai/tutorials/model-configuration/ollama)、[LocalAI](https://github.com/mudler/LocalAI)、[GPUStack](https://github.com/gpustack/gpustack).
2.  **Embedding Models:** Employed for embedding segmented documents in knowledge and processing user queries in applications.

    > Providers include OpenAI, ZHIPU (ChatGLM), Jina AI([Jina Embeddings](https://jina.ai/embeddings/)).
3.  [**Rerank Models**](https://docs.dify.ai/advanced/retrieval-augment/rerank)**:** Enhance search capabilities in LLMs.

    > Providers include Cohere, Jina AI([Jina Reranker](https://jina.ai/reranker)).
4.  **Speech-to-Text Models:** Convert spoken words to text in conversational applications.

    > Provider: OpenAI.

Dify plans to add more LLM providers as technology and user needs evolve.

## Hosted Model Trial Service

Dify offers trial quotas for cloud service users to experiment with different models. Set up your model provider before the trial ends to ensure uninterrupted application use.

* OpenAI Hosted Model Trial: Includes 200 invocations for models like GPT3.5-turbo, GPT3.5-turbo-16k, text-davinci-003 models.

## Setting the Default Model

Dify automatically selects the default model based on usage. Configure this in `Settings > Model Provider`.

<figure><img src="../../.gitbook/assets/image-default-models (1).png" alt=""><figcaption></figcaption></figure>

## Model Integration Settings

Choose your model in Dify's `Settings > Model Provider`.

<figure><img src="../../.gitbook/assets/image-20231210143654461 (1).png" alt=""><figcaption></figcaption></figure>

Model providers fall into two categories:

1. Proprietary Models: Developed by providers such as OpenAI and Anthropic.
2. Hosted Models: Offer third-party models, like Hugging Face and Replicate.

Integration methods differ between these categories.

**Proprietary Model Providers:** Dify connects to all models from an integrated provider. Set the provider's API key in Dify to integrate.

{% hint style="info" %}
Dify uses [PKCS1\_OAEP](https://pycryptodome.readthedocs.io/en/latest/src/cipher/oaep.html) encryption to protect your API keys. Each user (tenant) has a unique key pair for encryption, ensuring your API keys remain confidential.
{% endhint %}

**Hosted Model Providers:** Integrate third-party models individually.

Specific integration methods are not detailed here.

* [Hugging Face](https://docs.dify.ai/advanced/model-configuration/hugging-face)
* [Replicate](https://docs.dify.ai/advanced/model-configuration/replicate)
* [Xinference](https://docs.dify.ai/advanced/model-configuration/xinference)
* [OpenLLM](https://docs.dify.ai/advanced/model-configuration/openllm)

## Using Models

Once configured, these models are ready for application use.

<figure><img src="../../.gitbook/assets/choice-model-in-app (1).png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/model-configuration/schema.md
```markdown
# Configuration Rules

- Provider rules are based on the [Provider](#Provider) entity.
- Model rules are based on the [AIModelEntity](#AIModelEntity) entity.

> All entities mentioned below are based on `Pydantic BaseModel` and can be found in the `entities` module.

### Provider

- `provider` (string) Provider identifier, e.g., `openai`
- `label` (object) Provider display name, i18n, with `en_US` English and `zh_Hans` Chinese language settings
  - `zh_Hans` (string) [optional] Chinese label name, if `zh_Hans` is not set, `en_US` will be used by default.
  - `en_US` (string) English label name
- `description` (object) Provider description, i18n
  - `zh_Hans` (string) [optional] Chinese description
  - `en_US` (string) English description
- `icon_small` (string) [optional] Small provider ICON, stored in the `_assets` directory under the corresponding provider implementation directory, with the same language strategy as `label`
  - `zh_Hans` (string) Chinese ICON
  - `en_US` (string) English ICON
- `icon_large` (string) [optional] Large provider ICON, stored in the `_assets` directory under the corresponding provider implementation directory, with the same language strategy as `label`
  - `zh_Hans` (string) Chinese ICON
  - `en_US` (string) English ICON
- `background` (string) [optional] Background color value, e.g., #FFFFFF, if empty, the default frontend color value will be displayed.
- `help` (object) [optional] help information
  - `title` (object) help title, i18n
    - `zh_Hans` (string) [optional] Chinese title
    - `en_US` (string) English title
  - `url` (object) help link, i18n
    - `zh_Hans` (string) [optional] Chinese link
    - `en_US` (string) English link
- `supported_model_types` (array[[ModelType](#ModelType)]) Supported model types
- `configurate_methods` (array[[ConfigurateMethod](#ConfigurateMethod)]) Configuration methods
- `provider_credential_schema` ([ProviderCredentialSchema](#ProviderCredentialSchema)) Provider credential specification
- `model_credential_schema` ([ModelCredentialSchema](#ModelCredentialSchema)) Model credential specification

### AIModelEntity

- `model` (string) Model identifier, e.g., `gpt-3.5-turbo`
- `label` (object) [optional] Model display name, i18n, with `en_US` English and `zh_Hans` Chinese language settings
  - `zh_Hans` (string) [optional] Chinese label name
  - `en_US` (string) English label name
- `model_type` ([ModelType](#ModelType)) Model type
- `features` (array[[ModelFeature](#ModelFeature)]) [optional] Supported feature list
- `model_properties` (object) Model properties
  - `mode` ([LLMMode](#LLMMode)) Mode (available for model type `llm`)
  - `context_size` (int) Context size (available for model types `llm`, `text-embedding`)
  - `max_chunks` (int) Maximum number of chunks (available for model types `text-embedding`, `moderation`)
  - `file_upload_limit` (int) Maximum file upload limit, in MB (available for model type `speech2text`)
  - `supported_file_extensions` (string) Supported file extension formats, e.g., mp3, mp4 (available for model type `speech2text`)
  - `default_voice` (string)  default voice, e.g.：alloy,echo,fable,onyx,nova,shimmer（available for model type `tts`）
  - `voices` (list)  List of available voice.（available for model type `tts`）
    - `mode` (string)  voice model.（available for model type `tts`）
    - `name` (string)  voice model display name.（available for model type `tts`）
    - `language` (string)  the voice model supports languages.（available for model type `tts`）
  - `word_limit` (int)  Single conversion word limit, paragraphwise by default（available for model type `tts`）
  - `audio_type` (string)  Support audio file extension format, e.g.：mp3,wav（available for model type `tts`）
  - `max_workers` (int)  Number of concurrent workers supporting text and audio conversion（available for model type`tts`）
  - `max_characters_per_chunk` (int) Maximum characters per chunk (available for model type `moderation`)
- `parameter_rules` (array[[ParameterRule](#ParameterRule)]) [optional] Model invocation parameter rules
- `pricing` ([PriceConfig](#PriceConfig)) [optional] Pricing information
- `deprecated` (bool) Whether deprecated. If deprecated, the model will no longer be displayed in the list, but those already configured can continue to be used. Default False.

### ModelType

- `llm` Text generation model
- `text-embedding` Text Embedding model
- `rerank` Rerank model
- `speech2text` Speech to text
- `tts` Text to speech
- `moderation` Moderation

### ConfigurateMethod

- `predefined-model` Predefined model

  Indicates that users can use the predefined models under the provider by configuring the unified provider credentials.
- `customizable-model` Customizable model

  Users need to add credential configuration for each model.

- `fetch-from-remote` Fetch from remote

  Consistent with the `predefined-model` configuration method, only unified provider credentials need to be configured, and models are obtained from the provider through credential information.

### ModelFeature

- `agent-thought` Agent reasoning, generally over 70B with thought chain capability.
- `vision` Vision, i.e., image understanding.
- `tool-call`
- `multi-tool-call`
- `stream-tool-call`

### FetchFrom

- `predefined-model` Predefined model
- `fetch-from-remote` Remote model

### LLMMode

- `completion` Text completion
- `chat` Dialogue

### ParameterRule

- `name` (string) Actual model invocation parameter name
- `use_template` (string) [optional] Using template

  By default, 5 variable content configuration templates are preset:

  - `temperature`
  - `top_p`
  - `frequency_penalty`
  - `presence_penalty`
  - `max_tokens`
  
  In use_template, you can directly set the template variable name, which will use the default configuration in entities.defaults.PARAMETER_RULE_TEMPLATE
  No need to set any parameters other than `name` and `use_template`. If additional configuration parameters are set, they will override the default configuration.
  Refer to `openai/llm/gpt-3.5-turbo.yaml`.

- `label` (object) [optional] Label, i18n

  - `zh_Hans`(string) [optional] Chinese label name
  - `en_US` (string) English label name

- `type`(string) [optional] Parameter type

  - `int` Integer
  - `float` Float
  - `string` String
  - `boolean` Boolean

- `help` (string) [optional] Help information

  - `zh_Hans` (string) [optional] Chinese help information
  - `en_US` (string) English help information

- `required` (bool) Required, default False.

- `default`(int/float/string/bool) [optional] Default value

- `min`(int/float) [optional] Minimum value, applicable only to numeric types

- `max`(int/float) [optional] Maximum value, applicable only to numeric types

- `precision`(int) [optional] Precision, number of decimal places to keep, applicable only to numeric types

- `options` (array[string]) [optional] Dropdown option values, applicable only when `type` is `string`, if not set or null, option values are not restricted

### PriceConfig

- `input` (float) Input price, i.e., Prompt price
- `output` (float) Output price, i.e., returned content price
- `unit` (float) Pricing unit, e.g., if the price is meausred in 1M tokens, the corresponding token amount for the unit price is `0.000001`.
- `currency` (string) Currency unit

### ProviderCredentialSchema

- `credential_form_schemas` (array[[CredentialFormSchema](#CredentialFormSchema)]) Credential form standard

### ModelCredentialSchema

- `model` (object) Model identifier, variable name defaults to `model`
  - `label` (object) Model form item display name
    - `en_US` (string) English
    - `zh_Hans`(string) [optional] Chinese
  - `placeholder` (object) Model prompt content
    - `en_US`(string) English
    - `zh_Hans`(string) [optional] Chinese
- `credential_form_schemas` (array[[CredentialFormSchema](#CredentialFormSchema)]) Credential form standard

### CredentialFormSchema

- `variable` (string) Form item variable name
- `label` (object) Form item label name
  - `en_US`(string) English
  - `zh_Hans` (string) [optional] Chinese
- `type` ([FormType](#FormType)) Form item type
- `required` (bool) Whether required
- `default`(string) Default value
- `options` (array[[FormOption](#FormOption)]) Specific property of form items of type `select` or `radio`, defining dropdown content
- `placeholder`(object) Specific property of form items of type `text-input`, placeholder content
  - `en_US`(string) English
  - `zh_Hans` (string) [optional] Chinese
- `max_length` (int) Specific property of form items of type `text-input`, defining maximum input length, 0 for no limit.
- `show_on` (array[[FormShowOnObject](#FormShowOnObject)]) Displayed when other form item values meet certain conditions, displayed always if empty.

### FormType

- `text-input` Text input component
- `secret-input` Password input component
- `select` Single-choice dropdown
- `radio` Radio component
- `switch` Switch component, only supports `true` and `false` values

### FormOption

- `label` (object) Label
  - `en_US`(string) English
  - `zh_Hans`(string) [optional] Chinese
- `value` (string) Dropdown option value
- `show_on` (array[[FormShowOnObject](#FormShowOnObject)]) Displayed when other form item values meet certain conditions, displayed always if empty.

### FormShowOnObject

- `variable` (string) Variable name of other form items
- `value` (string) Variable value of other form items
```

## File: en/guides/monitoring/integrate-external-ops-tools/integrate-langfuse.md
```markdown
# Integrate Langfuse

### 1. What is Langfuse

Langfuse is an open-source LLM engineering platform that helps teams collaborate on debugging, analyzing, and iterating their applications.

{% hint style="info" %}
Introduction to Langfuse: [https://langfuse.com/](https://langfuse.com/)
{% endhint %}

***

### 2. How to Configure Langfuse

1. Register and log in to Langfuse on the [official website](https://langfuse.com/)
2. Create a project in Langfuse. After logging in, click **New** on the homepage to create your own project. The **project** will be used to associate with **applications** in Dify for data monitoring.

<figure><img src="../../../.gitbook/assets/image (249) (1).png" alt=""><figcaption><p>Create a project in Langfuse</p></figcaption></figure>

Edit a name for the project.

<figure><img src="../../../.gitbook/assets/image (251) (1).png" alt=""><figcaption><p>Create a project in Langfuse</p></figcaption></figure>

3. Create project API credentials. In the left sidebar of the project, click **Settings** to open the settings.

<figure><img src="../../../.gitbook/assets/image (253) (1).png" alt=""><figcaption><p>Create project API credentials</p></figcaption></figure>

In Settings, click **Create API Keys** to create project API credentials.

<figure><img src="../../../.gitbook/assets/image (252) (1).png" alt=""><figcaption><p>Create project API credentials</p></figcaption></figure>

Copy and save the **Secret Key**, **Public Key**, and **Host**.

<figure><img src="../../../.gitbook/assets/image (254) (1).png" alt=""><figcaption><p>Get API Key configuration</p></figcaption></figure>

4. Configure Langfuse in Dify. Open the application you need to monitor, open **Monitoring** in the side menu, and select **Tracing app performance** on the page.

<figure><img src="../../../.gitbook/assets/tracing-app-performance-langfuse.png" alt=""><figcaption><p>Configure Langfuse</p></figcaption></figure>

After clicking configure, paste the **Secret Key, Public Key, Host** created in Langfuse into the configuration and save.

<figure><img src="../../../.gitbook/assets/config-langfuse.png" alt=""><figcaption><p>Configure Langfuse</p></figcaption></figure>

Once successfully saved, you can view the status on the current page. If it shows as started, it is being monitored.

<figure><img src="../../../.gitbook/assets/monitored-by-langfuse.png" alt=""><figcaption><p>View configuration status</p></figcaption></figure>

***

### 3. Viewing Monitoring Data in Langfuse

After configuration, debugging or production data of the application in Dify can be viewed in Langfuse.

<figure><img src="../../../.gitbook/assets/debug-app-in-dify.png" alt=""><figcaption><p>Debugging Applications in Dify</p></figcaption></figure>

<figure><img src="../../../.gitbook/assets/view-traces-in-langfuse.png" alt=""><figcaption><p>Viewing application data in Langfuse</p></figcaption></figure>

***

### 4 List of monitoring data

#### Trace the information of Workflow and Chatflow

**Tracing workflow and chatflow**

| Workflow                                 | LangFuse Trace          |
| ---------------------------------------- | ----------------------- |
| workflow\_app\_log\_id/workflow\_run\_id | id                      |
| user\_session\_id                        | user\_id                |
| workflow\_{id}                           | name                    |
| start\_time                              | start\_time             |
| end\_time                                | end\_time               |
| inputs                                   | input                   |
| outputs                                  | output                  |
| Model token consumption                  | usage                   |
| metadata                                 | metadata                |
| error                                    | level                   |
| error                                    | status\_message         |
| \[workflow]                              | tags                    |
| \["message", conversation\_mode]         | session\_id             |
| conversion\_id                           | parent\_observation\_id |

**Workflow Trace Info**

* workflow\_id - Unique ID of Workflow
* conversation\_id - Conversation ID
* workflow\_run\_id - Workflow ID of this runtime
* tenant\_id - Tenant ID
* elapsed\_time - Elapsed time at this runtime
* status - Runtime status
* version - Workflow version
* total\_tokens - Total token used at this runtime
* file\_list - List of files processed
* triggered\_from - Source that triggered this runtime
* workflow\_run\_inputs - Input of this workflow
* workflow\_run\_outputs - Output of this workflow
* error - Error Message
* query - Queries used at runtime
* workflow\_app\_log\_id - Workflow Application Log ID
* message\_id - Relevant Message ID
* start\_time - Start time of this runtime
* end\_time - End time of this runtime
* workflow node executions - Workflow node runtime information
* Metadata
  * workflow\_id - Unique ID of Workflow
  * conversation\_id - Conversation ID
  * workflow\_run\_id - Workflow ID of this runtime
  * tenant\_id - Tenant ID
  * elapsed\_time - Elapsed time at this runtime
  * status - Operational state
  * version - Workflow version
  * total\_tokens - Total token used at this runtime
  * file\_list - List of files processed
  * triggered\_from - Source that triggered this runtime

#### Message Trace Info

**For trace llm conversation**

| Message                          | LangFuse Generation/Trace |
| -------------------------------- | ------------------------- |
| message\_id                      | id                        |
| user\_session\_id                | user\_id                  |
| message\_{id}                    | name                      |
| start\_time                      | start\_time               |
| end\_time                        | end\_time                 |
| inputs                           | input                     |
| outputs                          | output                    |
| Model token consumption          | usage                     |
| metadata                         | metadata                  |
| error                            | level                     |
| error                            | status\_message           |
| \["message", conversation\_mode] | tags                      |
| conversation\_id                 | session\_id               |
| conversion\_id                   | parent\_observation\_id   |

**Message Trace Info**

* message\_id - Message ID
* message\_data - Message data
* user\_session\_id - Session ID for user
* conversation\_model - Conversation model
* message\_tokens - Message tokens
* answer\_tokens - Answer Tokens
* total\_tokens - Total Tokens from Message and Answer
* error - Error Message
* inputs - Input data
* outputs - Output data
* file\_list - List of files processed
* start\_time - Start time
* end\_time - End time
* message\_file\_data - Message of relevant file data
* conversation\_mode - Conversation mode
* Metadata
  * conversation\_id - Conversation ID
  * ls\_provider - Model provider
  * ls\_model\_name - Model ID
  * status - Message status
  * from\_end\_user\_id - Sending user's ID
  * from\_account\_id - Sending account's ID
  * agent\_based - Whether agent based
  * workflow\_run\_id - Workflow ID of this runtime
  * from\_source - Message source
  * message\_id - Message ID

#### Moderation Trace Information

**Used to track conversation moderation**

| Moderation    | LangFuse Generation/Trace |
| ------------- | ------------------------- |
| user\_id      | user\_id                  |
| moderation    | name                      |
| start\_time   | start\_time               |
| end\_time     | end\_time                 |
| inputs        | input                     |
| outputs       | output                    |
| metadata      | metadata                  |
| \[moderation] | tags                      |
| message\_id   | parent\_observation\_id   |

**Message Trace Info**

* message\_id - Message ID
* user\_id - user ID
* workflow\_app\_log\_id workflow\_app\_log\_id
* inputs - Input data for review
* message\_data - Message Data
* flagged - Whether it is flagged for attention
* action - Specific actions to implement
* preset\_response - Preset response
* start\_time - Start time of review
* end\_time - End time of review
* Metadata
  * message\_id - Message ID
  * action - Specific actions to implement
  * preset\_response - Preset response

#### Suggested Question Trace Information

**Used to track suggested questions**

| Suggested Question     | LangFuse Generation/Trace |
| ---------------------- | ------------------------- |
| user\_id               | user\_id                  |
| suggested\_question    | name                      |
| start\_time            | start\_time               |
| end\_time              | end\_time                 |
| inputs                 | input                     |
| outputs                | output                    |
| metadata               | metadata                  |
| \[suggested\_question] | tags                      |
| message\_id            | parent\_observation\_id   |

**Message Trace Info**

* message\_id - Message ID
* message\_data - Message data
* inputs - Input data
* outputs - Output data
* start\_time - Start time
* end\_time - End time
* total\_tokens - Total tokens
* status - Message Status
* error - Error Message
* from\_account\_id - Sending account ID
* agent\_based - Whether agent based
* from\_source - Message source
* model\_provider - Model provider
* model\_id - Model ID
* suggested\_question - Suggested question
* level - Status level
* status\_message - Message status
* Metadata
  * message\_id - Message ID
  * ls\_provider - Model Provider
  * ls\_model\_name - Model ID
  * status - Message status
  * from\_end\_user\_id - Sending user's ID
  * from\_account\_id - Sending Account ID
  * workflow\_run\_id - Workflow ID of this runtime
  * from\_source - Message source

#### Dataset Retrieval Trace Information

**Used to track knowledge base retrieval**

| Dataset Retrieval     | LangFuse Generation/Trace |
| --------------------- | ------------------------- |
| user\_id              | user\_id                  |
| dataset\_retrieval    | name                      |
| start\_time           | start\_time               |
| end\_time             | end\_time                 |
| inputs                | input                     |
| outputs               | output                    |
| metadata              | metadata                  |
| \[dataset\_retrieval] | tags                      |
| message\_id           | parent\_observation\_id   |

**Dataset Retrieval Trace Info**

* message\_id - Message ID
* inputs - Input Message
* documents - Document data
* start\_time - Start time
* end\_time - End time
* message\_data - Message data
* Metadata
  * message\_id - Message ID
  * ls\_provider - Model Provider
  * ls\_model\_name - Model ID
  * status - Model status
  * from\_end\_user\_id - Sending user's ID
  * from\_account\_id - Sending account's ID
  * agent\_based - Whether agent based
  * workflow\_run\_id - Workflow ID of this runtime
  * from\_source - Message Source

#### Tool Trace Information

**Used to track tool invocation**

| Tool                  | LangFuse Generation/Trace |
| --------------------- | ------------------------- |
| user\_id              | user\_id                  |
| tool\_name            | name                      |
| start\_time           | start\_time               |
| end\_time             | end\_time                 |
| inputs                | input                     |
| outputs               | output                    |
| metadata              | metadata                  |
| \["tool", tool\_name] | tags                      |
| message\_id           | parent\_observation\_id   |

**Tool Trace Info**

* message\_id - Message ID
* tool\_name - Tool Name
* start\_time - Start time
* end\_time - End time
* tool\_inputs - Tool inputs
* tool\_outputs - Tool outputs
* message\_data - Message data
* error - Error Message，if exist
* inputs - Input of Message
* outputs - Output of Message
* tool\_config - Tool config
* time\_cost - Time cost
* tool\_parameters - Tool Parameters
* file\_url - URL of relevant files
* Metadata
  * message\_id - Message ID
  * tool\_name - Tool Name
  * tool\_inputs - Tool inputs
  * tool\_outputs - Tool outputs
  * tool\_config - Tool config
  * time\_cost - Time. cost
  * error - Error Message
  * tool\_parameters - Tool parameters
  * message\_file\_id - Message file ID
  * created\_by\_role - Created by role
  * created\_user\_id - Created user ID

#### Generate Name Trace

**Used to track conversation title generation**

| Generate Name     | LangFuse Generation/Trace |
| ----------------- | ------------------------- |
| user\_id          | user\_id                  |
| generate\_name    | name                      |
| start\_time       | start\_time               |
| end\_time         | end\_time                 |
| inputs            | input                     |
| outputs           | output                    |
| metadata          | metadata                  |
| \[generate\_name] | tags                      |

**Generate Name Trace Info**

* conversation\_id - Conversation ID
* inputs - Input data
* outputs - Generated session name
* start\_time - Start time
* end\_time - End time
* tenant\_id - Tenant ID
* Metadata
  * conversation\_id - Conversation ID
  * tenant\_id - Tenant ID
```

## File: en/guides/monitoring/integrate-external-ops-tools/integrate-langsmith.md
```markdown
# Integrate LangSmith

### What is LangSmith

LangSmith is a platform for building production-grade LLM applications. It is used for developing, collaborating, testing, deploying, and monitoring LLM applications.

{% hint style="info" %}
For more details, please refer to [LangSmith](https://www.langchain.com/langsmith).
{% endhint %}

***

### How to Configure LangSmith

#### 1. Register/Login to [LangSmith](https://www.langchain.com/langsmith)

#### 2. Create a Project

Create a project in LangSmith. After logging in, click **New Project** on the homepage to create your own project. The **project** will be used to associate with **applications** in Dify for data monitoring.

<figure><img src="../../../.gitbook/assets/image (3) (1) (1) (1).png" alt=""><figcaption><p>Create a project in LangSmith</p></figcaption></figure>

Once created, you can view all created projects in the Projects section.

<figure><img src="../../../.gitbook/assets/image (7) (1) (2).png" alt=""><figcaption><p>View created projects in LangSmith</p></figcaption></figure>

#### 3. Create Project Credentials

Find the project settings **Settings** in the left sidebar.

<figure><img src="../../../.gitbook/assets/image (8) (1) (2).png" alt=""><figcaption><p>Project settings</p></figcaption></figure>

Click **Create API Key** to create project credentials.

<figure><img src="../../../.gitbook/assets/image (3) (1) (1) (1) (2).png" alt=""><figcaption><p>Create a project API Key</p></figcaption></figure>

Select **Personal Access Token** for subsequent API authentication.

<figure><img src="../../../.gitbook/assets/image (5) (1) (1) (1).png" alt=""><figcaption><p>Create an API Key</p></figcaption></figure>

Copy and save the created API key.

<figure><img src="../../../.gitbook/assets/image (9) (2).png" alt=""><figcaption><p>Copy API Key</p></figcaption></figure>

#### 4. Integrating LangSmith with Dify

Configure LangSmith in the Dify application. Open the application you need to monitor, open **Monitoring** in the side menu, and select **Tracing app performance** on the page.

<figure><img src="../../../.gitbook/assets/tracing-app-performance.png" alt=""><figcaption><p>Tracing app performance</p></figcaption></figure>

After clicking configure, paste the **API Key** and **project name** created in LangSmith into the configuration and save.

<figure><img src="../../../.gitbook/assets/config-langsmith.png" alt=""><figcaption><p>Configure LangSmith</p></figcaption></figure>

{% hint style="info" %}
The configured project name needs to match the project set in LangSmith. If the project names do not match, LangSmith will automatically create a new project during data synchronization.
{% endhint %}

Once successfully saved, you can view the monitoring status on the current page.

<figure><img src="../../../.gitbook/assets/integrate-with-langsmith.png" alt=""><figcaption><p>View configuration status</p></figcaption></figure>

### Viewing Monitoring Data in LangSmith

Once configured, the debug or production data from applications within Dify can be monitored in LangSmith.

<figure><img src="../../../.gitbook/assets/debug-app-in-dify.png" alt=""><figcaption><p>Debugging Applications in Dify</p></figcaption></figure>

When you switch to LangSmith, you can view detailed operation logs of Dify applications in the dashboard.

<figure><img src="../../../.gitbook/assets/image (2) (1) (1) (1).png" alt=""><figcaption><p>Viewing application data in LangSmith</p></figcaption></figure>

Detailed LLM operation logs through LangSmith will help you optimize the performance of your Dify application.

<figure><img src="../../../.gitbook/assets/viewing-app-data-in-langsmith.png" alt=""><figcaption><p>Viewing application data in LangSmith</p></figcaption></figure>

### Monitoring Data List

#### **Workflow/Chatflow Trace Information**

**Used to track workflows and chatflows**

| Workflow                                 | LangSmith Chain              |
| ---------------------------------------- | ---------------------------- |
| workflow\_app\_log\_id/workflow\_run\_id | id                           |
| user\_session\_id                        | - placed in metadata         |
| workflow\_{id}                           | name                         |
| start\_time                              | start\_time                  |
| end\_time                                | end\_time                    |
| inputs                                   | inputs                       |
| outputs                                  | outputs                      |
| Model token consumption                  | usage\_metadata              |
| metadata                                 | extra                        |
| error                                    | error                        |
| \[workflow]                              | tags                         |
| "conversation\_id/none for workflow"     | conversation\_id in metadata |
| conversion\_id                           | parent\_run\_id              |

**Workflow Trace Info**

* workflow\_id - Unique identifier of the workflow
* conversation\_id - Conversation ID
* workflow\_run\_id - ID of the current run
* tenant\_id - Tenant ID
* elapsed\_time - Time taken for the current run
* status - Run status
* version - Workflow version
* total\_tokens - Total tokens used in the current run
* file\_list - List of processed files
* triggered\_from - Source that triggered the current run
* workflow\_run\_inputs - Input data for the current run
* workflow\_run\_outputs - Output data for the current run
* error - Errors encountered during the current run
* query - Query used during the run
* workflow\_app\_log\_id - Workflow application log ID
* message\_id - Associated message ID
* start\_time - Start time of the run
* end\_time - End time of the run
* workflow node executions - Information about workflow node executions
* Metadata
  * workflow\_id - Unique identifier of the workflow
  * conversation\_id - Conversation ID
  * workflow\_run\_id - ID of the current run
  * tenant\_id - Tenant ID
  * elapsed\_time - Time taken for the current run
  * status - Run status
  * version - Workflow version
  * total\_tokens - Total tokens used in the current run
  * file\_list - List of processed files
  * triggered\_from - Source that triggered the current run

#### **Message Trace Information**

**Used to track LLM-related conversations**

| Chat                             | LangSmith LLM                |
| -------------------------------- | ---------------------------- |
| message\_id                      | id                           |
| user\_session\_id                | - placed in metadata         |
| “message\_{id}"                  | name                         |
| start\_time                      | start\_time                  |
| end\_time                        | end\_time                    |
| inputs                           | inputs                       |
| outputs                          | outputs                      |
| Model token consumption          | usage\_metadata              |
| metadata                         | extra                        |
| error                            | error                        |
| \["message", conversation\_mode] | tags                         |
| conversation\_id                 | conversation\_id in metadata |
| conversion\_id                   | parent\_run\_id              |

**Message Trace Info**

* message\_id - Message ID
* message\_data - Message data
* user\_session\_id - User session ID
* conversation\_model - Conversation mode
* message\_tokens - Number of tokens in the message
* answer\_tokens - Number of tokens in the answer
* total\_tokens - Total number of tokens in the message and answer
* error - Error information
* inputs - Input data
* outputs - Output data
* file\_list - List of processed files
* start\_time - Start time
* end\_time - End time
* message\_file\_data - File data associated with the message
* conversation\_mode - Conversation mode
* Metadata
  * conversation\_id - Conversation ID
  * ls\_provider - Model provider
  * ls\_model\_name - Model ID
  * status - Message status
  * from\_end\_user\_id - ID of the sending user
  * from\_account\_id - ID of the sending account
  * agent\_based - Whether the message is agent-based
  * workflow\_run\_id - Workflow run ID
  * from\_source - Message source

#### **Moderation Trace Information**

**Used to track conversation moderation**

| Moderation    | LangSmith Tool       |
| ------------- | -------------------- |
| user\_id      | - placed in metadata |
| “moderation"  | name                 |
| start\_time   | start\_time          |
| end\_time     | end\_time            |
| inputs        | inputs               |
| outputs       | outputs              |
| metadata      | extra                |
| \[moderation] | tags                 |
| message\_id   | parent\_run\_id      |

**Moderation Trace Info**

* message\_id - Message ID
* user\_id: User ID
* workflow\_app\_log\_id - Workflow application log ID
* inputs - Moderation input data
* message\_data - Message data
* flagged - Whether the content is flagged for attention
* action - Specific actions taken
* preset\_response - Preset response
* start\_time - Moderation start time
* end\_time - Moderation end time
* Metadata
  * message\_id - Message ID
  * action - Specific actions taken
  * preset\_response - Preset response

#### **Suggested Question Trace Information**

**Used to track suggested questions**

| Suggested Question     | LangSmith LLM        |
| ---------------------- | -------------------- |
| user\_id               | - placed in metadata |
| suggested\_question    | name                 |
| start\_time            | start\_time          |
| end\_time              | end\_time            |
| inputs                 | inputs               |
| outputs                | outputs              |
| metadata               | extra                |
| \[suggested\_question] | tags                 |
| message\_id            | parent\_run\_id      |

**Message Trace Info**

* message\_id - Message ID
* message\_data - Message data
* inputs - Input content
* outputs - Output content
* start\_time - Start time
* end\_time - End time
* total\_tokens - Number of tokens
* status - Message status
* error - Error information
* from\_account\_id - ID of the sending account
* agent\_based - Whether the message is agent-based
* from\_source - Message source
* model\_provider - Model provider
* model\_id - Model ID
* suggested\_question - Suggested question
* level - Status level
* status\_message - Status message
* Metadata
  * message\_id - Message ID
  * ls\_provider - Model provider
  * ls\_model\_name - Model ID
  * status - Message status
  * from\_end\_user\_id - ID of the sending user
  * from\_account\_id - ID of the sending account
  * workflow\_run\_id - Workflow run ID
  * from\_source - Message source

#### **Dataset Retrieval Trace Information**

**Used to track knowledge base retrieval**

| Dataset Retrieval     | LangSmith Retriever  |
| --------------------- | -------------------- |
| user\_id              | - placed in metadata |
| dataset\_retrieval    | name                 |
| start\_time           | start\_time          |
| end\_time             | end\_time            |
| inputs                | inputs               |
| outputs               | outputs              |
| metadata              | extra                |
| \[dataset\_retrieval] | tags                 |
| message\_id           | parent\_run\_id      |

**Dataset Retrieval Trace Info**

* message\_id - Message ID
* inputs - Input content
* documents - Document data
* start\_time - Start time
* end\_time - End time
* message\_data - Message data
* Metadata
  * message\_id - Message ID
  * ls\_provider - Model provider
  * ls\_model\_name - Model ID
  * status - Message status
  * from\_end\_user\_id - ID of the sending user
  * from\_account\_id - ID of the sending account
  * agent\_based - Whether the message is agent-based
  * workflow\_run\_id - Workflow run ID
  * from\_source - Message source

#### **Tool Trace Information**

**Used to track tool invocation**

| Tool                  | LangSmith Tool       |
| --------------------- | -------------------- |
| user\_id              | - placed in metadata |
| tool\_name            | name                 |
| start\_time           | start\_time          |
| end\_time             | end\_time            |
| inputs                | inputs               |
| outputs               | outputs              |
| metadata              | extra                |
| \["tool", tool\_name] | tags                 |
| message\_id           | parent\_run\_id      |

#### **Tool Trace Info**

* message\_id - Message ID
* tool\_name - Tool name
* start\_time - Start time
* end\_time - End time
* tool\_inputs - Tool inputs
* tool\_outputs - Tool outputs
* message\_data - Message data
* error - Error information, if any
* inputs - Inputs for the message
* outputs - Outputs of the message
* tool\_config - Tool configuration
* time\_cost - Time cost
* tool\_parameters - Tool parameters
* file\_url - URL of the associated file
* Metadata
  * message\_id - Message ID
  * tool\_name - Tool name
  * tool\_inputs - Tool inputs
  * tool\_outputs - Tool outputs
  * tool\_config - Tool configuration
  * time\_cost - Time cost
  * error - Error information, if any
  * tool\_parameters - Tool parameters
  * message\_file\_id - Message file ID
  * created\_by\_role - Role of the creator
  * created\_user\_id - User ID of the creator

**Generate Name Trace Information**

**Used to track conversation title generation**

| Generate Name     | LangSmith Tool       |
| ----------------- | -------------------- |
| user\_id          | - placed in metadata |
| generate\_name    | name                 |
| start\_time       | start\_time          |
| end\_time         | end\_time            |
| inputs            | inputs               |
| outputs           | outputs              |
| metadata          | extra                |
| \[generate\_name] | tags                 |

**Generate Name Trace Info**

* conversation\_id - Conversation ID
* inputs - Input data
* outputs - Generated conversation name
* start\_time - Start time
* end\_time - End time
* tenant\_id - Tenant ID
* Metadata
  * conversation\_id - Conversation ID
  * tenant\_id - Tenant ID
```

## File: en/guides/monitoring/integrate-external-ops-tools/README.md
```markdown
# Integrate External Ops Tools

### Why LLMOps Tools Are Necessary

While LLMs (Large Language Models) possess exceptional reasoning and text generation capabilities, their internal workings are still not fully understood, presenting challenges for developing LLM-based applications. For instance:

* Evaluating output **quality**
* Assessing inference **costs**
* Measuring model response **latency**
* **Debugging complexity** introduced by chain calls, agents, and tools
* Understanding complex user intents

Tools like LangSmith and Langfuse, known as LLMOps tools, provide comprehensive tracking and deep evaluation capabilities for LLM applications, offering developers complete lifecycle support from prototyping to production and operations.

* #### Prototyping Phase

In the prototyping phase, LLM applications typically involve rapid experimentation with prompt testing, model selection, RAG (Retrieval-Augmented Generation) strategies, and other parameter combinations. Quickly understanding the model's execution performance is crucial in this stage. Integrating Langfuse allows tracking of every step of Dify application execution, providing clear visibility and debugging information, enabling developers to quickly pinpoint issues and reduce debugging time.

* **Testing Phase**

In the testing phase, data collection continues to improve and enhance performance. LangSmith can add runs as examples to the dataset, extending test coverage to real-world scenarios. This is a key advantage of having logging and evaluation/testing systems on the same platform.

* #### Production Phase

In the production environment, development teams need to carefully monitor key data points, add benchmark datasets, perform manual annotations, and deeply analyze operational results. Especially during large-scale application usage, operations and data teams must continuously monitor application costs and performance, optimizing both the model and application performance.

### Integrating Dify with Ops Tools

When using Dify Workflow to orchestrate LLM applications, it typically involves a series of nodes and logic with high complexity.

Integrating Dify with external Ops tools helps to break the "black box" issue often faced in application orchestration. Developers can simply configure the platform to track data and metrics throughout the application lifecycle, easily assessing the quality, performance, and cost of LLM applications created on Dify.
```

## File: en/guides/monitoring/analysis.md
```markdown
# Data Analysis

The **Overview -- Data Analysis** section displays metrics such as usage, active users, and LLM (Language Learning Model) invocation costs. This allows you to continuously improve the effectiveness, engagement, and cost-efficiency of your application operations. We will gradually provide more useful visualization capabilities, so please let us know what you need.

<figure><img src="../../.gitbook/assets/overview-data-analysis.png" alt=""><figcaption><p>Overview—Data Analysis</p></figcaption></figure>

***

**Total Messages**

Reflects the total number of daily interactions between users and AI. Each time the AI answers a user's question, it counts as one message. Prompt orchestration and debugging sessions are not included.

**Active Users**

The number of unique users who have had effective interactions with the AI, defined as having more than one question-and-answer exchange. Prompt orchestration and debugging sessions are not included.

**Average Session Interactions**

Reflects the number of continuous interactions per session user. For example, if a user has a 10-round Q\&A with the AI, it is counted as 10. This metric reflects user engagement. It is available only for conversational applications.

**Token Output Speed**

The number of tokens output per second, indirectly reflecting the model's generation rate and the application's usage frequency.

**User Satisfaction Rate**

The number of likes per 1,000 messages, reflecting the proportion of users who are very satisfied with the answers.

**Token Usage**

Reflects the daily token expenditure for language model requests by the application, useful for cost control.

**Total Conversations**

Daily AI conversation count; each new conversation session counts as one. A single conversation session may contain multiple message exchanges; messages related to prompt engineering and debugging are not included.
```

## File: en/guides/monitoring/README.md
```markdown
# Monitoring

You can monitor and track the performance of your application in a production environment within the **Overview** section. In the data analytics dashboard, you can analyze various metrics such as usage costs, latency, user feedback, and performance in the production environment. By continuously debugging and iterating, you can continually improve your application.

<figure><img src="../../.gitbook/assets/monitoring-app.png" alt=""><figcaption><p>概览</p></figcaption></figure>
```

## File: en/guides/tools/tool-configuration/alphavantage.md
```markdown
# AlphaVantage Stock Analysis Tool

> Tool developed by [@zhuhao](https://github.com/hwzhuhao).

AlphaVantage is a comprehensive online platform offering financial market data and APIs, enabling individual investors and developers to easily access stock quotes, technical indicators, and in-depth stock analysis. Dify has integrated the AlphaVantage tool, and the following are the steps to configure and use the AlphaVantage tool in Dify.

## 1. Obtaining an AlphaVantage API Key

To get started, acquire an API Key from [AlphaVantage](https://www.alphavantage.co/support/#api-key).

## 2. Configuring AlphaVantage in Dify

Navigate to the Dify dashboard and select `Tools > AlphaVantage > To Authorize` to input your API Key.

## 3. Implementing the Tool

- **In Chatflow / Workflow Applications**

Both Chatflow and Workflow applications support the integration of the `AlphaVantage` block. After adding a node, utilize [variables](https://docs.dify.ai/v/zh-hans/guides/workflow/variables) to reference the user's input query in the "Input Variables → Stock Code" field. In the "End" node, use variables to reference the output from the `AlphaVantage` block.

- **In Agent Applications**

Incorporate the `AlphaVantage` tool into your Agent application. Users can then input stock codes or general stock descriptions in the chat interface to trigger the tool and retrieve precise financial data.
```

## File: en/guides/tools/tool-configuration/bing.md
```markdown
# Bing

> Tool author @Dify.

The Bing search tool can help you obtain online search results when using LLM applications. Below are the steps to configure and use the Bing search tool in Dify.

## 1. Apply for Bing API Key

Please apply for an API Key on the [Azure platform](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api).

## 2. Fill in the configuration in Dify

In the Dify navigation page, click `Tools > Azure > Authorize` to fill in the API Key.

![](../../../.gitbook/assets/tools-bing.png)

## 3. Use the tool

You can use the Bing tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding `Bing` tool nodes.

* **Agent applications**

Add the `Bing` tool in the Agent application, then enter the online search command to call this tool.
```

## File: en/guides/tools/tool-configuration/comfyui.md
```markdown
# ComfyUI
[ComfyUI](https://www.comfy.org/): The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface. Now you can use it in dify, input the prompt or images, and get the generated image.

## 1. Ensure that the ComfyUI workflow is running normally.  
Please refer to its [official documentation](https://docs.comfy.org/get_started/gettingstarted) to ensure that ComfyUI can run normally and generate images.

## 2. Prompt setting
If you don't need dify to pass in the prompt, you can skip this step. If your prompt node is connected to the only `KSampler` node in ComfyUI, you can skip this step.  
Otherwise, use the string `{{positive_prompt}}` to replace the positive prompt content, and `{{negative_prompt}}` to replace the negative prompt content.
<figure><img src="/en/.gitbook/assets/guides/tools/comfyui_prompt.png" alt=""><figcaption></figcaption></figure>

## 3. Export the API file of the workflow. 
<figure><img src="/en/.gitbook/assets/guides/tools/comfyui.png" alt=""><figcaption></figcaption></figure>
As shown in the figure, select `Save(API Format)`, if there is no such selection, you need to enable `Dev Mode` in the settings.

## 4. Integrate ComfyUI in Dify  
Fill in the access address in `Tools > ComfyUI > Go to Authentication`, if you are using a docker deployment of Dify, this address is usually `http://host.docker.internal:8188`.

## 5. Use ComfyUI in Dify
Open its `Workflow` tool, fill in the content in the API file you just exported in `WORKFLOW JSON`, and you can generate normally.

## 6. Image input
Some ComfyUI workflows require multiple images inputs. In dify, it will find every `LoadImage` node in the `WORKFLOW JSON` and fill in the image files input by the user in order. When you want to change this order, you can adjust it by filling in the `Image node ID list`. For example, if your workflow needs to input images into the 35th, 69th, and 87th nodes, then input `69,35,87` will pass the first image to the 69th node.
```

## File: en/guides/tools/tool-configuration/dall-e.md
```markdown
# Dall-e

> Tool author @Dify.

DALL-E is an AI image generator developed by OpenAI that generates images based on text prompts. Dify has integrated the DALL-E tool, and the following are the steps to configure and use the DALL-E tool in Dify.

## 1. Apply for OpenAI's API Key

Please apply for an API Key at [OpenAI Platform](https://platform.openai.com/), and ensure that your account has sufficient Credits.

## 2. Fill in the configuration in Dify

In the Dify navigation page, click `Tools > DALL-E > Authorize` and fill in the API Key.

![](../../../.gitbook/assets/tools-dalle.png)

## 3. Use the tool

* **Chatflow / Workflow Applications**

Both Chatflow and Workflow applications support the `DALL-E` tool node. After adding it, you need to fill in the "Input Variables → Prompt" in the node with [variables](https://docs.dify.ai/guides/workflow/variables) to reference the user's input prompt or the content generated by the previous node. Finally, use the variable to reference the image output by `DALL-E` in the "End" node.

* **Agent Applications**

Add the `DALL-E` tool in the Agent application, then send a picture description in the dialog box to call the tool to generate an AI image.
```

## File: en/guides/tools/tool-configuration/google.md
```markdown
# Google

> Tool author @Dify.

The Google Search tool can help you obtain online search results when using LLM applications. Here are the steps to configure and use the Google Search tool in Dify.

## 1. Apply for Serp API Key

Please apply for an API Key on the [Serp](https://serpapi.com/dashboard).

## 2. Fill in the configuration in Dify

In the Dify navigation page, click `Tools > Google > Go to authorize` to fill in the API Key.

![](../../../.gitbook/assets/tools-google.png)

## 3. Using the tool

You can use the Google tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding a `Google` tool node.

* **Agent applications**

Add the `Google` tool in the Agent application, then enter online search instructions to call this tool.
```

## File: en/guides/tools/tool-configuration/perplexity.md
```markdown
# Perplexity Search

> Tool author @Dify.

Perplexity is an AI-based search engine that can understand complex queries and provide accurate, relevant real-time answers. Here are the steps to configure and use the Perplexity Search tool in Dify.

## 1. Apply for Perplexity API Key

Please apply for an API Key at [Perplexity](https://www.perplexity.ai/settings/api), and ensure that your account has sufficient Credits.

## 2. Fill in the configuration in Dify

In the Dify navigation page, click on `Tools > Perplexity > Go to authorize` to fill in the API Key.

![](../../../.gitbook/assets/tools-perplexity.png)

## 3. Using the tool

You can use the Perplexity Search tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding Perplexity tool nodes. Pass the user's input content through variables to the "Query" box in the Perplexity tool node, adjust the built-in parameters of the Perplexity tool as needed, and finally select the output content of the Perplexity tool node in the response box of the "End" node.

![](../../../.gitbook/assets/tools-chatflow-perplexity.png)

* **Agent applications**

Add the `Perplexity Search` tool in the Agent application, then enter relevant commands to invoke this tool.

![](../../../.gitbook/assets/tools-agent-perplexity.png)
```

## File: en/guides/tools/tool-configuration/README.md
```markdown
---
description: Learn about the Different Tools Supported by Dify.
---

# Tool Configuration

Dify supports various tools to enhance your application's capabilities. Each tool has unique features and parameters, so select a tool that suits your application's needs. **Obtain the API key from the tool provider's official website before using it in Dify.**

## Tools Integration Guides

* [StableDiffusion](stable-diffusion.md): A tool for generating images based on text prompts.
* [SearXNG](searxng.md): A free internet metasearch engine which aggregates results from various search services and databases.
```

## File: en/guides/tools/tool-configuration/searchapi.md
```markdown
# SearchApi

> Tool author @SearchApi.

SearchApi is a powerful real-time SERP API that provides structured data from a collection of search engines including Google Search, Google Jobs, YouTube, Google News, and more. Here are the steps to configure and use the SearchApi search tool in Dify.

## 1. Apply for Search API Key

Please apply for an API Key at [SearchApi](https://www.searchapi.io/).

## 2. Fill in the configuration in Dify

In the Dify navigation page, click on `Tools > SearchApi > Go to authorize` to fill in the API Key.

![](../../../.gitbook/assets/tool-searchapi.png)

## 3. Using the tool

You can use the SearchApi tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding `SearchApi` series tool nodes, providing four tools: Google Jobs API, Google News API, Google Search API, and YouTube Scraper API.

![](../../../.gitbook/assets/tool-searchapi-flow.png)

* **Agent applications**

Select the `SearchApi` tool you need to add in the Agent application, then enter commands to call the tool.
```

## File: en/guides/tools/tool-configuration/searxng.md
```markdown
# SearXNG
SearXNG is a free internet metasearch engine which aggregates results from various search services and databases. Users are neither tracked nor profiled. Now you can use this tool directly in Dify.

Below are the steps for integrating SearXNG into Dify using Docker in the [Community Edition](https://docs.dify.ai/getting-started/install-self-hosted/docker-compose).

> If you want to use SearXNG within the Dify cloud service, please refer to the [SearXNG installation documentation](https://docs.searxng.org/admin/installation.html) to set up your own service, then return to Dify and fill in the service's Base URL in the "Tools > SearXNG > Authenticate" page.

## 1. Modify Dify Configuration File

The configuration file is located at `dify/api/core/tools/provider/builtin/searxng/docker/settings.yml`, and you can refer to the config documentation [here](https://docs.searxng.org/admin/settings/index.html).

## 2. Start the Service

Start the Docker container in the dify root directory.

```bash
cd dify
docker run --rm -d -p 8081:8080 -v "${PWD}/api/core/tools/provider/builtin/searxng/docker:/etc/searxng" searxng/searxng
```

## 3. Use SearXNG

Fill in the access address in "Tools > SearXNG > Authenticate" to establish a connection between the Dify service and the SearXNG service. The Docker internal address for SearXNG is usually `http://host.docker.internal:8081`.
```

## File: en/guides/tools/tool-configuration/serper.md
```markdown
# Serper

> Tool author @zhuhao.

Serper is a powerful real-time search engine tool API that provides structured data from Google search engine results. Here are the steps to configure and use the Serper search tool in Dify.

## 1. Apply for Serper API Key

Please apply for an API Key on the [Serper platform](https://serper.dev/signup).

## 2. Fill in the configuration in Dify

In the Dify navigation page, click `Tools > Serper > Authorize` to fill in the API Key.

![](../../../.gitbook/assets/tool-serper.png)

## 3. Using the tool

You can use the Serper tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding `Serper` tool nodes.

* **Agent applications**

In Agent applications, select the `Serper` tool you want to add, then enter commands to call the tool.
```

## File: en/guides/tools/tool-configuration/siliconflow.md
```markdown
# SiliconFlow (Flux AI Supported)

> Tool author @hjlarry.

SiliconFlow provides high-quality GenAI services based on excellent open-source foundation models. You can use SiliconFlow in Dify to call image generation models like Flux and Stable Diffusion, and build your own AI image generation application.

## 1. Apply for SiliconCloud API Key

Create a new API Key on the [SiliconCloud API management page](https://cloud.siliconflow.cn/account/ak) and ensure that you have sufficient balance.

## 2. Fill in the Configuration in Dify

In the Dify tool page, click on `SiliconCloud > To Authorize` and fill in the API Key.

<figure><img src="../../../.gitbook/assets/截屏2024-09-27 13.04.16.png" alt=""><figcaption></figcaption></figure>

## 3. Using the Tool

* **Chatflow/Workflow Application**

Chatflow and Workflow applications both support adding `SiliconFlow` tool nodes. You can pass user input content to the SiliconFlow tool node's "prompt" and "negative prompt" boxes through [variables](https://docs.dify.ai/v/zh-hans/guides/workflow/variables), adjust the built-in parameters as needed, and finally select the output content (text, images, etc.) of the SiliconFlow tool node in the "end" node's reply box.

<figure><img src="../../../.gitbook/assets/截屏2024-09-27 13.17.40.png" alt=""><figcaption></figcaption></figure>

* **Agent Application**

In the Agent application, add the `Stable Diffusion` or `Flux` tool, and then send a picture description in the conversation box to call the tool to generate images.

<figure><img src="../../../.gitbook/assets/截屏2024-09-27 13.14.16.png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/截屏2024-09-27 13.13.06.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/tools/tool-configuration/stable-diffusion.md
```markdown
# Stable Diffusion

Stable Diffusion is a tool for generating images based on text prompts, Dify has implemented the interface to access the Stable Diffusion WebUI API, so you can use it directly in Dify. followings are steps to integrate Stable Diffusion in Dify.

## 1. Make sure you have a machine with a GPU
Stable Diffusion requires a machine with a GPU to generate images. but it's not necessary, you can just use CPU to generate images, but it will be slow.

## 2. Launch Stable Diffusion WebUI
Launch the Stable Diffusion WebUI on your local machine or server.

### 2.1. Clone the Stable Diffusion WebUI repository
Clone the Stable Diffusion WebUI repository from the [official repository](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
    
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

### 2.2. Launch it locally
After cloning the repository, you should change directory to the cloned repository and run the following command to launch the Stable Diffusion WebUI.

#### Windows
```bash
cd stable-diffusion-webui
./webui.bat --api --listen
```

#### Linux
```bash
cd stable-diffusion-webui
./webui.sh --api --listen
```

### 2.3. Prepare Models
Now you can access the Stable Diffusion WebUI on your browser according to the address shown in the terminal, but the models are not available yet. You need to download the models HuggingFace or other sources and put them in the `models` directory of the Stable Diffusion WebUI.

For example, we use [pastel-mix](https://huggingface.co/JamesFlare/pastel-mix) as the model, use `git lfs` to download the model and put it in the `models` directory in `stable-diffusion-webui`.

```bash
git clone https://huggingface.co/JamesFlare/pastel-mix
```

### 2.4 Get Model Name
Now you can see `pastel-mix` in the model list, but we still need to get the model name, visit `http://your_id:port/sdapi/v1/sd-models`, you will see the model name like below.

```json
[
    {
        "title": "pastel-mix/pastelmix-better-vae-fp32.ckpt [943a810f75]",
        "model_name": "pastel-mix_pastelmix-better-vae-fp32",
        "hash": "943a810f75",
        "sha256": "943a810f7538b32f9d81dc5adea3792c07219964c8a8734565931fcec90d762d",
        "filename": "/home/takatost/stable-diffusion-webui/models/Stable-diffusion/pastel-mix/pastelmix-better-vae-fp32.ckpt",
        "config": null
    },
]
```

The `model_name` is what we need, in this case, it's `pastel-mix_pastelmix-better-vae-fp32`.

## 3. Integrate Stable Diffusion in Dify
Fill in the Authentication and Model Configuration in `Tools > StableDiffusion > To Authorize` with the information you get from the previous steps.

## 4. Finish

- **Chatflow / Workflow Applications**

Both Chatflow and Workflow applications support adding `Stable Diffusion` tool nodes. After adding, you need to fill in the [variable](https://docs.dify.ai/v/zh-hans/guides/workflow/variables) referencing the user's input prompt or the content generated by the previous node in the "Input Variables → Prompt" section within the node. Finally, use a variable to reference the image output by `Stable Diffusion` in the "End" node.

- **Agent Applications**

Add the `Stable Diffusion` tool in the Agent application, then send image descriptions in the chat box to invoke the tool and generate AI images.
```

## File: en/guides/tools/tool-configuration/youtube.md
```markdown
# Youtube

> Tool author @Dify.

[Youtube](https://www.youtube.com/) is the biggest online video sharing platform. Currently Dify.ai has two relevant tools `Video Statisctics` and `Free YouTube Transcript API` to analysis the videos' information by entering the url or keyword.  

## 1. Ensure that you allow to use the Google Cloud Service

> If you don't have an account, go to the [Google credential site](https://console.cloud.google.com/apis/credentials) and follow their instruction to create an account.

If you have the account, go to the APIs & Services page and click `Create credentials -> API key` to create an API key.

![](/img/en-google-api.jpg)

Follow the step and click `Enabled APIs and services -> YouTube Data API v3` to enable the Youtube Data API.

## 2. Setup the Youtube API in Dify Tool page

Back to the [Dify Tools page](https://cloud.dify.ai/tools) and open the Youtube API card, fill in the API from Step 1 to get the authorization.

![](/img/en-set-youtube-api.jpeg)

## 3. Using the tool

You can use the Youtube tool in the following application types.

* **Chatflow / Workflow applications**

Both Chatflow and Workflow applications support adding `Video statistics` nodes. 

![](../../../../img/en-youtube-workflow.jpg)

* **Agent applications**

Add the `Free YouTube Transcript API` tool in the Agent application, then enter relevant commands to invoke this tool.

![](../../../../img/en-youtube-agent.png)
```

## File: en/guides/tools/advanced-tool-integration.md
```markdown
# Advanced Tool Integration

Before starting with this advanced guide, please make sure you have a basic understanding of the tool integration process in Dify. Check out [Quick Integration](https://docs.dify.ai/tutorials/quick-tool-integration) for a quick run through.

### Tool Interface

We have defined a series of helper methods in the `Tool` class to help developers quickly build more complex tools.

#### Message Return

Dify supports various message types such as `text`, `link`, `image`, and `file BLOB`. You can return different types of messages to the LLM and users through the following interfaces.

Please note, some parameters in the following interfaces will be introduced in later sections.

**Image URL**

You only need to pass the URL of the image, and Dify will automatically download the image and return it to the user.

```python
    def create_image_message(self, image: str, save_as: str = '') -> ToolInvokeMessage:
        """
            create an image message

            :param image: the url of the image
            :return: the image message
        """
```

**Link**

If you need to return a link, you can use the following interface.

```python
    def create_link_message(self, link: str, save_as: str = '') -> ToolInvokeMessage:
        """
            create a link message

            :param link: the url of the link
            :return: the link message
        """
```

**Text**

If you need to return a text message, you can use the following interface.

```python
    def create_text_message(self, text: str, save_as: str = '') -> ToolInvokeMessage:
        """
            create a text message

            :param text: the text of the message
            :return: the text message
        """
```

**File BLOB**

If you need to return the raw data of a file, such as images, audio, video, PPT, Word, Excel, etc., you can use the following interface.

* `blob` The raw data of the file, of bytes type
* `meta` The metadata of the file, if you know the type of the file, it is best to pass a `mime_type`, otherwise Dify will use `octet/stream` as the default type

```python
    def create_blob_message(self, blob: bytes, meta: dict = None, save_as: str = '') -> ToolInvokeMessage:
        """
            create a blob message

            :param blob: the blob
            :return: the blob message
        """
```

#### Shortcut Tools

In large model applications, we have two common needs:

* First, summarize a long text in advance, and then pass the summary content to the LLM to prevent the original text from being too long for the LLM to handle
* The content obtained by the tool is a link, and the web page information needs to be crawled before it can be returned to the LLM

To help developers quickly implement these two needs, we provide the following two shortcut tools.

**Text Summary Tool**

This tool takes in an user\_id and the text to be summarized, and returns the summarized text. Dify will use the default model of the current workspace to summarize the long text.

```python
    def summary(self, user_id: str, content: str) -> str:
        """
            summary the content

            :param user_id: the user id
            :param content: the content
            :return: the summary
        """
```

**Web Page Crawling Tool**

This tool takes in web page link to be crawled and a user\_agent (which can be empty), and returns a string containing the information of the web page. The `user_agent` is an optional parameter that can be used to identify the tool. If not passed, Dify will use the default `user_agent`.

```python
    def get_url(self, url: str, user_agent: str = None) -> str:
        """
            get url
        """ the crawled result
```

#### Variable Pool

We have introduced a variable pool in `Tool` to store variables, files, etc. generated during the tool's operation. These variables can be used by other tools during the tool's operation.

Next, we will use `DallE3` and `Vectorizer.AI` as examples to introduce how to use the variable pool.

* `DallE3` is an image generation tool that can generate images based on text. Here, we will let `DallE3` generate a logo for a coffee shop
* `Vectorizer.AI` is a vector image conversion tool that can convert images into vector images, so that the images can be infinitely enlarged without distortion. Here, we will convert the PNG icon generated by `DallE3` into a vector image, so that it can be truly used by designers.

**DallE3**

First, we use DallE3. After creating the image, we save the image to the variable pool. The code is as follows:

```python
from typing import Any, Dict, List, Union
from core.tools.entities.tool_entities import ToolInvokeMessage
from core.tools.tool.builtin_tool import BuiltinTool

from base64 import b64decode

from openai import OpenAI

class DallE3Tool(BuiltinTool):
    def _invoke(self, 
                user_id: str, 
               tool_Parameters: Dict[str, Any], 
        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:
        """
            invoke tools
        """
        client = OpenAI(
            api_key=self.runtime.credentials['openai_api_key'],
        )

        # prompt
        prompt = tool_Parameters.get('prompt', '')
        if not prompt:
            return self.create_text_message('Please input prompt')

        # call openapi dalle3
        response = client.images.generate(
            prompt=prompt, model='dall-e-3',
            size='1024x1024', n=1, style='vivid', quality='standard',
            response_format='b64_json'
        )

        result = []
        for image in response.data:
            # Save all images to the variable pool through the save_as parameter. The variable name is self.VARIABLE_KEY.IMAGE.value. If new images are generated later, they will overwrite the previous images.
            result.append(self.create_blob_message(blob=b64decode(image.b64_json), 
                                                   meta={ 'mime_type': 'image/png' },
                                                    save_as=self.VARIABLE_KEY.IMAGE.value))

        return result
```

Note that we used `self.VARIABLE_KEY.IMAGE.value` as the variable name of the image. In order for developers' tools to cooperate with each other, we defined this `KEY`. You can use it freely, or you can choose not to use this `KEY`. Passing a custom KEY is also acceptable.

**Vectorizer.AI**

Next, we use Vectorizer.AI to convert the PNG icon generated by DallE3 into a vector image. Let's go through the functions we defined here. The code is as follows:

```python
from core.tools.tool.builtin_tool import BuiltinTool
from core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter
from core.tools.errors import ToolProviderCredentialValidationError

from typing import Any, Dict, List, Union
from httpx import post
from base64 import b64decode

class VectorizerTool(BuiltinTool):
    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \
        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:
        """
        Tool invocation, the image variable name needs to be passed in from here, so that we can get the image from the variable pool
        """
        
    
    def get_runtime_parameters(self) -> List[ToolParameter]:
        """
        Override the tool parameter list, we can dynamically generate the parameter list based on the actual situation in the current variable pool, so that the LLM can generate the form based on the parameter list
        """
        
    
    def is_tool_available(self) -> bool:
        """
        Whether the current tool is available, if there is no image in the current variable pool, then we don't need to display this tool, just return False here
        """     
```

Next, let's implement these three functions

```python
from core.tools.tool.builtin_tool import BuiltinTool
from core.tools.entities.tool_entities import ToolInvokeMessage, ToolParameter
from core.tools.errors import ToolProviderCredentialValidationError

from typing import Any, Dict, List, Union
from httpx import post
from base64 import b64decode

class VectorizerTool(BuiltinTool):
    def _invoke(self, user_id: str, tool_Parameters: Dict[str, Any]) \
        -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:
        """
            invoke tools
        """
        api_key_name = self.runtime.credentials.get('api_key_name', None)
        api_key_value = self.runtime.credentials.get('api_key_value', None)

        if not api_key_name or not api_key_value:
            raise ToolProviderCredentialValidationError('Please input api key name and value')

        # Get image_id, the definition of image_id can be found in get_runtime_parameters
        image_id = tool_Parameters.get('image_id', '')
        if not image_id:
            return self.create_text_message('Please input image id')

        # Get the image generated by DallE from the variable pool
        image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)
        if not image_binary:
            return self.create_text_message('Image not found, please request user to generate image firstly.')

        # Generate vector image
        response = post(
            'https://vectorizer.ai/api/v1/vectorize',
            files={ 'image': image_binary },
            data={ 'mode': 'test' },
            auth=(api_key_name, api_key_value), 
            timeout=30
        )

        if response.status_code != 200:
            raise Exception(response.text)
        
        return [
            self.create_text_message('the vectorized svg is saved as an image.'),
            self.create_blob_message(blob=response.content,
                                    meta={'mime_type': 'image/svg+xml'})
        ]
    
    def get_runtime_parameters(self) -> List[ToolParameter]:
        """
        override the runtime parameters
        """
        # Here, we override the tool parameter list, define the image_id, and set its option list to all images in the current variable pool. The configuration here is consistent with the configuration in yaml.
        return [
            ToolParameter.get_simple_instance(
                name='image_id',
                llm_description=f'the image id that you want to vectorize, \
                    and the image id should be specified in \
                        {[i.name for i in self.list_default_image_variables()]}',
                type=ToolParameter.ToolParameterType.SELECT,
                required=True,
                options=[i.name for i in self.list_default_image_variables()]
            )
        ]
    
    def is_tool_available(self) -> bool:
        # Only when there are images in the variable pool, the LLM needs to use this tool
        return len(self.list_default_image_variables()) > 0
```

It's worth noting that we didn't actually use `image_id` here. We assumed that there must be an image in the default variable pool when calling this tool, so we directly used `image_binary = self.get_variable_file(self.VARIABLE_KEY.IMAGE)` to get the image. In cases where the model's capabilities are weak, we recommend developers to do the same, which can effectively improve fault tolerance and avoid the model passing incorrect parameters.
```

## File: en/guides/tools/quick-tool-integration.md
```markdown
# Quick Tool Integration

Here, we will use GoogleSearch as an example to demonstrate how to quickly integrate a tool.

### 1. Prepare the Tool Provider yaml

#### Introduction

This yaml declares a new tool provider, and includes information like the provider's name, icon, author, and other details that are fetched by the frontend for display.

#### Example

We need to create a `google` module (folder) under `core/tools/provider/builtin`, and create `google.yaml`. The name must be consistent with the module name.

Subsequently, all operations related to this tool will be carried out under this module.

```yaml
identity: # Basic information of the tool provider
  author: Dify # Author
  name: google # Name, unique, no duplication with other providers
  label: # Label for frontend display
    en_US: Google # English label
    zh_Hans: Google # Chinese label
  description: # Description for frontend display
    en_US: Google # English description
    zh_Hans: Google # Chinese description
  icon: icon.svg # Icon, needs to be placed in the _assets folder of the current module

```

* The `identity` field is mandatory, it contains the basic information of the tool provider, including author, name, label, description, icon, etc.
  *   The icon needs to be placed in the `_assets` folder of the current module, you can refer to: api/core/tools/provider/builtin/google/\_assets/icon.svg

      ```xml
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="25" viewBox="0 0 24 25" fill="none">
        <path d="M22.501 12.7332C22.501 11.8699 22.4296 11.2399 22.2748 10.5865H12.2153V14.4832H18.12C18.001 15.4515 17.3582 16.9099 15.9296 17.8898L15.9096 18.0203L19.0902 20.435L19.3106 20.4565C21.3343 18.6249 22.501 15.9298 22.501 12.7332Z" fill="#4285F4"/>
        <path d="M12.214 23C15.1068 23 17.5353 22.0666 19.3092 20.4567L15.9282 17.8899C15.0235 18.5083 13.8092 18.9399 12.214 18.9399C9.38069 18.9399 6.97596 17.1083 6.11874 14.5766L5.99309 14.5871L2.68583 17.0954L2.64258 17.2132C4.40446 20.6433 8.0235 23 12.214 23Z" fill="#34A853"/>
        <path d="M6.12046 14.5766C5.89428 13.9233 5.76337 13.2233 5.76337 12.5C5.76337 11.7766 5.89428 11.0766 6.10856 10.4233L6.10257 10.2841L2.75386 7.7355L2.64429 7.78658C1.91814 9.20993 1.50146 10.8083 1.50146 12.5C1.50146 14.1916 1.91814 15.7899 2.64429 17.2132L6.12046 14.5766Z" fill="#FBBC05"/>
        <path d="M12.2141 6.05997C14.2259 6.05997 15.583 6.91163 16.3569 7.62335L19.3807 4.73C17.5236 3.03834 15.1069 2 12.2141 2C8.02353 2 4.40447 4.35665 2.64258 7.78662L6.10686 10.4233C6.97598 7.89166 9.38073 6.05997 12.2141 6.05997Z" fill="#EB4335"/>
      </svg>
      ```

### 2. Prepare Provider Credentials

Google, as a third-party tool, uses the API provided by SerpApi, which requires an API Key to use. This means that this tool needs a credential to use. For tools like `wikipedia`, there is no need to fill in the credential field, you can refer to: api/core/tools/provider/builtin/wikipedia/wikipedia.yaml

```yaml
identity:
  author: Dify
  name: wikipedia
  label:
    en_US: Wikipedia
    zh_Hans: 维基百科
    pt_BR: Wikipedia
  description:
    en_US: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.
    zh_Hans: 维基百科是一个由全世界的志愿者创建和编辑的免费在线百科全书。
    pt_BR: Wikipedia is a free online encyclopedia, created and edited by volunteers around the world.
  icon: icon.svg
credentials_for_provider:
```

After configuring the credential field, the effect is as follows:

```yaml
identity:
  author: Dify
  name: google
  label:
    en_US: Google
    zh_Hans: Google
  description:
    en_US: Google
    zh_Hans: Google
  icon: icon.svg
credentials_for_provider: # Credential field
  serpapi_api_key: # Credential field name
    type: secret-input # Credential field type
    required: true # Required or not
    label: # Credential field label
      en_US: SerpApi API key # English label
      zh_Hans: SerpApi API key # Chinese label
    placeholder: # Credential field placeholder
      en_US: Please input your SerpApi API key # English placeholder
      zh_Hans: 请输入你的 SerpApi API key # Chinese placeholder
    help: # Credential field help text
      en_US: Get your SerpApi API key from SerpApi # English help text
      zh_Hans: 从 SerpApi 获取你的 SerpApi API key # Chinese help text
    url: https://serpapi.com/manage-api-key # Credential field help link

```

* `type`: Credential field type, currently can be either `secret-input`, `text-input`, or `select` , corresponding to password input box, text input box, and drop-down box, respectively. If set to `secret-input`, it will mask the input content on the frontend, and the backend will encrypt the input content.

### 3. Prepare Tool yaml

A provider can have multiple tools, each tool needs a yaml file to describe, this file contains the basic information, parameters, output, etc. of the tool.

Still taking GoogleSearch as an example, we need to create a `tools` module under the `google` module, and create `tools/google_search.yaml`, the content is as follows.

```yaml
identity: # Basic information of the tool
  name: google_search # Tool name, unique, no duplication with other tools
  author: Dify # Author
  label: # Label for frontend display
    en_US: GoogleSearch # English label
    zh_Hans: 谷歌搜索 # Chinese label
description: # Description for frontend display
  human: # Introduction for frontend display, supports multiple languages
    en_US: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query.
    zh_Hans: 一个用于执行 Google SERP 搜索并提取片段和网页的工具。输入应该是一个搜索查询。
  llm: A tool for performing a Google SERP search and extracting snippets and webpages.Input should be a search query. # Introduction passed to LLM, in order to make LLM better understand this tool, we suggest to write as detailed information about this tool as possible here, so that LLM can understand and use this tool
parameters: # Parameter list
  - name: query # Parameter name
    type: string # Parameter type
    required: true # Required or not
    label: # Parameter label
      en_US: Query string # English label
      zh_Hans: 查询语句 # Chinese label
    human_description: # Introduction for frontend display, supports multiple languages
      en_US: used for searching
      zh_Hans: 用于搜索网页内容
    llm_description: key words for searching # Introduction passed to LLM, similarly, in order to make LLM better understand this parameter, we suggest to write as detailed information about this parameter as possible here, so that LLM can understand this parameter
    form: llm # Form type, llm means this parameter needs to be inferred by Agent, the frontend will not display this parameter
  - name: result_type
    type: select # Parameter type
    required: true
    options: # Drop-down box options
      - value: text
        label:
          en_US: text
          zh_Hans: 文本
      - value: link
        label:
          en_US: link
          zh_Hans: 链接
    default: link
    label:
      en_US: Result type
      zh_Hans: 结果类型
    human_description:
      en_US: used for selecting the result type, text or link
      zh_Hans: 用于选择结果类型，使用文本还是链接进行展示
    form: form # Form type, form means this parameter needs to be filled in by the user on the frontend before the conversation starts

```

* The `identity` field is mandatory, it contains the basic information of the tool, including name, author, label, description, etc.
* `parameters` Parameter list
  * `name` Parameter name, unique, no duplication with other parameters
  * `type` Parameter type, currently supports `string`, `number`, `boolean`, `select` four types, corresponding to string, number, boolean, drop-down box
  * `required` Required or not
    * In `llm` mode, if the parameter is required, the Agent is required to infer this parameter
    * In `form` mode, if the parameter is required, the user is required to fill in this parameter on the frontend before the conversation starts
  * `options` Parameter options
    * In `llm` mode, Dify will pass all options to LLM, LLM can infer based on these options
    * In `form` mode, when `type` is `select`, the frontend will display these options
  * `default` Default value
  * `label` Parameter label, for frontend display
  * `human_description` Introduction for frontend display, supports multiple languages
  * `llm_description` Introduction passed to LLM, in order to make LLM better understand this parameter, we suggest to write as detailed information about this parameter as possible here, so that LLM can understand this parameter
  * `form` Form type, currently supports `llm`, `form` two types, corresponding to Agent self-inference and frontend filling

### 4. Add Tool Logic

After completing the tool configuration, we can start writing the tool code that defines how it is invoked.

Create `google_search.py` under the `google/tools` module, the content is as follows.

```python
from core.tools.tool.builtin_tool import BuiltinTool
from core.tools.entities.tool_entities import ToolInvokeMessage

from typing import Any, Dict, List, Union

class GoogleSearchTool(BuiltinTool):
    def _invoke(self, 
                user_id: str,
               tool_parameters: Dict[str, Any], 
        ) -> Union[ToolInvokeMessage, List[ToolInvokeMessage]]:
        """
            invoke tools
        """
        query = tool_parameters['query']
        result_type = tool_parameters['result_type']
        api_key = self.runtime.credentials['serpapi_api_key']
        # TODO: search with serpapi
        result = SerpAPI(api_key).run(query, result_type=result_type)

        if result_type == 'text':
            return self.create_text_message(text=result)
        return self.create_link_message(link=result)
```

#### Parameters

The overall logic of the tool is in the `_invoke` method, this method accepts two parameters: `user_id` and `tool_parameters`, which represent the user ID and tool parameters respectively

#### Return Data

When the tool returns, you can choose to return one message or multiple messages, here we return one message, using `create_text_message` and `create_link_message` can create a text message or a link message.

### 5. Add Provider Code

Finally, we need to create a provider class under the provider module to implement the provider's credential verification logic. If the credential verification fails, it will throw a `ToolProviderCredentialValidationError` exception.

Create `google.py` under the `google` module, the content is as follows.

```python
from core.tools.entities.tool_entities import ToolInvokeMessage, ToolProviderType
from core.tools.tool.tool import Tool
from core.tools.provider.builtin_tool_provider import BuiltinToolProviderController
from core.tools.errors import ToolProviderCredentialValidationError

from core.tools.provider.builtin.google.tools.google_search import GoogleSearchTool

from typing import Any, Dict

class GoogleProvider(BuiltinToolProviderController):
    def _validate_credentials(self, credentials: Dict[str, Any]) -> None:
        try:
            # 1. Here you need to instantiate a GoogleSearchTool with GoogleSearchTool(), it will automatically load the yaml configuration of GoogleSearchTool, but at this time it does not have credential information inside
            # 2. Then you need to use the fork_tool_runtime method to pass the current credential information to GoogleSearchTool
            # 3. Finally, invoke it, the parameters need to be passed according to the parameter rules configured in the yaml of GoogleSearchTool
            GoogleSearchTool().fork_tool_runtime(
                meta={
                    "credentials": credentials,
                }
            ).invoke(
                user_id='',
                tool_parameters={
                    "query": "test",
                    "result_type": "link"
                },
            )
        except Exception as e:
            raise ToolProviderCredentialValidationError(str(e))
```

### Completion

After the above steps are completed, we can see this tool on the frontend, and it can be used in the Agent.

Of course, because google\_search needs a credential, before using it, you also need to input your credentials on the frontend.

<figure><img src="../../.gitbook/assets/Feb 4, 2024 (1).png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/tools/README.md
```markdown
# Tools

### Tool Definition

Tools can extend the capabilities of LLMs (Language Learning Models), such as performing web searches, scientific calculations, or generating images, thereby enhancing the LLM's ability to connect with the external world. Dify provides two types of tools: **First-party Tools** and **Custom Tools**.

You can directly use the first-party built-in tools provided by the Dify ecosystem, or easily import custom API tools (currently supporting OpenAPI / Swagger and OpenAI Plugin specifications).

#### Functions of Tools:

1. Tools allow users to create more powerful AI applications on Dify. For example, you can arrange suitable tools for an intelligent assistant application (Agent) that can complete complex tasks through task reasoning, step-by-step breakdown, and tool invocation.
2. They facilitate connecting your application with other systems or services and interacting with the external environment, such as code execution or access to proprietary information sources.

### How to Configure First-party Tools

<figure><img src="../../.gitbook/assets/first-party-tools.png" alt=""><figcaption><p>First-party Tools List</p></figcaption></figure>

Dify currently supports:

<table><thead><tr><th width="154">Tool</th><th>Description</th></tr></thead><tbody><tr><td>Google Search</td><td>Tool for performing Google SERP searches and extracting snippets and web pages. The input should be a search query.</td></tr><tr><td>Wikipedia</td><td>Tool for performing Wikipedia searches and extracting snippets and web pages.</td></tr><tr><td>DALL-E Drawing</td><td>Tool for generating high-quality images through natural language input.</td></tr><tr><td>Web Scraping</td><td>Tool for scraping web data.</td></tr><tr><td>WolframAlpha</td><td>A powerful computational knowledge engine that provides standardized answers based on questions and has strong mathematical computation capabilities.</td></tr><tr><td>Chart Generation</td><td>Tool for generating visual charts, allowing you to create bar charts, line charts, pie charts, and other types of charts.</td></tr><tr><td>Current Time</td><td>Tool for querying the current time.</td></tr><tr><td>Yahoo Finance</td><td>Tool for obtaining and organizing the latest financial information, such as news and stock quotes.</td></tr><tr><td>Stable Diffusion</td><td>A tool for generating images that can be deployed locally using stable-diffusion-webui.</td></tr><tr><td>Vectorizer</td><td>Tool for quickly and easily converting PNG and JPG images to SVG vector graphics.</td></tr><tr><td>YouTube</td><td>Tool for retrieving statistics of YouTube channel videos.</td></tr></tbody></table>

{% hint style="info" %}
We welcome you to contribute your developed tools to Dify. For detailed methods on how to contribute, please refer to the [Dify Development Contribution Documentation](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md). Your support is invaluable to us.
{% endhint %}

#### First-party Tool Authorization

If you need to use the first-party built-in tools provided by the Dify ecosystem, you need to configure the corresponding credentials before using them.

<figure><img src="../../.gitbook/assets/configure-first-party-tool-api.png" alt=""><figcaption><p>Configure First-party Tool Credentials</p></figcaption></figure>

Once the credentials are successfully verified, the tool will display an "Authorized" status. After configuring the credentials, all members in the workspace can use this tool when arranging applications.

### How to Create Custom Tools

You can import custom API tools in the "Tools - Custom Tools" section, currently supporting OpenAPI / Swagger and ChatGPT Plugin specifications. You can directly paste the OpenAPI schema content or import it from a URL. For the OpenAPI / Swagger specification, you can refer to the [official documentation](https://swagger.io/specification/).

Currently, tools support two authentication methods: No Authentication and API Key.

<figure><img src="../../.gitbook/assets/en-tools-create-customized-tools-1.png" alt=""><figcaption><p>Create Custom Tools</p></figcaption></figure>

After importing the schema content, the system will automatically parse the parameters in the file, and you can preview the specific parameters, methods, and paths of the tool. You can also test the tool parameters here.

<figure><img src="../../.gitbook/assets/en-tools-create-customized-tools-2.png" alt=""><figcaption><p>Custom Tool Parameter Testing</p></figcaption></figure>

Once the custom tool is created, all members in the workspace can use this tool when arranging applications in the "Studio."

<figure><img src="../../.gitbook/assets/en-tools-create-customized-tools-3.png" alt=""><figcaption><p>Custom Tool Added</p></figcaption></figure>

#### Cloudflare Workers

You can also use [dify-tools-worker](https://github.com/crazywoola/dify-tools-worker) to quickly deploy custom tools. This tool provides:

* Routes that can be imported into Dify `https://difytoolsworker.yourname.workers.dev/doc`, offering an OpenAPI-compatible interface documentation.
* API implementation code that can be directly deployed to Cloudflare Workers.

### How to Use Tools in Applications

Currently, you can use the configured tools when creating **intelligent assistant applications** in the "Studio."

<figure><img src="../../.gitbook/assets/use-tools-in-app.png" alt=""><figcaption><p>Add Tools When Creating Intelligent Assistant Applications</p></figcaption></figure>

For example, after adding tools in a financial analysis application, the intelligent assistant will autonomously invoke tools when needed to query financial report data, analyze the data, and complete the conversation with the user.

<figure><img src="../../.gitbook/assets/ai-using-tools-during-conversation.png" alt=""><figcaption><p>Intelligent Assistant Using Tools to Answer Questions During Conversation</p></figcaption></figure>
```

## File: en/guides/workflow/debug-and-preview/checklist.md
```markdown
# Checklist

Before publishing the App, you can check the checklist to see if there are any nodes with incomplete configurations or that have not been connected.

<figure><img src="../../../.gitbook/assets/checklist.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/debug-and-preview/history.md
```markdown
# Run History

In the "Run History," you can view the run results and log information from the historical debugging of the current workflow.

<figure><img src="../../../.gitbook/assets/logs-history.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/debug-and-preview/log.md
```markdown
# Conversation/Run Logs

Clicking **"Run History - View Log — Details"** allows you to see a comprehensive overview of the run in the details section. This includes information on inputs and outputs, metadata, and other relevant data.

This detailed information enables you to review various aspects of each node throughout the complete execution process of the workflow. You can examine inputs and outputs, analyze token consumption, evaluate runtime duration, and assess other pertinent metrics.

<figure><img src="../../../.gitbook/assets/conversation-logs.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/debug-and-preview/README.md
```markdown
# Debug and Preview
```

## File: en/guides/workflow/debug-and-preview/step-run.md
```markdown
# Step Run

Workflow supports step-by-step debugging of nodes, where you can repetitively test whether the execution of the current node meets expectations.

<figure><img src="../../../.gitbook/assets/output (3) (3).png" alt=""><figcaption></figcaption></figure>

After running a step test, you can review the execution status, input/output, and metadata information.

<figure><img src="../../../.gitbook/assets/output (4) (2).png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/debug-and-preview/yu-lan-yu-yun-hang.md
```markdown
# Preview and Run

Dify Workflow offers a comprehensive set of execution and debugging features. In conversational applications, clicking "Preview" enters debugging mode.

<figure><img src="../../../.gitbook/assets/output (7).png" alt=""><figcaption></figcaption></figure>

In workflow applications, clicking "Run" enters debugging mode.

<figure><img src="../../../.gitbook/assets/output (1) (5).png" alt=""><figcaption></figcaption></figure>

Once in debugging mode, you can debug the configured workflow using the interface on the right side of the screen.
```

## File: en/guides/workflow/error-handling/error-type.md
```markdown
# Error Type

This article summarizes the potential exceptions and corresponding error types that may occur in different types of nodes.

### General Error

*   **System error**

    Typically caused by system issues such as a disabled sandbox service or network connection problems.
*   **Operational Error**

    Occurs when developers are unable to configure or run the node correctly.

### Code Node

[Code](../node/code.md) nodes support running Python and JavaScript code for data transformation in workflows or chat flows. Here are 4 common runtime errors:

1.  **Code Node Error (CodeNodeError)**&#x20;

    This error occurs due to exceptions in developer-written code, such as: missing variables, calculation logic errors, or treating string array inputs as string variables. You can locate the issue using the error message and exact line number.

<figure><img src="https://assets-docs.dify.ai/2024/12/c86b11af7f92368180ea1bac38d77083.png" alt=""><figcaption><p>Code Error</p></figcaption></figure>

2.  **Sandbox Network Issues (System Error)**

    This error commonly occurs when there are network traffic or connection issues, such as when the sandbox service isn't running or proxy services have interrupted the network. You can resolve this through the following steps:

    1. Check network service quality
    2. Start the sandbox service
    3. Verify proxy settings

<figure><img src="https://assets-docs.dify.ai/2024/12/d95007adf67c4f232e46ec455c348e2c.PNG" alt="" width="375"><figcaption><p>Sandbox network issues</p></figcaption></figure>

3.  **Depth Limit Error (DepthLimitError)**

    The current node's default configuration only supports up to 5 levels of nested structures. An error will occur if it exceeds 5 levels.

<figure><img src="https://assets-docs.dify.ai/2024/12/5649d52a6e80ddd4180b336266701f7b.png" alt=""><figcaption><p><strong>OutputValidationError</strong></p></figcaption></figure>

4.  **Output Validation Error (OutputValidationError)**

    An error occurs when the actual output variable type doesn't match the selected output variable type. Developers need to change the selected output variable type to avoid this issue.

<figure><img src="https://assets-docs.dify.ai/2024/12/ab8cae01a590b037017dfe9ea4dbbb8b.png" alt=""><figcaption></figcaption></figure>

### LLM Node

The [LLM](../node/llm.md) node is a core component of Chatflow and Workflow, utilizing LLM' capabilities in dialogue, generation, classification, and processing to complete various tasks based on user input instructions.&#x20;

Here are 6 common runtime errors:

1.  **Variable Not Found Error (VariableNotFoundError)**

    This error occurs when the LLM cannot find system prompts or variables set in the context. Application developers can resolve this by replacing the problematic variables.

<figure><img src="https://assets-docs.dify.ai/2024/12/f20c5fbde345144de6183374ab277662.png" alt=""><figcaption></figcaption></figure>

2.  **Invalid Context Structure Error (InvalidContextStructureError)**&#x20;

    An error occurs when the context within the LLM node receives an invalid data structure (such as `array[object]`).&#x20;

    > Context only supports string (String) data structures.

<figure><img src="https://assets-docs.dify.ai/2024/12/f20c5fbde345144de6183374ab277662.png" alt=""><figcaption><p><strong>InvalidContextStructureError</strong></p></figcaption></figure>

3.  **Invalid Variable Type Error (InvalidVariableTypeError)**

    This error appears when the system prompt type is not in the standard Prompt text format or Jinja syntax format.
4.  **Model Not Exist Error (ModelNotExistError)**

    Each LLM node requires a configured model. This error occurs when no model is selected.
5.  **LLM Authorization Required Error (LLMModeRequiredError)**

    The model selected in the LLM node has no configured API Key. You can refer to the documentation for model authorization.
6.  **No Prompt Found Error (NoPromptFoundError)**

    An error occurs when the LLM node's prompt is empty, as prompts cannot be blank.

<figure><img src="https://assets-docs.dify.ai/2024/12/9882f7a5ee544508ba11b51fb469a911.png" alt=""><figcaption></figcaption></figure>

### HTTP

[HTTP](../node/http-request.md) nodes allow seamless integration with external services through customizable requests for data retrieval, webhook triggering, image generation, or file downloads via HTTP requests. Here are 5 common errors for this node:

1.  **Authorization Configuration Error (AuthorizationConfigError)**&#x20;

    This error occurs when authentication information (Auth) is not configured.
2. **File Fetch Error (FileFetchError)** This error appears when file variables cannot be retrieved.
3.  **Invalid HTTP Method Error (InvalidHttpMethodError)**&#x20;

    An error occurs when the request header method is not one of the following: GET, HEAD, POST, PUT, PATCH, or DELETE.
4.  **Response Size Error (ResponseSizeError)**&#x20;

    HTTP response size is limited to 10MB. An error occurs if the response exceeds this limit.
5. **HTTP Response Code Error (HTTPResponseCodeError)** An error occurs when the request response returns a code that doesn't start with 2 (such as 200, 201). If exception handling is enabled, errors will occur for status codes 400, 404, and 500; otherwise, these won't trigger errors.

### Tool

The following 3 errors commonly occur during runtime:

1.  **Tool Execution Error (ToolNodeError)**&#x20;

    An error that occurs during tool execution itself, such as when reaching the target API's request limit.



    <figure><img src="https://assets-docs.dify.ai/2024/12/84af0831b7cb23e64159dfbba80e9b28.jpg" alt="" width="375"><figcaption></figcaption></figure>
2.  **Tool Parameter Error (ToolParameterError)**&#x20;

    An error occurs when the configured tool node parameters are invalid, such as passing parameters that don't match the tool node's defined parameters.
3.  **Tool File Processing Error (ToolFileError)**&#x20;

    An error occurs when the tool node cannot find the required files.
```

## File: en/guides/workflow/error-handling/predefined-error-handling-logic.md
```markdown
# Predefined Error Handling Logic

Here are four types of nodes that provide with predefined logic for handling unexpected situations:

•  [LLM](../node/llm.md)

•  [HTTP](../node/http-request.md)&#x20;

•  [Code](../node/code.md)

•  [Tool](../node/tools.md)

The error handling feature provides three predefined options:

• **None**: Errors are not handled. The node throws its built-in error message, causing the entire workflow to stop.

• **Default value**: Developers can predefine an alternative output for the node. If an error occurs, the workflow throws the predefined value instead of the node’s original error output, allowing the workflow process to continue seamlessly.

• **Fail branch**: When an error occurs, a predefined error-handling branch is executed. This provides flexibility for developers to create alternative paths in the workflow to address the failure scenario.

<figure><img src="https://assets-docs.dify.ai/2024/12/6e2655949889d4d162945d840d698649.png" alt=""><figcaption></figcaption></figure>

### Logic: None

Default option for the node’s error-handling feature. If the node encounters a timeout or an error during execution, it directly throws the node’s built-in error message, immediately halting the entire workflow. The workflow execution is then recorded as failed.

### Logic: Default Value

This option lets developers customize a node’s error output through the default value editor, similar to the step-by-step debugging approach used in programming. It helps clarify issues, making the debugging process more transparent and efficient.

For example:

* For `object` and `array` data types, the system provides an intuitive JSON editor.
* For `number` and `string` data types, corresponding type-specific editors are available.

When a node fails to execute, the workflow automatically uses the developer’s predefined default value to replace the original error output from the node, ensuring the workflow remains uninterrupted. Clearer error messages improve troubleshooting efficiency, allowing developers to focus on optimizing the workflow design.

The predefined default value’s data type must match the node’s output variable type. For example, if the output variable of a code node is set to the data type `array[number]`, the default value must also be of the `array[number]` data type.

<figure><img src="https://assets-docs.dify.ai/2024/12/e9e5e757090679243e0c9976093c7e6c.png" alt="" width="375"><figcaption><p>Error handling: default value</p></figcaption></figure>

### Logic: Fail Branch

If the current node encounters an error, it triggers the predefined fail branch. When you select the fail branch option, new connection points are provided for the node, allowing developers to continue designing the workflow or add downstream nodes by clicking the bottom-right corner of the node details.

For instance, you might connect a mail tool node to send error notifications, providing real-time alerts when issues arise.

> The fail branch is highlighted with orange.

<figure><img src="https://assets-docs.dify.ai/2024/12/e5ea1af947818bd9e27cab3042c1c4f3.png" alt=""><figcaption></figcaption></figure>

A common approach to handling errors is enable fail branch on nodes. These nodes can address issues, and the corrected outputs can be merged back into the main flow by using variable aggregation nodes to ensure consistency in the final results.&#x20;

### Exception Variables

When the **Default Value** or **Fail Branch** is selected for a node’s error handling, the node will transfer the error information to the downstream nodes using the `error_type` and `error_message` exception variables when it encounters an error.

| Variable        | Descriptions                                                                                                                                                                                   |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `error_type`    | Error Types. Different types of nodes come with distin[ct error types](error-type.md). Developers can design tailored solutions based on these error identifiers.                              |
| `error_message` | Error information. Specific fault information is output by the abnormal node. Developers can pass it to the downstream LLM node for repair or connect to the mailbox tool to push information. |
```

## File: en/guides/workflow/error-handling/README.md
```markdown
# Error Handling

Workflow applications often comprise multiple interconnected nodes operating in sequence. When an error occurs—such as an API request failure, an LLM output issue, or an unexpected exception—it can disrupt the entire process. Such disruptions force developers to spend significant time troubleshooting, especially in workflows with complex node dependencies.

**Error handling** introduces robust strategies to manage node failures effectively. These feature allow workflows to log and monitor errors without halting execution or switch seamlessly to predefined fallback paths, ensuring task continuity. **Developers can significantly improve application reliability and operational resilience by integrating strong error-handling feature into critical nodes.**

Developers no longer need to handle potential node errors by embedding complex logic within nodes or adding extra nodes. The error-handling feature simplifies workflow design, enabling streamlined execution through various predefined strategies.

{% @arcade/embed flowId="g0ePRj5dA5WVv6noiPKX" url="https://app.arcade.software/share/g0ePRj5dA5WVv6noiPKX" %}

### Application Scenarios

#### 1. Handling Network Exceptions

**Example**: In a workflow that retrieves and aggregates data from three API services (such as weather services, news summaries, and social media analysis), one service might fail to respond due to request limits, causing data retrieval to fail. With the error-handling function, the main process can continue using the data from the other two successful services while logging the failed API request. This log helps developers analyze the issue later and refine their service call strategies.

#### 2. Backup Workflow Design

**Example**: An LLM node tasked with generating detailed document summaries may encounter token limit errors when processing lengthy input. The workflow can switch to a backup path by setting the "Fail branch" on Error-handling Feature.

For instance, a code node on the alternative path can split the content into smaller chunks and re-invoke the LLM node, preventing the workflow from breaking down.

#### 3. Predefined Error Messages

**Example**: When running a workflow, you might occasionally encounter a node returning vague error messages (such as a simple "request failed"), complicating pinpointing the issue quickly. Developers can write predefined error messages within the error handling feature to provide more explicit and more precise error information for subsequent application debugging.

### Error Handling Feature

The following four types of nodes have added error-handling feature. Click on the title to read the detailed documents:

* [LLM](../node/llm.md)
* [HTTP](../node/http-request.md)
* [Code](../node/code.md)
* [Tools](../node/tools.md)

**Retry on Failure**

Some exceptions can be resolved by retrying the node. In this case, you can enable the **Retry on Failure** feature in the node and set the number of max retries and the retry interval.

![](https://assets-docs.dify.ai/2024/12/18097e4c94b67a79150b967fc50f9f43.png)

If an error is still reported after retrying the node, the next process will be run according to the predefined strategy in the Error Handling feature.

**Error Handling**

The error handling feature provides the following three options:

• **None**: Do not handle the exception, directly throw the node's error message and interrupt the entire process.&#x20;

• **Default Value**: Allows developers to predefine exception messages. After an exception occurs, use the predefined value to replace the original built-in error output message of the node.&#x20;

• **Fail Branch**: Execute the pre-arranged fail branch after an exception occurs.

For explanations and configuration methods of each strategy, please refer to the [predefined error handling logic](predefined-error-handling-logic.md).

![Error handling](https://assets-docs.dify.ai/2024/12/3c198be3a7b9c1f9649bbd8b9a0a9ec5.png)

### Quick Start

Scenario: Enabling Error-handling feature for Workflow Application

Error Handling feature for Code Output in Workflow Applications The following example demonstrates how to implement error handling feature within a workflow application, using fail branch to handle node exceptions.

The idea of the workflow design: An LLM node generates JSON code content (either correctly or incorrectly formatted) based on user's input instructions, which is then executed and output through Code Node A.&#x20;

If Code Node A receives incorrectly formatted JSON content, it follows the predefined error handling design, executing the backup path while continuing the main process.

1. **Creating a JSON Code Generation Node**&#x20;

Create a new Workflow application and add both LLM and Code nodes. Use a Prompt to instruct the LLM to generate either correctly or incorrectly formatted JSON content, which will then be validated through Code Node A.&#x20;

**The Prompt reference of LLM node:**

```
You are a teaching assistant. According to the user's requirements, you only output a correct or incorrect sample code in json format.
```

**The JSON verification of Code Node：**

```python
def main(json_str: str) -> dict:
    obj = json.loads(json_str)
    return {'result': obj}
```

2. **Enable Error Handling Feature for Node A**

Node A is responsible for validating JSON content. If it receives incorrectly formatted JSON content, the error handling feature will be triggered and execute th backup path, allowing the subsequent LLM node to fix the incorrect content and revalidate the JSON, thereby continuing the main process.&#x20;

In the "Error Handling" tab of Node A, select "Fail Branch" and create a new LLM node.

{% @arcade/embed flowId="rKbAJ2tYTbTA9JXhMMun" url="https://app.arcade.software/share/rKbAJ2tYTbTA9JXhMMun" %}

3. **Correct the Error Output from Node A**

In the new LLM node, fill in the prompt and reference the exception output from Node A using variables for correction. Add Node B to revalidate the JSON content.

**4. End**

Add a variable aggregation node to consolidate the results from both the correct and error branches and output them to the end node, completing the entire process.

<figure><img src="https://assets-docs.dify.ai/2024/12/059b5a814514cd9abe10f1f4077ed17f.png" alt=""><figcaption></figcaption></figure>

> Click [here](https://assets-docs.dify.ai/2024/12/087861aa20e06bb4f8a2bef7e7ae0522.yml) to download the Demo DSL file.

### Status Overview

In workflow applications, understanding both node and workflow status is crucial for effective monitoring and troubleshooting. Let's explore how status indicators help developers track execution progress and handle exceptions efficiently.

#### Node Status Types

* **Success**: Every node runs properly - the node completes its task and produces the expected output.
* **Failure**: When error handling isn't enabled, the node stops working and reports an error.
* **Exception**: Even though an error occurs, the node doesn't completely fail because error handling (either default values or alternative paths) kicks in to manage the situation.

#### Workflow Status Types

* **Success**: A perfect run - all nodes complete their tasks successfully, and the workflow produces the intended output.
* **Failure**: The workflow stops completely due to an unhandled node error.
* **Partial Success**: Think of this as a "managed failure" - while some nodes encounter issues, error handling mechanisms keep the workflow moving forward to completion.

### FAQ

1. **What is the difference before and after enabling the exception handling mechanism?**

#### Before Implementation

Without error handling, workflows are quite fragile:

* A single node failure (like an LLM timeout or network hiccup) brings everything to a halt
* Developers must manually investigate and fix issues before restarting
* Creating workarounds means building complex, redundant safety nets
* Error messages tend to be vague and unhelpful

#### After Implementation

Error handling transforms your workflow into a more resilient system:

* Workflows keep running even when things go wrong
* Developers can create custom responses for different types of errors
* The overall design becomes cleaner and more maintainable
* Detailed error logging makes troubleshooting much faster

***

2. **How to debug the execution of backup paths?**

Need to check if your error handling is working? It's simple - just look for yellow-highlighted paths in your workflow logs. These visual indicators show exactly when and where your backup error handling routes are being used.
```

## File: en/guides/workflow/node/agent.md
```markdown
# Agent

## Definition

An Agent Node is a component in Dify Chatflow/Workflow that enables autonomous tool invocation. By integrating different Agent reasoning strategies, LLMs can dynamically select and execute tools at runtime, thereby performing multi-step reasoning.

## Configuration Steps

### Add the Node

In the Dify Chatflow/Workflow editor, drag the Agent node from the components panel onto the canvas.

<figure><img src="../../../.gitbook/assets/en-1-9-1.png" alt=""><figcaption></figcaption></figure>

### Select an Agent Strategy

In the node configuration panel, click Agent Strategy.

<figure><img src="../../../.gitbook/assets/en-1-9-0.png" alt=""><figcaption></figcaption></figure>

From the dropdown menu, select the desired Agent reasoning strategy. Dify provides two built-in strategies, **Function Calling and ReAct**, which can be installed from the **Marketplace → Agent Strategies category**.

<figure><img src="../../../.gitbook/assets/en-1-9-2.png" alt=""><figcaption></figcaption></figure>

#### 1. Function Calling

Function Calling maps user commands to predefined functions or tools. The LLM first identifies user intent, then decides which function to call and extracts the required parameters. Its core mechanism involves explicitly calling external functions or tools.

Pros:

**• Precision:** For well-defined tasks, it can call the corresponding tool directly without requiring complex reasoning.

**• Easier external feature integration:** Various external APIs or tools can be wrapped into functions for the model to call.

**• Structured output:** The model outputs structured information about function calls, facilitating processing by downstream nodes.

<figure><img src="../../../.gitbook/assets/en-agent-1.png" alt=""><figcaption></figcaption></figure>

#### 2. ReAct (Reason + Act)

ReAct enables the Agent to alternate between reasoning and taking action: the LLM first thinks about the current state and goal, then selects and calls the appropriate tool. The tool’s output in turn informs the LLM’s next step of reasoning and action. This cycle continues until the problem is resolved.

Pros:

**• Effective external information use:** It can leverage external tools to retrieve information and handle tasks that the model alone cannot accomplish.

**• Improved explainability:** Because reasoning and actions are interwoven, there is a certain level of traceability in the Agent’s thought process.

**• Wide applicability:** Suitable for scenarios that require external knowledge or need to perform specific actions, such as Q\&A, information retrieval, and task execution.

<figure><img src="../../../.gitbook/assets/en-agent-2.png" alt=""><figcaption></figcaption></figure>

Developers can contribute Agent strategy plugins to the public [repository](https://github.com/langgenius/dify-official-plugins). After review, these plugins will be listed in the Marketplace for others to install.

### Configure Node Parameters

After choosing the Agent strategy, the configuration panel will display the relevant options. For the Function Calling and ReAct strategies that ship with Dify, the available configuration items include:

1. **Model:** Select the large language model that drives the Agent.
2. **Tools List:** The approach to using tools is defined by the Agent strategy. Click + to add and configure tools the Agent can call.
   * Search: Select an installed tool plugin from the dropdown.
   * Authorization: Provide API keys and other credentials to enable the tool.
   * Tool Description and Parameter Settings: Provide a description to help the LLM understand when and why to use the tool, and configure any functional parameters.
3. **Instruction**: Define the Agent’s task goals and context. Jinja syntax is supported to reference upstream node variables.
4. **Query**: Receives user input.
5. **Maximum Iterations:** Set the maximum number of execution steps for the Agent.
6. **Output Variables:** Indicates the data structure output by the node.

<figure><img src="../../../.gitbook/assets/en-1-9-3.png" alt=""><figcaption></figcaption></figure>

## Logs

During execution, the Agent node generates detailed logs. You can see overall node execution information—including inputs and outputs, token usage, time spent, and status. Click Details to view the output from each round of Agent strategy execution.

<figure><img src="../../../.gitbook/assets/en-1-9-6.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/node/answer.md
```markdown
# Answer

Defining Reply Content in a Chatflow Process. In a text editor, you have the flexibility to determine the reply format. This includes crafting a fixed block of text, utilizing output variables from preceding steps as the reply content, or merging custom text with variables for the response.

Answer node can be seamlessly integrated at any point to dynamically deliver content into the dialogue responses. This setup supports a live-editing configuration mode, allowing for both text and image content to be arranged together. The configurations include:

1. Outputting the reply content from a Language Model (LLM) node.
2. Outputting generated images.
3. Outputting plain text.

Example 1: Output plain text.

<figure><img src="../../../.gitbook/assets/answer-plain-text.png" alt=""><figcaption></figcaption></figure>

Example 2: Output image and LLM reply.

<figure><img src="../../../.gitbook/assets/answer-img-1.png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/answer-img-2.png" alt="" width="275"><figcaption></figcaption></figure>
```

## File: en/guides/workflow/node/code.md
```markdown
# Code Execution

## Table of Contents

* [Introduction](code.md#introduction)
* [Usage Scenarios](code.md#usage-scenarios)
* [Local Deployment](code.md#local-deployment)
* [Security Policies](code.md#security-policies)

## Introduction

The code node supports running Python/NodeJS code to perform data transformations within a workflow. It can simplify your workflow and is suitable for scenarios such as arithmetic operations, JSON transformations, text processing, and more.

This node significantly enhances the flexibility for developers, allowing them to embed custom Python or JavaScript scripts within the workflow and manipulate variables in ways that preset nodes cannot achieve. Through configuration options, you can specify the required input and output variables and write the corresponding execution code:

<figure><img src="../../../.gitbook/assets/image (157).png" alt="" width="375"><figcaption></figcaption></figure>

## Configuration

If you need to use variables from other nodes in the code node, you must define the variable names in the `input variables` and reference these variables. You can refer to [Variable References](../key-concept.md#variables).

## Usage Scenarios

Using the code node, you can perform the following common operations:

### Structured Data Processing

In workflows, you often have to deal with unstructured data processing, such as parsing, extracting, and transforming JSON strings. A typical example is data processing from an HTTP node. In common API return structures, data may be nested within multiple layers of JSON objects, and you need to extract certain fields. The code node can help you perform these operations. Here is a simple example that extracts the `data.name` field from a JSON string returned by an HTTP node:

```python
def main(http_response: str) -> str:
    import json
    data = json.loads(http_response)
    return {
        # Note to declare 'result' in the output variables
        'result': data['data']['name'] 
    }
```

### Mathematical Calculations

When you need to perform complex mathematical calculations in a workflow, you can also use the code node. For example, calculating a complex mathematical formula or performing some statistical analysis on data. Here is a simple example that calculates the variance of an array:

```python
def main(x: list) -> float:
    return {
        # Note to declare 'result' in the output variables
        'result': sum([(i - sum(x) / len(x)) ** 2 for i in x]) / len(x)
    }
```

### Data Concatenation

Sometimes, you may need to concatenate multiple data sources, such as multiple knowledge retrievals, data searches, API calls, etc. The code node can help you integrate these data sources together. Here is a simple example that merges data from two knowledge bases:

```python
def main(knowledge1: list, knowledge2: list) -> list:
    return {
        # Note to declare 'result' in the output variables
        'result': knowledge1 + knowledge2
    }
```

## Local Deployment

If you are a local deployment user, you need to start a sandbox service to ensure that malicious code is not executed. This service requires the use of Docker. You can find specific information about the sandbox service [here](https://github.com/langgenius/dify/tree/main/docker/docker-compose.middleware.yaml). You can also start the service directly via `docker-compose`:

```bash
docker-compose -f docker-compose.middleware.yaml up -d
```

## Security Policies

Both Python and JavaScript execution environments are strictly isolated (sandboxed) to ensure security. This means that developers cannot use functions that consume large amounts of system resources or may pose security risks, such as direct file system access, making network requests, or executing operating system-level commands. These limitations ensure the safe execution of the code while avoiding excessive consumption of system resources.

### Advanced Features

**Retry on Failure**

For some exceptions that occur in the node, it is usually sufficient to retry the node again. When the error retry function is enabled, the node will automatically retry according to the preset strategy when an error occurs. You can adjust the maximum number of retries and the interval between each retry to set the retry strategy.

- The maximum number of retries is 10
- The maximum retry interval is 5000 ms

![](https://assets-docs.dify.ai/2024/12/9fdd5525a91dc925b79b89272893becf.png)

**Error Handling**

When processing information, code nodes may encounter code execution exceptions. Developers can follow these steps to configure fail branches, enabling contingency plans when nodes encounter exceptions, thus avoiding workflow interruptions.

1. Enable "Error Handling" in the code node
2. Select and configure an error handling strategy

![Code Error handling](https://assets-docs.dify.ai/2024/12/58f392734ce44b22cd8c160faf28cd14.png)

For more information about exception handling approaches, please refer to [Error Handling](https://docs.dify.ai/zh-hans/guides/workflow/error-handling).

### FAQ

**Why can't I save the code it in the code node?**

Please check if the code contains potentially dangerous behaviors. For example:

```python
def main() -> dict:
    return {
        "result": open("/etc/passwd").read(),
    }
```

This code snippet has the following issues:

* **Unauthorized file access:** The code attempts to read the "/etc/passwd" file, which is a critical system file in Unix/Linux systems that stores user account information.
* **Sensitive information disclosure:** The "/etc/passwd" file contains important information about system users, such as usernames, user IDs, group IDs, home directory paths, etc. Direct access could lead to information leakage.

Dangerous code will be automatically blocked by Cloudflare WAF. You can check if it's been blocked by looking at the "Network" tab in your browser's "Web Developer Tools".

<figure><img src="https://assets-docs.dify.ai/2024/12/ad4dc065c4c567c150ab7fa7bfd123a3.png" alt=""><figcaption><p>Cloudflare WAF</p></figcaption></figure>
```

## File: en/guides/workflow/node/doc-extractor.md
```markdown
# Doc Extractor

#### Definition

LLMs cannot directly read or interpret document contents. Therefore, it's necessary to parse and read information from user-uploaded documents through a document extractor node, convert it to text, and then pass the content to the LLM to process the file contents.

#### Application Scenarios

* Building LLM applications that can interact with files, such as ChatPDF or ChatWord;
* Analyzing and examining the contents of user-uploaded files;

#### Node Functionality

The document extractor node can be understood as an information processing center. It recognizes and reads files in the input variables, extracts information, and converts it into string-type output variables for downstream nodes to call.

<figure><img src="../../../.gitbook/assets/image (11).png" alt=""><figcaption><p>doc extractor</p></figcaption></figure>

The document extractor node structure is divided into input variables and output variables.

**Input Variables**

The document extractor only accepts variables with the following data structures:

* `File`, a single file
* `Array[File]`, multiple files

The document extractor can only extract information from document-type files, such as the contents of TXT, Markdown, PDF, HTML, DOCX format files. It cannot process image, audio, video, or other file formats.

**Output Variables**

The output variable is fixed and named as text. The type of output variable depends on the input variable:

* If the input variable is `File`, the output variable is `string`
* If the input variable is `Array[File]`, the output variable is `array[string]`

> Array variables generally need to be used in conjunction with list operation nodes. For detailed instructions, please refer to list-operator.

#### Configuration Example

In a typical file interaction Q\&A scenario, the document extractor can serve as a preliminary step for the LLM node, extracting file information from the application and passing it to the downstream LLM node to answer user questions about the file.

This section will introduce the usage of the document extractor node through a typical ChatPDF example workflow template.

<figure><img src="../../../.gitbook/assets/image (12).png" alt=""><figcaption><p>Chatpdf workflow</p></figcaption></figure>

**Configuration Process:**

1. Enable file upload for the application. Add a **single file variable** in the "Start" node and name it `pdf`.
2. Add a document extractor node and select the `pdf` variable in the input variables.
3. Add an LLM node and select the output variable of the document extractor node in the system prompt. The LLM can read the contents of the file through this output variable.

<figure><img src="../../../.gitbook/assets/image (13).png" alt=""><figcaption></figcaption></figure>

Configure the end node by selecting the output variable of the LLM node in the end node.

<figure><img src="../../../.gitbook/assets/image (14).png" alt=""><figcaption><p>chat with pdf</p></figcaption></figure>

After configuration, the application will have file upload functionality, allowing users to upload PDF files and engage in conversation.

{% hint style="info" %}
To learn how to upload files in chat conversations and interact with the LLM, please refer to Additional Features.
{% endhint %}
```

## File: en/guides/workflow/node/end.md
```markdown
# End

### 1 Definition

Define the final output content of a workflow. Every workflow needs at least one end node after complete execution to output the final result.

The end node is a termination point in the process; no further nodes can be added after it. In a workflow application, results are only output when the end node is reached. If there are conditional branches in the process, multiple end nodes need to be defined.

The end node must declare one or more output variables, which can reference any upstream node's output variables.

{% hint style="info" %}
End nodes are not supported within Chatflow.
{% endhint %}

***

### 2 Scenarios

In the following [long story generation workflow](iteration.md#example-2-long-article-iterative-generation-another-scheduling-method), the variable `Output` declared by the end node is the output of the upstream code node. This means the workflow will end after the Code node completes execution and will output the execution result of Code.

<figure><img src="../../../.gitbook/assets/end-answer.png" alt=""><figcaption><p>End Node - Long Story Generation Example</p></figcaption></figure>

**Single Path Execution Example:**

<figure><img src="../../../.gitbook/assets/single-path-execution.png" alt=""><figcaption></figcaption></figure>

**Multi-Path Execution Example:**

<figure><img src="../../../.gitbook/assets/output (1) (3).png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/node/http-request.md
```markdown
# HTTP Request

### Definition

Allows sending server requests via the HTTP protocol, suitable for scenarios such as retrieving external data, webhooks, generating images, and downloading files. It enables you to send customized HTTP requests to specified web addresses, achieving interconnectivity with various external services.

This node supports common HTTP request methods:

* **GET**: Used to request the server to send a specific resource.
* **POST**: Used to submit data to the server, typically for submitting forms or uploading files.
* **HEAD**: Similar to GET requests, but the server only returns the response headers without the resource body.
* **PATCH**: Used to apply partial modifications to a resource.
* **PUT**: Used to upload resources to the server, typically for updating an existing resource or creating a new one.
* **DELETE**: Used to request the server to delete a specified resource.

You can configure various aspects of the HTTP request, including URL, request headers, query parameters, request body content, and authentication information.

<figure><img src="../../../.gitbook/assets/workflow-http-request-node.png" alt="" width="332"><figcaption><p>HTTP Request Configuration</p></figcaption></figure>

***

### Scenarios

* **Send Application Interaction Content to a Specific Server**

One practical feature of this node is the ability to dynamically insert variables into different parts of the request based on the scenario. For example, when handling customer feedback requests, you can embed variables such as username or customer ID, feedback content, etc., into the request to customize automated reply messages or fetch specific customer information and send related resources to a designated server.

<figure><img src="../../../.gitbook/assets/customer-feedback-classification.png" alt=""><figcaption><p>Customer Feedback Classification</p></figcaption></figure>

The return values of an HTTP request include the response body, status code, response headers, and files. Notably, if the response contains a file, this node can automatically save the file for use in subsequent steps of the workflow. This design not only improves processing efficiency but also makes handling responses with files straightforward and direct.

* **Send File**

You can use an HTTP PUT request to send files from the application to other API services. In the request body, you can select the file variable within the `binary`. This method is commonly used in scenarios such as file transfer, document storage, or media processing.

Example: Suppose you are developing a document management application and need to send a user-uploaded PDF file to a third-party service. You can use an HTTP request node to pass the file variable.

Here is a configuration example:

<figure><img src="../../../.gitbook/assets/image (145).png" alt=""><figcaption><p>http-node-send-file</p></figcaption></figure>

### Advanced Features

**Retry on Failure**

For some exceptions that occur in the node, it is usually sufficient to retry the node again. When the error retry function is enabled, the node will automatically retry according to the preset strategy when an error occurs. You can adjust the maximum number of retries and the interval between each retry to set the retry strategy.

- The maximum number of retries is 10
- The maximum retry interval is 5000 ms

![](https://assets-docs.dify.ai/2024/12/2e7c6080c0875e31a074c2a9a4543797.png)

**Error Handling**

When processing information, HTTP nodes may encounter exceptional situations such as network request timeouts or request limits. Application developers can follow these steps to configure fail branches, enabling contingency plans when nodes encounter exceptions and avoiding workflow interruptions.

1. Enable "Error Handling" in the HTTP node
2. Select and configure an error handling strategy

For more information about exception handling approaches, please refer to [Error Handling](https://docs.dify.ai/zh-hans/guides/workflow/error-handling).

![](https://assets-docs.dify.ai/2024/12/91daa86d9770390ab2a41d6d0b6ed1e7.png)
```

## File: en/guides/workflow/node/ifelse.md
```markdown
# Conditional Branch IF/ELSE

### Definition

Allows you to split the workflow into two branches based on if/else conditions.

A conditional branching node has three parts:

* IF Condition: Select a variable, set the condition, and specify the value that satisfies the condition.
* IF condition evaluates to `True`, execute the IF path.
* IF condition evaluates to `False`, execute the ELSE path.
* If the ELIF condition evaluates to `True`, execute the ELIF path;
* If the ELIF condition evaluates to `False`, continue to evaluate the next ELIF path or execute the final ELSE path.

**Condition Types**

* Contains
* Not contains
* Starts with
* Ends with
* Is
* Is not
* Is empty
* Is not empty

***

### Scenario

<figure><img src="../../../.gitbook/assets/if-else-elif.png" alt=""><figcaption></figcaption></figure>

Taking the above **Text Summary Workflow** as an example:

* IF Condition: Select the `summarystyle` variable from the start node, with the condition **Contains** `technical`.
* IF condition evaluates to `True`, follow the IF path by querying technology-related knowledge through the knowledge retrieval node, then respond via the LLM node (as shown in the upper half of the diagram);
* IF condition evaluates to `False`, but an `ELIF` condition is added, where the input for the `summarystyle` variable **does not include** `technology`, yet the `ELIF` condition includes `science`, check if the condition in `ELIF` is `True`, then execute the steps defined within that path;
* If the condition within `ELIF` is `False`, meaning the input variable contains neither `technology` nor `science`, continue to evaluate the next `ELIF` condition or execute the final `ELSE` path;
* IF condition evaluates to `False`, i.e., the `summarystyle` variable input **does not contain** `technical`, execute the ELSE path, responding via the LLM2 node (lower part of the diagram).

**Multiple Condition Judgments**

For complex condition judgments, you can set multiple condition judgments and configure **AND** or **OR** between conditions to take the **intersection** or **union** of the conditions, respectively.

<figure><img src="../../../.gitbook/assets/mutliple-judgement (1).png" alt="" width="369"><figcaption><p>Multiple Condition Judgments</p></figcaption></figure>
```

## File: en/guides/workflow/node/iteration.md
```markdown
# Iteration

### Definition

Sequentially performs the same operations on array elements until all results are outputted, functioning as a task batch processor. Iteration nodes typically work in conjunction with array variables.

For example, when processing long text translations, inputting all content directly into an LLM node may reach the single conversation limit. To address the issue, upstream nodes first split the long text into multiple chunks, then use iteration nodes to perform batch translations, thus avoiding the message limit of a single LLM conversation.

***

### Functional Description

Using iteration nodes requires input values to be formatted as list objects. The node sequentially processes all elements in the array variable from the iteration start node, applying identical processing steps to each element. Each processing cycle is called an iteration, culminating in the final output.

An iteration node consists of three core components: **Input Variables**, **Iteration Workflow**, and **Output Variables**.

**Input Variables:** Accepts only Array type data.

**Iteration Workflow:** Supports multiple workflow nodes to orchestrate task sequences within the iteration node.

**Output Variables:** Outputs only array variables (`Array[List]`). 

<figure><img src="https://assets-docs.dify.ai/2024/12/7c94bccbb6f8dc4570c69c2bf02ec6d3.png" alt=""><figcaption><p>Iteration Node Functional Description</p></figcaption></figure>

### Scenarios

#### **Example 1: Long Article Iteration Generator**

<figure><img src="../../../.gitbook/assets/long-article-iteration-generator.png" alt=""><figcaption><p>Long Story Generator</p></figcaption></figure>

1. Enter the story title and outline in the **Start Node**.
2. Use a **Generate Subtitles and Outlines Node** to use LLM to generate the complete content from user input.
3. Use a **Extract Subtitles and Outlines Node** to convert the complete content into an array format.
4. Use an **Iteration Node** to wrap an **LLM Node** and generate content for each chapter through multiple iterations.
5. Add a **Direct Answer Node** inside the iteration node to achieve streaming output after each iteration.

**Detailed Configuration Steps**

1. Configure the story title (title) and outline (outline) in the **Start Node**.

<figure><img src="../../../.gitbook/assets/workflow-start-node.png" alt="" width="375"><figcaption><p>Start Node Configuration</p></figcaption></figure>

2. Use a **Generate Subtitles and Outlines Node** to convert the story title and outline into complete text.

<figure><img src="../../../.gitbook/assets/workflow-generate-subtitles-node.png" alt="" width="375"><figcaption><p>Template Node</p></figcaption></figure>

3. Use a **Extract Subtitles and Outlines Node** to convert the story text into an array (Array) structure. The parameter to extract is `sections`, and the parameter type is `Array[Object]`.

<figure><img src="../../../.gitbook/assets/workflow-extract-subtitles-and-outlines.png" alt="" width="375"><figcaption><p>Parameter Extraction</p></figcaption></figure>

{% hint style="info" %}
The effectiveness of parameter extraction is influenced by the model's inference capability and the instructions given. Using a model with stronger inference capabilities and adding examples in the **instructions** can improve the parameter extraction results.
{% endhint %}

4. Use the array-formatted story outline as the input for the iteration node and process it within the iteration node using an **LLM Node**.

<figure><img src="../../../.gitbook/assets/workflow-iteration-node.png" alt="" width="375"><figcaption><p>Configure Iteration Node</p></figcaption></figure>

Configure the input variables `GenerateOverallOutline/output` and `Iteration/item` in the LLM Node.

<figure><img src="../../../.gitbook/assets/workflow-iteration-llm-node.png" alt="" width="375"><figcaption><p>Configure LLM Node</p></figcaption></figure>

{% hint style="info" %}
Built-in variables for iteration: `items[object]` and `index[number]`.

`items[object]` represents the input item for each iteration;

`index[number]` represents the current iteration round;
{% endhint %}

5. Configure a **Direct Reply Node** inside the iteration node to achieve streaming output after each iteration.

<figure><img src="../../../.gitbook/assets/workflow-configure-anwer-node.png" alt="" width="375"><figcaption><p>Configure Answer Node</p></figcaption></figure>

6. Complete debugging and preview.

<figure><img src="../../../.gitbook/assets/iteration-node-iteration-through-story-chapters.png" alt=""><figcaption><p>Generate by Iterating Through Story Chapters</p></figcaption></figure>

#### **Example 2: Long Article Iteration Generator (Another Arrangement)**

<figure><img src="../../../.gitbook/assets/iteration-node-iteration-long-article-iteration-generator.png" alt=""><figcaption></figcaption></figure>

* Enter the story title and outline in the **Start Node**.
* Use an **LLM Node** to generate subheadings and corresponding content for the article.
* Use a **Code Node** to convert the complete content into an array format.
* Use an **Iteration Node** to wrap an **LLM Node** and generate content for each chapter through multiple iterations.
* Use a **Template Conversion** Node to convert the string array output from the iteration node back to a string.
* Finally, add a **Direct Reply Node** to directly output the converted string.

***

### Advanced Feature

#### Parallel Mode

The iteration node supports parallel processing, improving execution efficiency when enabled.

<figure><img src="https://assets-docs.dify.ai/2024/12/516af5e7427fce9a58fa9d9b583230d4.png" alt=""><figcaption><p>Enable parallel mode</p></figcaption></figure>

Below illustrates the comparison between parallel and sequential execution in the iteration node.

<figure><img src="https://assets-docs.dify.ai/2024/12/2656dec26d6357556a280fcd69ccd9a7.png" alt=""><figcaption><p>Sequential and Parallel Processing Flow Diagram</p></figcaption></figure>

Parallel mode supports up to 10 concurrent iterations. When processing more than 10 tasks, the first 10 elements execute simultaneously, with remaining tasks processed after the completion of the initial batch.

{% hint style="info" %}
Avoid placing Direct Answer, Variable Assignment, or Tool nodes within the iteration node to prevent potential errors.
{% endhint %}

* **Error response method**

Iteration nodes process multiple tasks and may encounter errors during element processing. To prevent a single error from interrupting all tasks, configure the **Error Response Method**:

* **Terminated**: Terminates the iteration node and outputs error messages when an exception is detected.
* **Continue on error**: Ignores error messages and continues processing remaining elements. The output contains successful results with null values for errors.
* **Remove abnormal output**: Ignores error messages and continues processing remaining elements. The output contains only successful results.

Input and output variables maintain a one-to-one correspondence. For example:

* Input: \[1, 2, 3]
* Output: \[result-1, result-2, result-3]

Error handling examples:

* With **Continue on error**: \[result-1, null, result-3]
* With **Remove abnormal output**: \[result-1, result-3]

***

### Reference

#### How to Obtain Array-Formatted Content

Array variables can be generated via the following nodes as iteration node inputs:

* [Code Node](code.md)

<figure><img src="../../../.gitbook/assets/workflow-extract-subtitles-and-outlines.png" alt="" width="375"><figcaption><p>Parameter Extraction</p></figcaption></figure>

* [Parameter Extraction](parameter-extractor.md)

<figure><img src="../../../.gitbook/assets/workflow-parameter-extraction-node.png" alt="" width="375"><figcaption><p>Parameter Extraction</p></figcaption></figure>

* [Knowledge Base Retrieval](knowledge-retrieval.md)
* [Iteration](iteration.md)
* [Tools](tools.md)
* [HTTP Request](http-request.md)

***

#### How to Convert an Array to Text

The output variable of the iteration node is in array format and cannot be directly output. You can use a simple step to convert the array back to text.

**Convert Using a Code Node**

<figure><img src="../../../.gitbook/assets/iteration-code-node-convert.png" alt="" width="334"><figcaption><p>Code Node Conversion</p></figcaption></figure>

CODE Example:

```python
def main(articleSections: list):
    data = articleSections
    return {
        "result": "/n".join(data)
    }
```

**Convert Using a Template Node**

<figure><img src="../../../.gitbook/assets/workflow-template-node.png" alt="" width="332"><figcaption><p>Template Node Conversion</p></figcaption></figure>

CODE Example:

```django
{{ articleSections | join("/n") }}
```
```

## File: en/guides/workflow/node/knowledge-retrieval.md
```markdown
# Knowledge Retrieval

The Knowledge Base Retrieval Node is designed to query text content related to user questions from the Dify Knowledge Base, which can then be used as context for subsequent answers by the Large Language Model (LLM).

<figure><img src="../../../.gitbook/assets/knowledge-retrieval.png" alt=""><figcaption></figcaption></figure>

Configuring the Knowledge Base Retrieval Node involves three main steps:

1. **Selecting the Query Variable**
2. **Choosing the Knowledge Base for Query**
3. **Configuring the Retrieval Strategy**

**Selecting the Query Variable**

In knowledge base retrieval scenarios, the query variable typically represents the user's input question. In the "Start" node of conversational applications, the system pre-sets "sys.query" as the user input variable. This variable can be used to query the knowledge base for text chunks most closely related to the user's question. The maximum query content sent to the knowledge base is 200 characters.

**Choosing the Knowledge Base for Query**

Within the knowledge base retrieval node, you can add an existing knowledge base from Dify. For instructions on creating a knowledge base within Dify, please refer to the knowledge base [help documentation](https://docs.dify.ai/guides/knowledge-base/create-knowledge-and-upload-documents).

**Configuring the Retrieval Strategy**

It's possible to modify the indexing strategy and retrieval mode for an individual knowledge base within the node. For a detailed explanation of these settings, refer to the knowledge base [help documentation](https://docs.dify.ai/guides/knowledge-base/retrieval-test-and-citation).

<figure><img src="../../../.gitbook/assets/knowledge-retrieval-1.png" alt=""><figcaption></figcaption></figure>

Dify offers two recall strategies for different knowledge base retrieval scenarios: "N-to-1 Recall" and "Multi-way Recall". In the N-to-1 mode, knowledge base queries are executed through function calling, requiring the selection of a system reasoning model. In the multi-way recall mode, a Rerank model needs to be configured for result re-ranking. For a detailed explanation of these two recall strategies, refer to the retrieval mode explanation in the [help documentation](https://docs.dify.ai/guides/knowledge-base/create-knowledge-and-upload-documents#id-5-indexing-methods).

<figure><img src="../../../.gitbook/assets/knowledge-retrieval-2.png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/node/list-operator.md
```markdown
# List Operator

File list variables support simultaneous uploading of multiple file types such as document files, images, audio, and video files. When application users upload files, all files are stored in the same `Array[File]` array-type variable, which **is not conducive to subsequent individual file processing.**

> The `Array` data type means that the actual value of the variable could be \[1.mp3, 2.png, 3.doc]. LLMs only support reading single values such as image files or text content as input variables and cannot directly read array variables.

#### Node Functionality

The list operator can filter and extract attributes such as file format type, file name, and size, passing different format files to corresponding processing nodes to achieve precise control over different file processing flows.

For example, in an application that allows users to upload both document files and image files simultaneously, different files need to be sorted through the **list operation node**, with different files being handled by different processes.

<figure><img src="../../../.gitbook/assets/image (123).png" alt=""><figcaption></figcaption></figure>

List operation nodes are generally used to extract information from array variables, converting them into variable types that can be accepted by downstream nodes through setting conditions. Its structure is divided into input variables, filter conditions, sorting, taking the first N items, and output variables.

<figure><img src="../../../.gitbook/assets/image (132).png" alt=""><figcaption></figcaption></figure>

**Input Variables**

The list operation node only accepts variables with the following data structures:

* Array\[string]
* Array\[number]
* Array\[file]

**Filter Conditions**

Process arrays in input variables by adding filter conditions. Sort out all array variables that meet the conditions from the array, which can be understood as filtering the attributes of variables.

Example: Files may contain multiple dimensions of attributes, such as file name, file type, file size, etc. Filter conditions allow setting screening conditions to select and extract specific files from array variables.

Supports extracting the following variables:

* type: File category, including image, document, audio, and video types
* size: File size
* name: File name
* url: Refers to files uploaded by application users via URL, can fill in the complete URL for filtering
* extension: File extension
* mime\_type: [MIME types](https://datatracker.ietf.org/doc/html/rfc2046) are standardized strings used to identify file content types. Example: "text/html" indicates an HTML document.
* transfer\_method: File upload method, divided into local upload or upload via URL

**Sorting**

Provides the ability to sort arrays in input variables, supporting sorting based on file attributes.

* Ascending order(ASC): Default sorting option, sorted from small to large. For letters and text, sorted in alphabetical order (A - Z)
* Descending order(DESC): Sorted from large to small, for letters and text, sorted in reverse alphabetical order (Z - A)

This option is often used in conjunction with first\_record and last\_record in output variables.

**Take First N Items**

You can choose a value between 1-20, used to select the first n items of the array variable.

**Output Variables**

Array elements that meet all filter conditions. Filter conditions, sorting, and limitations can be enabled separately. If enabled simultaneously, array elements that meet all conditions are returned.

* Result: Filtering result, data type is array variable. If the array contains only 1 file, the output variable contains only 1 array element;
* first\_record: The first element of the filtered array, i.e., result\[0];
* last\_record: The last element of the filtered array, i.e., result\[array.length-1].

***

#### Configuration Example

In file interaction Q\&A scenarios, application users may upload document files or image files simultaneously. LLMs only support the ability to recognize image files and do not support reading document files. At this time, the List Operation node is needed to preprocess the array of file variables and send different file types to corresponding processing nodes. The orchestration steps are as follows:

1. Enable the [Features](../additional-features.md) function and check both "Images" and "Document" types in the file types.
2. Add two list operation nodes, setting to extract image and document variables respectively in the "List Operator" conditions.
3. Extract document file variables and pass them to the "Doc Extractor" node; extract image file variables and pass them to the "LLM" node.
4. Add a "Answer" node at the end, filling in the output variable of the LLM node.

<figure><img src="../../../.gitbook/assets/image (133).png" alt=""><figcaption></figcaption></figure>

After the application user uploads both document files and images, document files are automatically diverted to the doc extractor node, and image files are automatically diverted to the LLM node to achieve joint processing of mixed files.
```

## File: en/guides/workflow/node/llm.md
```markdown
# LLM

### Definition

Invokes the capabilities of large language models to process information input by users in the "Start" node (natural language, uploaded files, or images) and provide effective response information.

<figure><img src="../../../.gitbook/assets/llm-node-1.png" alt=""><figcaption><p>LLM Node</p></figcaption></figure>

***

### Scenarios

LLM is the core node of Chatflow/Workflow, utilizing the conversational/generative/classification/processing capabilities of large language models to handle a wide range of tasks based on given prompts and can be used in different stages of workflows.

* **Intent Recognition**: In customer service scenarios, identifying and classifying user inquiries to guide downstream processes.
* **Text Generation**: In content creation scenarios, generating relevant text based on themes and keywords.
* **Content Classification**: In email batch processing scenarios, automatically categorizing emails, such as inquiries/complaints/spam.
* **Text Conversion**: In translation scenarios, translating user-provided text into a specified language.
* **Code Generation**: In programming assistance scenarios, generating specific business code or writing test cases based on user requirements.
* **RAG**: In knowledge base Q\&A scenarios, reorganizing retrieved relevant knowledge to respond to user questions.
* **Image Understanding**: Using multimodal models with vision capabilities to understand and answer questions about the information within images.
* **File Analysis**: In file processing scenarios, use LLMs to recognize and analyze the information contained within files.

By selecting the appropriate model and writing prompts, you can build powerful and reliable solutions within Chatflow/Workflow.

***

### How to Configure

<figure><img src="../../../.gitbook/assets/llm-node-2.png" alt=""><figcaption><p>LLM Node Configuration - Model Selection</p></figcaption></figure>

**Configuration Steps:**

1. **Select a Model**: Dify supports major global models, including OpenAI's GPT series, Anthropic's Claude series, and Google's Gemini series. Choosing a model depends on its inference capability, cost, response speed, context window, etc. You need to select a suitable model based on the scenario requirements and task type.
2. **Configure Model Parameters**: Model parameters control the generation results, such as temperature, TopP, maximum tokens, response format, etc. To facilitate selection, the system provides three preset parameter sets: Creative, Balanced, and Precise.
3. **Write Prompts**: The LLM node offers an easy-to-use prompt composition page. Selecting a chat model or completion model will display different prompt composition structures.
4. **Advanced Settings**: You can enable memory, set memory windows, and use the Jinja-2 template language for more complex prompts.

{% hint style="info" %}
If you are using Dify for the first time, you need to complete the [model configuration](../../model-configuration/) in **System Settings-Model Providers** before selecting a model in the LLM node.
{% endhint %}

#### **Writing Prompts**

In the LLM node, you can customize the model input prompts. If you select a chat model, you can customize the System/User/Assistant sections.

**Prompt Generator**

If you're struggling to come up with effective system prompts (System), you can use the Prompt Generator to quickly create prompts suitable for your specific business scenarios, leveraging AI capabilities.

<figure><img src="../../../.gitbook/assets/en-prompt-generator.png" alt="" width="563"><figcaption></figcaption></figure>

In the prompt editor, you can call out the **variable insertion menu** by typing `/` or `{` to insert **special variable blocks** or **upstream node variables** into the prompt as context content.

<figure><img src="../../../.gitbook/assets/llm-node-3.png" alt="" width="366"><figcaption><p>Calling Out the Variable Insertion Menu</p></figcaption></figure>

***

### Explanation of Special Variables

**Context Variables**

Context variables are a special type of variable defined within the LLM node, used to insert externally retrieved text content into the prompt.

<figure><img src="../../../.gitbook/assets/llm-node-4.png" alt=""><figcaption><p>Context Variables</p></figcaption></figure>

In common knowledge base Q\&A applications, the downstream node of knowledge retrieval is typically the LLM node. The **output variable** `result` of knowledge retrieval needs to be configured in the **context variable** within the LLM node for association and assignment. After association, inserting the **context variable** at the appropriate position in the prompt can incorporate the externally retrieved knowledge into the prompt.

This variable can be used not only as external knowledge introduced into the prompt context for LLM responses but also supports the application's [**citation and attribution**](../../knowledge-base/retrieval-test-and-citation#id-2.-citation-and-attribution) feature due to its data structure containing segment reference information.

{% hint style="info" %}
If the context variable is associated with a common variable from an upstream node, such as a string type variable from the start node, the context variable can still be used as external knowledge, but the **citation and attribution** feature will be disabled.
{% endhint %}

**File Variables**

Some LLMs, such as [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support), now support direct processing of file content, enabling the use of file variables in prompts. To prevent potential issues, application developers should verify the supported file types on the LLM's official website before utilizing the file variable.

![](https://assets-docs.dify.ai/2024/11/05b3d4a78038bc7afbb157078e3b2b26.png)

> Refer to [File Upload](https://docs.dify.ai/guides/workflow/file-upload) for guidance on building a Chatflow/Workflow application with file upload functionality.

**Conversation History**

To achieve conversational memory in text completion models (e.g., gpt-3.5-turbo-Instruct), Dify designed the conversation history variable in the original [Prompt Expert Mode (discontinued)](../../../learn-more/extended-reading/prompt-engineering/prompt-engineering-1/). This variable is carried over to the LLM node in Chatflow, used to insert chat history between the AI and the user into the prompt, helping the LLM understand the context of the conversation.

{% hint style="info" %}
The conversation history variable is not widely used and can only be inserted when selecting text completion models in Chatflow.
{% endhint %}

<figure><img src="../../../.gitbook/assets/image (204).png" alt=""><figcaption><p>Inserting Conversation History Variable</p></figcaption></figure>

**Model Parameters**

The parameters of the model affect the output of the model. Different models have different parameters. The following figure shows the parameter list for `gpt-4`.

<figure><img src="../../../.gitbook/assets/截屏2024-10-18 11.35.17.png" alt="" width="363"><figcaption></figcaption></figure>

The main parameter terms are explained as follows:

**Temperature**: Usually a value between 0-1, it controls randomness. The closer the temperature is to 0, the more certain and repetitive the results; the closer it is to 1, the more random the results.

**Top P**: Controls the diversity of the results. The model selects from candidate words based on probability, ensuring that the cumulative probability does not exceed the preset threshold P.

**Presence Penalty**: Used to reduce the repetitive generation of the same entity or information by imposing penalties on content that has already been generated, making the model inclined to generate new or different content. As the parameter value increases, greater penalties are applied in subsequent generations to content that has already been generated, lowering the likelihood of repeating content.

**Frequency Penalty**: Imposes penalties on words or phrases that appear too frequently by reducing their probability of generation. With an increase in parameter value, greater penalties are imposed on frequently occurring words or phrases. Higher parameter values reduce the frequency of these words, thereby increasing the lexical diversity of the text.

If you do not understand what these parameters are, you can choose to load presets and select from the three presets: Creative, Balanced, and Precise.

<figure><img src="../../../.gitbook/assets/截屏2024-10-18 11.37.43.png" alt="" width="364"><figcaption></figcaption></figure>

***

### Advanced Features

**Memory**: When enabled, each input to the intent classifier will include chat history from the conversation to help the LLM understand the context and improve question comprehension in interactive dialogues.

**Memory Window**: When the memory window is closed, the system dynamically filters the amount of chat history passed based on the model's context window; when open, users can precisely control the amount of chat history passed (in terms of numbers).

**Conversation Role Name Settings**: Due to differences in model training stages, different models adhere to role name instructions differently, such as Human/Assistant, Human/AI, Human/Assistant, etc. To adapt to the prompt response effects of multiple models, the system provides conversation role name settings. Modifying the role name will change the role prefix in the conversation history.

**Jinja-2 Templates**: The LLM prompt editor supports Jinja-2 template language, allowing you to leverage this powerful Python template language for lightweight data transformation and logical processing. Refer to the [official documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/).

**Retry on Failure**: For some exceptions that occur in the node, it is usually sufficient to retry the node again. When the error retry function is enabled, the node will automatically retry according to the preset strategy when an error occurs. You can adjust the maximum number of retries and the interval between each retry to set the retry strategy.

- The maximum number of retries is 10
- The maximum retry interval is 5000 ms

![](https://assets-docs.dify.ai/2024/12/dfb43c1cbbf02cdd36f7d20973a5529b.png)

**Error Handling**: Provides diverse node error handling strategies that can throw error messages when the current node fails without interrupting the main process, or continue completing tasks through backup paths. For detailed information, please refer to the [Error Handling](https://docs.dify.ai/guides/workflow/error-handling).

***

#### Use Cases

* **Reading Knowledge Base Content**

To enable workflow applications to read "[Knowledge Base](../../knowledge-base/)" content, such as building an intelligent customer service application, please follow these steps:

1. Add a knowledge base retrieval node upstream of the LLM node;
2. Fill in the **output variable** `result` of the knowledge retrieval node into the **context variable** of the LLM node;
3. Insert the **context variable** into the application prompt to give the LLM the ability to read text within the knowledge base.

<figure><img src="../../../.gitbook/assets/image (135).png" alt=""><figcaption></figcaption></figure>

The `result` variable output by the Knowledge Retrieval Node also includes segmented reference information. You can view the source of information through the **Citation and Attribution** feature.

{% hint style="info" %}
Regular variables from upstream nodes can also be filled into context variables, such as string-type variables from the start node, but the **Citation and Attribution** feature will be ineffective.
{% endhint %}

* **Reading Document Files**

To enable workflow applications to read document contents, such as building a ChatPDF application, you can follow these steps:

* Add a file variable in the "Start" node;
* Add a document extractor node upstream of the LLM node, using the file variable as an input variable;
* Fill in the **output variable** `text` of the document extractor node into the prompt of the LLM node.

For more information, please refer to [File Upload](../file-upload.md).

<figure><img src="../../../.gitbook/assets/image (137).png" alt=""><figcaption><p>input system prompts</p></figcaption></figure>

* **Error Handling**

When processing information, LLM nodes may encounter errors such as input text exceeding token limits or missing key parameters. Developers can follow these steps to configure exception branches, enabling contingency plans when node errors occur to avoid interrupting the entire flow:

1. Enable "Error Handling" in the LLM node
2. Select and configure an error handling strategy

<figure><img src="https://assets-docs.dify.ai/2024/12/f7109ce5e87c0e0a81248bb2672c7667.png" alt=""><figcaption><p>input system prompts</p></figcaption></figure>

For more information about exception handling methods, please refer to the [Error Handling](https://docs.dify.ai/guides/workflow/error-handling).
```

## File: en/guides/workflow/node/parameter-extractor.md
```markdown
# Parameter Extraction

### 1 Definition

Utilize LLM to infer and extract structured parameters from natural language for subsequent tool invocation or HTTP requests.

Dify workflows provide a rich selection of [tools](../../tools.md), most of which require structured parameters as input. The parameter extractor can convert user natural language into parameters recognizable by these tools, facilitating tool invocation.

Some nodes within the workflow require specific data formats as inputs, such as the [iteration](iteration.md#definition) node, which requires an array format. The parameter extractor can conveniently achieve [structured parameter conversion](iteration.md#example-1-long-article-iteration-generator).

***

### 2 Scenarios

1. **Extracting key parameters required by tools from natural language**, such as building a simple conversational Arxiv paper retrieval application.

In this example: The Arxiv paper retrieval tool requires **paper author** or **paper ID** as input parameters. The parameter extractor extracts the paper ID **2405.10739** from the query "What is the content of this paper: 2405.10739" and uses it as the tool parameter for precise querying.

<figure><img src="../../../.gitbook/assets/precise-query.png" alt=""><figcaption><p>Arxiv Paper Retrieval Tool</p></figcaption></figure>

2. **Converting text to structured data**, such as in the long story iteration generation application, where it serves as a pre-step for the [iteration node](iteration.md), converting chapter content in text format to an array format, facilitating multi-round generation processing by the iteration node.

<figure><img src="../../../.gitbook/assets/convert-chapter-content.png" alt=""><figcaption></figcaption></figure>

1. **Extracting structured data and using the** [**HTTP Request**](https://docs.dify.ai/guides/workflow/node/http-request), which can request any accessible URL, suitable for obtaining external retrieval results, webhooks, generating images, and other scenarios.

***

### 3 How to Configure

**Configuration Steps**

1. Select the input variable, usually the variable input for parameter extraction.
2. Choose the model, as the parameter extractor relies on the LLM's inference and structured generation capabilities.
3. Define the parameters to extract, which can be manually added or **quickly imported from existing tools**.
4. Write instructions, where providing examples can help the LLM improve the effectiveness and stability of extracting complex parameters.

**Advanced Settings**

**Inference Mode**

Some models support two inference modes, achieving parameter extraction through function/tool calls or pure prompt methods, with differences in instruction compliance. For instance, some models may perform better in prompt inference if function calling is less effective.

* Function Call/Tool Call
* Prompt

**Memory**

When memory is enabled, each input to the question classifier will include the chat history in the conversation to help the LLM understand the context and improve question comprehension during interactive dialogues.

**Output Variables**

* Extracted defined variables
* Node built-in variables

`__is_success Number` Extraction success status, with a value of 1 for success and 0 for failure.

`__reason String` Extraction error reason
```

## File: en/guides/workflow/node/question-classifier.md
```markdown
# Question Classifier

### 1. Definition

By defining classification descriptions, the issue classifier can infer and match user inputs to the corresponding categories and output the classification results.

***

### 2. Scenarios

Common use cases include **customer service conversation intent classification, product review classification, and bulk email classification**.

In a typical product customer service Q\&A scenario, the issue classifier can serve as a preliminary step before knowledge base retrieval. It classifies the user's input question, directing it to different downstream knowledge base queries to accurately respond to the user's question.

The following diagram is an example workflow template for a product customer service scenario:

<figure><img src="../../../.gitbook/assets/question-classifier-scenarios.png" alt=""><figcaption></figcaption></figure>

In this scenario, we set up three classification labels/descriptions:

* Category 1: **Questions related to after-sales service**
* Category 2: **Questions related to product usage**
* Category 3: **Other questions**

When users input different questions, the issue classifier will automatically classify them based on the set classification labels/descriptions:

* "**How to set up contacts on iPhone 14?**" —> "**Questions related to product usage**"
* "**What is the warranty period?**" —> "**Questions related to after-sales service**"
* "**How's the weather today?**" —> "**Other questions**"

***

### 3. How to Configure

<figure><img src="../../../.gitbook/assets/question-classifier-1.png" alt=""><figcaption></figcaption></figure>

**Configuration Steps:**

1. **Select Input Variable**: This refers to the content to be classified, usually the user's question in a customer service Q\&A scenario, e.g., `sys.query`.
2. **Choose Inference Model**: The issue classifier leverages the natural language classification and inference capabilities of large language models. Selecting an appropriate model can enhance classification effectiveness.
3. **Write Classification Labels/Descriptions**: You can manually add multiple classifications by writing keywords or descriptive statements for each category, helping the large language model better understand the classification criteria.
4. **Choose Corresponding Downstream Nodes**: After classification, the issue classification node can direct the flow to different paths based on the relationship between the classification and downstream nodes.

#### Advanced Settings:

**Instructions**: In **Advanced Settings - Instructions**, you can add supplementary instructions, such as more detailed classification criteria, to enhance the classifier's capabilities.

**Memory**: When enabled, each input to the issue classifier will include chat history from the conversation to help the LLM understand the context and improve question comprehension in interactive dialogues.

**Memory Window**: When the memory window is closed, the system dynamically filters the amount of chat history passed based on the model's context window; when open, users can precisely control the amount of chat history passed (in terms of numbers).

**Output Variable**:

`class_name` stores the classification output label. You can reference this classification result in downstream nodes when needed.
```

## File: en/guides/workflow/node/README.md
```markdown
# Node Description

**Nodes are the key components of a workflow**, enabling the execution of a series of operations by connecting nodes with different functionalities.

### Core Nodes

<table data-view="cards"><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td><a href="start.md"><strong>Start</strong></a></td><td>Defines the initial parameters for starting a workflow process.</td><td></td></tr><tr><td><a href="end.md"><strong>End</strong></a></td><td>Defines the final output content for ending a workflow process.</td><td></td></tr><tr><td><a href="answer.md"><strong>Answer</strong></a></td><td>Defines the response content in a Chatflow process.</td><td></td></tr><tr><td><a href="llm.md"><strong>Large Language Model (LLM)</strong></a></td><td>Calls a large language model to answer questions or process natural language.</td><td></td></tr><tr><td><a href="knowledge_retrieval.md"><strong>Knowledge Retrieval</strong></a></td><td>Retrieves text content related to user questions from a knowledge base, which can serve as context for downstream LLM nodes.</td><td></td></tr><tr><td><a href="question-classifier.md"><strong>Question Classifier</strong></a></td><td>By defining classification descriptions, the LLM can select the matching classification based on user input.</td><td></td></tr><tr><td><a href="ifelse.md"><strong>IF/ELSE</strong></a></td><td>Allows you to split the workflow into two branches based on if/else conditions.</td><td></td></tr><tr><td><a href="code.md"><strong>Code Execution</strong></a></td><td>Runs Python/NodeJS code to execute custom logic such as data transformation within the workflow.</td><td></td></tr><tr><td><a href="template.md"><strong>Template</strong></a></td><td>Enables flexible data transformation and text processing using Jinja2, a Python templating language.</td><td></td></tr><tr><td><a href="variable-aggregator.md"><strong>Variable Aggregator</strong></a></td><td>Aggregates variables from multiple branches into one variable for unified configuration of downstream nodes.</td><td></td></tr><tr><td><a href="variable-assigner.md"><strong>Variable Assigner</strong></a></td><td>The variable assigner node is used to assign values to writable variables.</td><td></td></tr><tr><td><a href="parameter-extractor.md"><strong>Parameter Extractor</strong></a></td><td>Uses LLM to infer and extract structured parameters from natural language for subsequent tool calls or HTTP requests.</td><td></td></tr><tr><td><a href="iteration.md"><strong>Iteration</strong></a></td><td>Executes multiple steps on list objects until all results are output.</td><td></td></tr><tr><td><a href="http-request.md"><strong>HTTP Request</strong></a></td><td>Allows sending server requests via the HTTP protocol, suitable for retrieving external results, webhooks, generating images, and other scenarios.</td><td></td></tr><tr><td><a href="tools.md"><strong>Tools</strong></a></td><td>Enables calling built-in Dify tools, custom tools, sub-workflows, and more within the workflow.</td><td></td></tr></tbody></table>
```

## File: en/guides/workflow/node/start.md
```markdown
# Start

#### Definition

The **“Start”** node is a critical preset node in the Chatflow / Workflow application. It provides essential initial information, such as user input and [uploaded files](../file-upload.md), to support the normal flow of the application and subsequent workflow nodes.

#### Configuring the Node

On the Start node's settings page, you'll find two sections: **"Input Fields"** and preset **System Variables**.

<figure><img src="../../../.gitbook/assets/image (3).png" alt=""><figcaption><p>Chatflow and Workflow</p></figcaption></figure>

#### Input Field

Input field is configured by application developers to prompt users for additional information.

For example, in a weekly report application, users might be required to provide background information such as name, work date range, and work details in a specific format. This preliminary information helps the LLM generate higher quality responses.

Six types of input variables are supported, all of which can be set as required:

* **Text:** Short text, filled in by the user, with a maximum length of 256 characters.
* **Paragraph:** Long text, allowing users to input longer content.
* **Select:** Fixed options set by the developer; users can only select from preset options and cannot input custom content.
* **Number:** Only allows numerical input.
* **Single File:** Allows users to upload a single file. Supports document types, images, audio, video, and other file types. Users can upload locally or paste a file URL. For detailed usage, refer to File Upload.
* **File List:** Allows users to batch upload files. Supports document types, images, audio, video, and other file types. Users can upload locally or paste file URLs. For detailed usage, refer to File Upload.

{% hint style="info" %}
Dify's built-in document extractor node can only process certain document formats. For processing images, audio, or video files, refer to External Data Tools to set up corresponding file processing nodes.
{% endhint %}

Once configured, users will be guided to provide necessary information to the LLM before using the application. More information will help to improve the LLM's question-answering efficiency.

#### System Variables

System variables are preset system-level parameters in Chatflow / Workflow applications that can be globally accessed by other nodes in the application. They are typically used in advanced development scenarios, such as building multi-turn dialogue applications, collecting application logs and monitoring data, or recording usage behavior across different applications and users.

**Workflow**

Workflow application provides the following system variables:

<table><thead><tr><th width="193">Variable Name</th><th width="116">Data Type</th><th width="278">Description</th><th>Notes</th></tr></thead><tbody><tr><td><code>sys.files</code><br>[LEGACY]</td><td>Array[File]</td><td>File parameter, stores images uploaded by users when initially using the application</td><td>Image upload feature needs to be enabled in the "Features" section at the top right of the application orchestration page</td></tr><tr><td><code>sys.user_id</code></td><td>String</td><td>User ID, a unique identifier automatically assigned to each user when using the workflow application, used to distinguish different conversation users</td><td></td></tr><tr><td><code>sys.app_id</code></td><td>String</td><td>Application ID, a unique identifier assigned to each Workflow application by the system, used to distinguish different applications and record basic information of the current application</td><td>For users with development capabilities, this parameter can be used to differentiate and locate different Workflow applications</td></tr><tr><td><code>sys.workflow_id</code></td><td>String</td><td>Workflow ID, used to record all node information contained in the current Workflow application</td><td>For users with development capabilities, this parameter can be used to track and record node information within the Workflow</td></tr><tr><td><code>sys.workflow_run_id</code></td><td>String</td><td>Workflow application run ID, used to record the running status of the Workflow application</td><td>For users with development capabilities, this parameter can be used to track the application's run history</td></tr></tbody></table>

**Chatflow**

Chatflow application provides the following system variables:

<table><thead><tr><th>Variable Name</th><th width="127">Data Type</th><th width="283">Description</th><th>Notes</th></tr></thead><tbody><tr><td><code>sys.query</code></td><td>String</td><td>The initial content input by the user in the dialogue box</td><td></td></tr><tr><td><code>sys.files</code></td><td>Array[File]</td><td>Images uploaded by the user in the dialogue box</td><td>Image upload feature needs to be enabled in the "Features" section at the top right of the application orchestration page</td></tr><tr><td><code>sys.dialogue_count</code></td><td>Number</td><td>The number of dialogue turns during user interaction with the Chatflow application. Automatically increments by 1 after each turn. Can be used with if-else nodes to create rich branching logic. For example, at the Xth turn of dialogue, review the conversation history and provide analysis</td><td></td></tr><tr><td><code>sys.conversation_id</code></td><td>String</td><td>Unique identifier for the dialogue interaction session, grouping all related messages into the same conversation, ensuring the LLM continues the dialogue on the same topic and context</td><td></td></tr><tr><td><code>sys.user_id</code></td><td>String</td><td>Unique identifier assigned to each application user, used to distinguish different conversation users</td><td></td></tr><tr><td><code>sys.app_id</code></td><td>String</td><td>Application ID, a unique identifier assigned to each Workflow application by the system, used to distinguish different applications and record basic information of the current application</td><td>For users with development capabilities, this parameter can be used to differentiate and locate different Workflow applications</td></tr><tr><td><code>sys.workflow_id</code></td><td>String</td><td>Workflow ID, used to record all node information contained in the current Workflow application</td><td>For users with development capabilities, this parameter can be used to track and record node information within the Workflow</td></tr><tr><td><code>sys.workflow_run_id</code></td><td>String</td><td>Workflow application run ID, used to record the running status of the Workflow application</td><td>For users with development capabilities, this parameter can be used to track the application's run history</td></tr></tbody></table>
```

## File: en/guides/workflow/node/template.md
```markdown
# Template

Template lets you dynamically format and combine variables from previous nodes into a single text-based output using Jinja2, a powerful templating syntax for Python. It's useful for combining data from multiple sources into a specific structure required by subsequent nodes. The simple example below shows how to assemble an article by piecing together various previous outputs:

<figure><img src="../../../.gitbook/assets/image (158).png" alt="" width="375"><figcaption></figcaption></figure>

Beyond naive use cases, you can create more complex templates as per Jinja's [documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/) for a variety of tasks. Here's one template that structures retrieved chunks and their relevant metadata from a knowledge retrieval node into a formatted markdown:

```Plain
{% raw %}
{% for item in chunks %}
### Chunk {{ loop.index }}. 
### Similarity: {{ item.metadata.score | default('N/A') }}

#### {{ item.title }}

##### Content
{{ item.content | replace('\n', '\n\n') }}

---
{% endfor %}
{% endraw %}
```

<figure><img src="../../../.gitbook/assets/image (159).png" alt=""><figcaption></figcaption></figure>

This template node can then be used within a Chatflow to return intermediate outputs to the end user, before a LLM response is initiated.

> The `Answer` node in a Chatflow is non-terminal. It can be inserted anywhere to output responses at multiple points within the flow.
```

## File: en/guides/workflow/node/tools.md
```markdown
# Tools

The workflow provides a rich selection of tools, categorized into three types:

* **Built-in Tools**: Tools provided by Dify.
* **Custom Tools**: Tools imported or configured via the OpenAPI/Swagger standard format.
* **Workflows**: Workflows that have been published as tools.

## Add and Use the Tool Node

Before using built-in tools, you may need to **authorize** the tools.

If built-in tools do not meet your needs, you can create custom tools in the **Dify menu navigation -- Tools** section.

You can also orchestrate a more complex workflow and publish it as a tool.

<figure><img src="../../../.gitbook/assets/workflow-tool.png" alt="" width="258"><figcaption><p>Tool Selection</p></figcaption></figure>

<figure><img src="../../../.gitbook/assets/workflow-google-search-tool.png" alt=""><figcaption><p>Configuring Google Search Tool to Retrieve External Knowledge</p></figcaption></figure>

Configuring a tool node generally involves two steps:

1. Authorizing the tool/creating a custom tool/publishing a workflow as a tool.
2. Configuring the tool's input and parameters.

For more information on how to create custom tools and configure them, please refer to the [Tool Configuration Guide](https://docs.dify.ai/guides/tools).

### Advanced Features

**Retry on Failure**

For some exceptions that occur in the node, it is usually sufficient to retry the node again. When the error retry function is enabled, the node will automatically retry according to the preset strategy when an error occurs. You can adjust the maximum number of retries and the interval between each retry to set the retry strategy.

- The maximum number of retries is 10
- The maximum retry interval is 5000 ms

![](https://assets-docs.dify.ai/2024/12/34867b2d910d74d2671cd40287200480.png)

**Error Handling**

Tool nodes may encounter errors during information processing that could interrupt the workflow. Developers can follow these steps to configure fail branches, enabling contingency plans when nodes encounter exceptions, avoiding workflow interruptions.

1. Enable "Error Handling" in the tool node
2. Select and configure an error-handling strategy

![](https://assets-docs.dify.ai/2024/12/39dc3b5881d9a5fe35b877971f70d3a6.png)

For more information about exception handling approaches, please refer to [Error Handling](https://docs.dify.ai/guides/workflow/error-handling).

## Publishing Workflow Applications as Tools

Workflow applications can be published as tools and used by nodes in other workflows. For information about creating custom tools and tool configuration, please refer to the [Tool Configuration Guide](https://docs.dify.ai/guides/tools).
```

## File: en/guides/workflow/node/variable-aggregator.md
```markdown
# Variable Aggregator

### 1 Definition

Aggregate variables from multiple branches into a single variable to achieve unified configuration for downstream nodes.

The variable aggregation node (formerly the variable assignment node) is a key node in the workflow. It is responsible for integrating the output results from different branches, ensuring that regardless of which branch is executed, its results can be referenced and accessed through a unified variable. This is particularly useful in multi-branch scenarios, as it maps variables with the same function from different branches into a single output variable, avoiding the need for repeated definitions in downstream nodes.

***

### 2 Scenarios

Through variable aggregation, you can aggregate multiple outputs, such as from issue classification or conditional branching, into a single output for use and manipulation by downstream nodes, simplifying data flow management.

**Multi-Branch Aggregation after Issue Classification**

Without variable aggregation, the branches of Classification 1 and Classification 2, after different knowledge base retrievals, would require repeated definitions for downstream LLM and direct response nodes.

<figure><img src="../../../.gitbook/assets/image (227).png" alt=""><figcaption><p>Issue Classification (without Variable Aggregation)</p></figcaption></figure>

By adding variable aggregation, the outputs of the two knowledge retrieval nodes can be aggregated into a single variable.

<figure><img src="../../../.gitbook/assets/variable-aggregation.png" alt=""><figcaption><p>Multi-Branch Aggregation after Issue Classification</p></figcaption></figure>

**Multi-Branch Aggregation after IF/ELSE Conditional Branching**

<figure><img src="../../../.gitbook/assets/if-else-conditional.png" alt=""><figcaption><p>Multi-Branch Aggregation after Conditional Branching</p></figcaption></figure>

### 3 Format Requirements

The variable aggregator supports aggregating various data types, including strings (`String`), numbers (`Number`), objects (`Object`), and arrays (`Array`).

**The variable aggregator can only aggregate variables of the same data type**. If the first variable added to the variable aggregation node is of the `String` data type, subsequent connections will automatically filter and allow only `String` type variables to be added.

**Aggregation Grouping**

Starting from version v0.6.10, aggregation grouping is supported.

When aggregation grouping is enabled, the variable aggregator can aggregate multiple groups of variables, with each group requiring the same data type for aggregation.
```

## File: en/guides/workflow/node/variable-assigner.md
```markdown
# Variable Assigner

### Definition

The variable assigner node is used to assign values to writable variables. Currently supported writable variables include:

* [conversation variables](https://docs.dify.ai/guides/workflow/key-concepts#conversation-variables).

Usage: Through the variable assigner node, you can assign workflow variables to conversation variables for temporary storage, which can be continuously referenced in subsequent conversations.

<figure><img src="https://assets-docs.dify.ai/2024/11/83d0b9ef4c1fad947b124398d472d656.png" alt="" width="375"><figcaption></figcaption></figure>

***

### Usage Scenario Examples

Using the variable assigner node, you can write context from the conversation process, files uploaded to the dialog box, and user preference information into conversation variables. These stored variables can then be referenced in subsequent conversations to direct different processing flows or formulate responses.

**Scenario 1**

You can write the **context during the conversation, the file uploaded to the chatting box (coming soon), the preference information entered by the user,etc.** into the conversation variable using **Variable Assigner** node. These stored information can be referenced in subsequent chats to guide different processing flows or provide responses.

**Scenario 1**

**Automatically judge and extract content**, store history in the conversation, record important user information through the session variable array within the conversation, and use this history content to personalize responses in subsequent chats.

Example: After the conversation starts, LLM will automatically determine whether the user's input contains facts, preferences, or chat history that need to be remembered. If it has, LLM will first extract and store those information, then use it as context to respond. If there is no new information to remember, LLM will directly use the previously relevant memories to answer questions.

![](../../../.gitbook/assets/conversation-variables-scenario-1.png)

**Configuration process:**

1. **Set Conversation Variables:**
   * First, set up a Conversation Variables array `memories`, of type array\[object] to store user information, preferences, and chat history.
2. **Determine and Extract Memories:**
   * Add a Conditional Branching node, using LLM to determine whether the user's input contains new information that needs to be remembered.
   * If there's new information, follow the upper branch and use an LLM node to extract this information.
   * If there's no new information, go down the branch and directly use existing memories to answer.
3. **Variable Assignment/Writing:**
   * In the upper branch, use the variable assigner node to append the newly extracted information to the `memories` array.
   * Use the escape function to convert the text string output by LLM into a format suitable for storage in an array\[object].
4. **Variable Reading and Usage:**
   * In subsequent LLM nodes, convert the contents of the `memories` array to a string and insert it into the prompt of LLM as context.
   * Use these memories to generate personalized responses.

The code for the node in the upper diagram is as follows:

1. Escape the string to object

```python
import json

def main(arg1: str) -> object:
    try:
        # Parse the input JSON string
        input_data = json.loads(arg1)
        
        # Extract the memory object
        memory = input_data.get("memory", {})
        
        # Construct the return object
        result = {
            "facts": memory.get("facts", []),
            "preferences": memory.get("preferences", []),
            "memories": memory.get("memories", [])
        }
        
        return {
            "mem": result
        }
    except json.JSONDecodeError:
        return {
            "result": "Error: Invalid JSON string"
        }
    except Exception as e:
        return {
            "result": f"Error: {str(e)}"
        }
```

2. Escape object as string

```python
import json

def main(arg1: list) -> str:
    try:
        # Assume arg1[0] is the dictionary we need to process
        context = arg1[0] if arg1 else {}
        
        # Construct the memory object
        memory = {"memory": context}
        
        # Convert the object to a JSON string
        json_str = json.dumps(memory, ensure_ascii=False, indent=2)
        
        # Wrap the JSON string in <answer> tags
        result = f"<answer>{json_str}</answer>"
        
        return {
            "result": result
        }
    except Exception as e:
        return {
            "result": f"<answer>Error: {str(e)}</answer>"
        }
```

**Scenario 2**

**Recording initial user preferences input**: Remember the user's language preference input during the conversation and continue to use this language for responses in subsequent chatting.

Example: Before the chatting, the user specifies "English" in the `language` input box. This language will be written to the conversation variable, and the LLM will reference this information when responding, continuing to use "English" in subsequent conversations.

<figure><img src="../../../.gitbook/assets/conversation-var-scenario-1.png" alt=""><figcaption></figcaption></figure>

**Configuration Guide:**

**Set conversation variable**: First, set a conversation variable `language`. Add a condition judgment node at the beginning of the conversation flow to check if the `language` variable is empty.

**Variable writing/assignment**: At the start of the first round of chatting, if the `language` variable is empty, use an LLM node to extract the user's input language, then use a variable assigner node to write this language type into the conversation variable `language`.

**Variable reading**: In subsequent conversation rounds, the `language` variable has stored the user's language preference. The LLM node references the language variable to respond using the user's preferred language type.

**Scenario 3**

**Assisting with Checklist checks**: Record user inputs within the conversation using conversation variables, update the contents of the Checklist, and check for missing items in subsequent conversations.

Example: After starting the conversation, the LLM will ask the user to input items related to the Checklist in the chatting box. Once the user mentions content from the Checklist, it will be updated and stored in the Conversation Variable. The LLM will remind the user to continue supplementing missing items after each round of dialogue.

<figure><img src="../../../.gitbook/assets/conversation-var-scenario-2-1.png" alt=""><figcaption></figcaption></figure>

**Configuration Process:**

* **Set conversation variable:** First, set a conversation variable `ai_checklist`, and reference this variable within the LLM as context for checking.
* **variable assigner/writing**: During each round of dialogue, check the value in `ai_checklist` within the LLM node and compare it with user input. If the user provides new information, update the Checklist and write the output content to `ai_checklist` using the variable assigner node.
* **Variable reading:** Read the value in `ai_checklist` and compare it with user input in each round of dialogue until all checklist items are completed.

***

### Using the Variable Assigner Node

Click the `+` icon on the right side of the node and select the **“Variable Assignment”** node. Configure the target variables and their corresponding source variables. This node allows you to assign values to multiple variables simultaneously.

<figure><img src="https://assets-docs.dify.ai/2024/11/ee15dee864107ba5a93b459ebdfc32cf.png" alt="" width="375"><figcaption></figcaption></figure>

**Setting Variables:**

Variable: Select the variable to be assigned, i.e., specify the target conversation variable that needs to be assigned.

Set Variable: Select the variable to assign, i.e., specify the source variable that needs to be converted.

The variable assignment logic illustrated in the image above assigns the user’s language preference, specified on the initial page `Start/language`, to the system-level conversation variable `language`.

### **Operation Modes for Specifying Variables**

The data type of the target variable determines its operation method. Below are the operation modes for different variable types:

1.	Target variable data type: `String`

	•	**Overwrite**: Directly overwrite the target variable with the source variable.
	•	**Clear**: Clear the contents of the selected target variable.
	•	**Set**: Manually assign a value without requiring a source variable.

2.	Target variable data type: `Number`

	•	**Overwrite**: Directly overwrite the target variable with the source variable.
	•	**Clear**: Clear the contents of the selected target variable.
	•	**Set**: Manually assign a value without requiring a source variable.
	•	**Arithmetic**: Perform addition, subtraction, multiplication, or division on the target variable.

3.	Target variable data type: `Object`

	•	**Overwrite**: Directly overwrite the target variable with the source variable.
	•	**Clear**: Clear the contents of the selected target variable.
	•	**Set**: Manually assign a value without requiring a source variable.

4.	Target variable data type: `Array`

	•	**Overwrite**: Directly overwrite the target variable with the source variable.
	•	**Clear**: Clear the contents of the selected target variable.
	•	**Append**: Add a new element to the array in the target variable.
	•	**Extend**: Add a new array to the target variable, effectively adding multiple elements at once.
```

## File: en/guides/workflow/additional-features.md
```markdown
# Additional Features

Both Workflow and Chatflow applications support enabling additional features to enhance the user interaction experience. For example, adding a file upload entry, giving the LLM application a self-introduction, or using a welcome message can provide users with a richer interactive experience.

Click the **"Features"** button in the upper right corner of the application to add more functionality.

{% embed url="https://www.motionshot.app/walkthrough/6773d34ad27e58127b913945/embed?fullscreen=1&hideAsSteps=1&hideCopy=1&hideDownload=1&hideSteps=1" %}

#### Workflow

> Note: This method of adding file uploads to Workflow applications is deprecated. We recommend adding custom file variables on the start node instead.

Workflow type applications only support the **"Image Upload"** feature. When enabled, an image upload entry will appear on the usage page of the Workflow application.

{% embed url="https://www.motionshot.app/walkthrough/6773d3d86a0c3ed534f24da9/embed?fullscreen=1&hideAsSteps=1&hideCopy=1&hideDownload=1&hideSteps=1" %}



**Usage:**

**For application users:** Applications with image upload enabled will display an upload button on the usage page. Click the button or paste a file link to complete the image upload. You will receive the LLM's response to the image.

**For application developers:** After enabling the image upload feature, the uploaded image files will be stored in the `sys.files` variable. Next, add an LLM node, select a large model with vision capabilities, and enable the VISION feature within it. Choose the `sys.files` variable to allow the LLM to read the image file.

Finally, select the output variable of the LLM node in the END node to complete the setup.

#### Chatflow

Chatflow type applications support the following features:

*   **Conversation Opener**

    Allow AI to proactively send a message, which can be a welcome message or AI self-introduction, to bring it closer to the user.
*   **Follow-up**

    Automatically add suggestions for the next question after the conversation is complete, to increase the depth and frequency of dialogue topics.
*   **Text-to-Speech**

    Add an audio playback button in the Q\&A text box, using a TTS service (needs to be set up in Model Providers) to read out the text.
*   **File Upload**

    Supports the following file types: documents, images, audio, video, and other file types. After enabling this feature, application users can upload and update files at any time during the application dialogue. A maximum of 10 files can be uploaded simultaneously, with a size limit of 15MB per file.
*   **Citation and Attribution**

    Commonly used in conjunction with the "Knowledge Retrieval" node to display the reference source documents and attribution parts of the LLM's responses.
*   **Content Moderation**

    Supports using moderation APIs to maintain a sensitive word library, ensuring that the LLM can respond and output safe content. For detailed instructions, please refer to Sensitive Content Moderation.

**Usage:**

Except for the **File Upload** feature, the usage of other features in Chatflow applications is relatively simple. Once enabled, they can be intuitively used on the application interaction page.

This section will mainly introduce the specific usage of the **File Upload** feature:

**For application users:** Chatflow applications with file upload enabled will display a "paperclip" icon on the right side of the dialogue box. Click it to upload files and interact with the LLM.

<figure><img src="../../.gitbook/assets/image (8).png" alt=""><figcaption><p>Upload file</p></figcaption></figure>

**For application developers:**

After enabling the file upload feature, the files users send will be uploaded in the `sys.files` variable. It will be updated after the user sends a new message in the same conversation turn.

Different types of files correspond to different application orchestration methods based on the uploaded file differences.

* **Document Files**

LLMs do not have the ability to directly read document files, so a Document Extractor node is needed to preprocess the files in the `sys.files` variable. The orchestration steps are as follows:

1. Enable the Features function and only check "Documents" in the file types.
2. Select the `sys.files` variable in the input variables of the Document Extractor node.
3. Add an LLM node and select the output variable of the document extractor node in the system prompt.
4. Add a "Direct Reply" node at the end, filling in the output variable of the LLM node.

Chatflow applications built using this method cannot remember the content of uploaded files. Application users need to upload document files in the chat box for each conversation. If you want the application to remember uploaded files, please refer to File Upload: Adding Variables in the Start Node.

* **Image Files**

Some LLMs support directly obtaining information from images, so no additional nodes are needed to process images.

The orchestration steps are as follows:

1. Enable the Features function and only check "Images" in the file types.
2. Add an LLM node, enable the VISION feature, and select the `sys.files` variable.
3. Add a "Answer" node at the end, filling in the output variable of the LLM node.

<figure><img src="../../.gitbook/assets/image (9).png" alt=""><figcaption><p>Enable vision</p></figcaption></figure>

* **Mixed File Types**

If you want the application to have the ability to process both document files and image files simultaneously, you need to use the List Operation node to preprocess the files in the `sys.files` variable, extract more refined variables, and send them to the corresponding processing nodes. The orchestration steps are as follows:

1. Enable the Features function and check both "Images" and "Document Files" types.
2. Add two list operation nodes, extracting image and document variables in the "Filter" condition.
3. Extract document file variables and pass them to the "Document Extractor" node; extract image file variables and pass them to the "LLM" node.
4. Add a "Direct Reply" node at the end, filling in the output variable of the LLM node.

After the application user uploads both document files and images, document files are automatically diverted to the document extractor node, and image files are automatically diverted to the LLM node to achieve joint processing of files.

<figure><img src="../../.gitbook/assets/image (10).png" alt=""><figcaption><p><strong>Mixed File Types</strong></p></figcaption></figure>

* **Audio and Video Files**

LLMs do not yet support direct reading of audio and video files, and the Dify platform has not yet built-in related file processing tools. Application developers can refer to [External Data Tools](../extension/api-based-extension/external-data-tool.md) to integrate tools for processing file information themselves.
```

## File: en/guides/workflow/bulletin.md
```markdown
# Bulletin: Image Upload Replaced by File Upload

The image upload feature has been integrated into the more comprehensive [File Upload](file-upload.md) functionality. To avoid redundant features, we have decided to upgrade and adjust the “[Features](additional-features.md)” for Workflow and Chatflow applications as follows:

* The image upload option in Chatflow’s “Features” has been removed and replaced by the new “File Upload” feature. Within the “File Upload” feature, you can select the image file type. Additionally, the image upload icon in the application dialog has been replaced with a file upload icon.

<figure><img src="../../.gitbook/assets/image (138).png" alt=""><figcaption></figcaption></figure>

* The image upload option in Workflow’s “Features” and the `sys.files` [variable](variables.md) will be deprecated in the future. Both have been marked as `LEGACY`, and developers are encouraged to use custom file variables to add file upload functionality to Workflow applications.

<figure><img src="../../.gitbook/assets/image (140).png" alt=""><figcaption></figcaption></figure>

### Why Replace the “Image Upload” Feature?

Previously, Dify only supported image file uploads. In the latest version, a more comprehensive file upload capability has been introduced, supporting documents, images, audio, video, and custom file formats.&#x20;

**Image uploading is now part of the broader “File Upload” feature.** When adding the file upload feature, developers can simply check the “image” file type to enable image uploads.

To avoid confusion caused by redundant features, we have decided to replace the standalone image upload feature in Chatflow applications with the more comprehensive file upload capability, and no longer recommend enabling image upload for Workflow applications.

### More Comprehensive Functionality: File Upload

To enhance the information processing capabilities of your applications, we have introduced the “File Upload” feature in this update. Unlike chat text, document files can carry a large amount of information, such as academic reports or legal contracts.

* The file upload feature allows files to be uploaded, parsed, referenced, and downloaded as file variables within Workflow applications.&#x20;
* Developers can now easily build applications capable of understanding and processing complex tasks involving images, audio, and video.

<figure><img src="../../.gitbook/assets/image (141).png" alt="" width="375"><figcaption></figcaption></figure>

We no longer recommend using the standalone “Image Upload” feature and instead suggest transitioning to the more comprehensive “File Upload” feature to improve the application experience.

### What You Need to Do?

#### For Dify Cloud Users: 

* **Chatflow Applications**

If you have already created Chatflow applications with the “Image Upload” feature enabled and activated the Vision feature in the LLM node, the system will automatically switch the feature, and it will not affect the application’s image upload capability. If you need to update and republish the application, select the file variable in the Vision variable selection box of the LLM node, clear the item from the checklist, and republish the application.\


<figure><img src="../../.gitbook/assets/image (142).png" alt=""><figcaption></figcaption></figure>

If you wish to add the “Image Upload” feature to a Chatflow application, enable “File Upload” in the features and select only the “image” file type. Then enable the Vision feature in the LLM node and specify the sys.files variable. The upload entry will appear as a “paperclip” icon. For detailed instructions, refer to [Additional Features](additional-features.md).

<figure><img src="../../.gitbook/assets/image (143).png" alt=""><figcaption></figcaption></figure>

* **Workflow Applications**

If you have already created Workflow applications with the “Image Upload” feature enabled and activated the Vision feature in the LLM node, this change will not affect you immediately, but you will need to complete manual migration before the official deprecation.

If you wish to enable the “Image Upload” feature for a Workflow application, add a file variable in the [Start](node/start.md) node. Then, reference this file variable in subsequent nodes instead of using the `sys.files` variable.

#### For Dify Community Edition or Self-hosted Enterprise Users:

After upgrading to version v0.10.0, you will see the “File Upload” feature.

* Chatflow Applications:

Chatflow applications with the “Image Upload” feature enabled will automatically switch to the file upload feature, with no changes required.

If you wish to add the “Image Upload” feature to a Chatflow application, refer to the Additional Features section for detailed instructions.

* Workflow Applications:

Existing Workflow applications will not be affected, but please complete the manual migration before the official deprecation.

If you wish to enable the “Image Upload” feature for a Workflow application, add a file variable in the [Start](node/start.md) node. Then, reference this file variable in subsequent nodes instead of using the `sys.files` variable.\

### FAQs:

#### 1. Will This Update Affect My Existing Applications?

* Existing Chatflow applications will automatically migrate, seamlessly switching image upload capabilities to the file upload feature. The `sys.files` variable will still be used as the default Vision input. The image upload entry in the application interface will be replaced with a file upload entry.
* Existing Workflow applications will not be affected for now. The `sys.files` variable and the image upload feature have been marked as `LEGACY`, but they can still be used. However, these `LEGACY` features will be deprecated in the future, and a manual update will be required at that time.

#### 2. Do I Need to Update My Applications Immediately?

* For Chatflow applications, the system will automatically migrate, and no manual updates are required.
* For Workflow applications, although an immediate update is not necessary, we recommend familiarizing yourself with the new file upload feature to prepare for future migration.

#### 3. How Can I Ensure My Applications Are Compatible with the New File Upload Feature?

For Chatflow applications:

• Check if the file upload option is enabled in the features configuration.

• Ensure you’re using an LLM with Vision capabilities, and turn on the Vision toggle.

• Verify that `sys.files` is correctly selected as the input item in the Vision box.

\
For Workflow applications:

• Create a file-type variable in the “Start” node.

• Reference this file variable in subsequent nodes instead of using the LEGACY `sys.files` variable.

#### 4. How to handle missing image upload icons in previously published Chatflow applications?

It is recommended to republish the application, and the file upload icon will appear in the application's chat box.

#### We Value Your Feedback

As a key member of the Dify community, your experience and feedback are crucial to us. We warmly invite you to:

1. Try the file upload feature and experience its convenience and flexibility.
2.  Share your thoughts and suggestions via the following channels:

    • [GitHub](https://github.com/langgenius/dify)

    • [Discord channel](https://discord.com/invite/FngNHpbcY7)

Your feedback will help us continuously improve the product and provide a better experience for the entire community.
```

## File: en/guides/workflow/export_import.md
```markdown
# Export/Import

You can export/import application templates as YAML-format DSL (Domain Specific Language) files within the studio to share applications with your team members.

To import a DSL file in the studio application list:

<figure><img src="/en/.gitbook/assets/guides/workflow/export-import/output (5) (2).png" alt=""><figcaption></figcaption></figure>

To export a DSL file from the studio application list:

<figure><img src="/en/.gitbook/assets/guides/workflow/export-import/output (6) (1).png" alt=""><figcaption></figcaption></figure>

To export a DSL file from the workflow orchestration page:

<figure><img src="/en/.gitbook/assets/guides/workflow/export-import/output (7) (1).png" alt=""><figcaption></figcaption></figure>
```

## File: en/guides/workflow/file-upload.md
```markdown
# File Upload

Compared to chat text, document files can contain vast amounts of information, such as academic reports and legal contracts. However, Large Language Models (LLMs) are inherently limited to processing only text or images, making it challenging to extract the rich contextual information within these files. As a result, application users often resort to manually copying and pasting large amounts of information to converse with LLMs, significantly increasing unnecessary operational overhead.

The file upload feature addresses this limitation by allowing files to be uploaded, parsed, referenced, and downloaded as File variables within workflow applications. **This empowers developers to easily construct complex workflows capable of understanding and processing various media types, including images, audio, and video.**

## Application Scenarios

1. **Document Analysis**: Upload academic research reports for LLMs to quickly summarize key points and answer related questions based on the file content.
2. **Code Review**: Developers can upload code files to receive optimization suggestions and bug detection.
3. **Learning Assistance**: Students can upload assignments or study materials for personalized explanations and guidance.
4. **Legal Aid**: Upload complete contract texts for LLMs to assist in reviewing clauses and identifying potential risks.

## Difference Between File Upload and Knowledge Base

Both file upload and knowledge base provide additional contextual information for LLMs, but they differ significantly in usage scenarios and functionality:

1. **Information Source**:
   * File Upload: Allows end-users to dynamically upload files during conversations, providing immediate, personalized contextual information.
   * Knowledge Base: Pre-set and managed by application developers, containing a relatively fixed set of information.
2. **Usage Flexibility**:
   * File Upload: More flexible, users can upload different types of files based on specific needs.
   * Knowledge Base: Content is relatively fixed but can be reused across multiple sessions.
3. **Information Processing**:
   * File Upload: Requires document extractors or other tools to convert file content into text that LLMs can understand.
   * Knowledge Base: Usually pre-processed and indexed, ready for direct retrieval.
4. **Application Scenarios**:
   * File Upload: Suitable for scenarios that need to process user-specific documents, such as document analysis, personalized learning assistance, etc.
   * Knowledge Base: Suitable for scenarios that require access to a large amount of preset information, such as customer service, product inquiries, etc.
5. **Data Persistence**:
   * File Upload: Typically for temporary use, not stored long-term in the system.
   * Knowledge Base: Exists as a long-term part of the application, can be continuously updated and maintained.

## Quick Start: Building a Chatflow / Workflow Application with File Upload Feature

Dify supports file uploads in both [ChatFlow](key-concepts.md) and [WorkFlow](key-concepts.md#chatflow-and-workflow) type applications, processing them through variables for LLMs. Application developers can refer to the following methods to enable file upload functionality:

* In Workflow applications:
  * Add file variables in the ["Start Node"](node/start.md)
* In ChatFlow applications:
  * Enable file upload in ["Additional Features"](additional-features.md) to allow direct file uploads in the chat window
  * Add file variables in the "[Start Node"](node/start.md)
  * Note: These two methods can be configured simultaneously and are independent of each other. The file upload settings in additional features (including upload method and quantity limit) do not affect the file variables in the start node. For example, if you only want to create file variables through the start node, you don't need to enable the file upload feature in additional features.

These two methods provide flexible file upload options for applications to meet the needs of different scenarios.

**File Types**

`File` variables and `array[file]` variables support the following file types and formats:

<table data-header-hidden><thead><tr><th width="227"></th><th></th></tr></thead><tbody><tr><td>File Type</td><td>Supported Formats</td></tr><tr><td>Documents</td><td>TXT, MARKDOWN, PDF, HTML, XLSX, XLS, DOCX, CSV, EML, MSG, PPTX, PPT, XML, EPUB.</td></tr><tr><td>Images</td><td>JPG, JPEG, PNG, GIF, WEBP, SVG.</td></tr><tr><td>Audio</td><td>MP3, M4A, WAV, WEBM, AMR.</td></tr><tr><td>Video</td><td>MP4, MOV, MPEG, MPGA.</td></tr><tr><td>Others</td><td>Custom file extension support</td></tr></tbody></table>

#### Method 1: Using an LLM with File Processing Capabilities

Some LLMs, such as [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support), now support direct processing and analysis of file content, enabling the use of file variables in the LLM node's prompts.

> To prevent potential issues, application developers should verify the supported file types on the LLM's official website before utilizing the file variable.

1. Click to create a Chatflow or Workflow application.
2. Add an LLM node and select an LLM with file processing capabilities.
3. Add a file variable in the start node.
4. Enter the file variable in the system prompt of the LLM node.
5. Complete the setup.

![](https://assets-docs.dify.ai/2024/11/a7154e8966d979dcba13eac0a172ef89.png)

**Method 2: Enable File Upload in Application Chat Box (Chatflow Only)**

1. Click the **"Features"** button in the upper right corner of the Chatflow application to add more functionality to the application. After enabling this feature, application users can upload and update files at any time during the application dialogue. A maximum of 10 files can be uploaded simultaneously, with a size limit of 15MB per file.

<figure><img src="../../.gitbook/assets/image (1) (3) (1).png" alt=""><figcaption><p>file upload</p></figcaption></figure>

Enabling this feature does not grant LLMs the ability to directly read files. A **Document Extractor** is still needed to parse documents into text for LLM comprehension.

* For audio files, models like `gpt-4o-audio-preview` that support multimodal input can process audio directly without additional extractors.
* For video and other file types, there are currently no corresponding extractors. Application developers need to [integrate external tools](../extension/api-based-extension/external-data-tool.md) for processing.

2. Add a Document Extractor node, and select the `sys.files` variable in the input variables.
3. Add an LLM node and select the output variable of the Document Extractor node in the system prompt.
4. Add an "Answer" node at the end, filling in the output variable of the LLM node.

<figure><img src="../../.gitbook/assets/image (2) (4).png" alt=""><figcaption></figcaption></figure>

Once enabled, users can upload files and engage in conversations in the dialogue box. However, with this method, the LLM application does not have the ability to remember file contents, and files need to be uploaded for each conversation.

<figure><img src="../../.gitbook/assets/image (3) (3).png" alt=""><figcaption></figcaption></figure>

If you want the LLM to remember file contents during conversations, please refer to Method 3.

**Method 3: Enable File Upload by Adding File Variables**

**1. Add File Variables in the "Start" Node**

Add input fields in the application's "Start" node, choosing either **"Single File"** or **"File List"** as the field type for the variable.

{% embed url="https://www.motionshot.app/walkthrough/6773d0e0d27e58127b913943/embed?fullscreen=1&hideAsSteps=1&hideCopy=1&hideDownload=1&hideSteps=1" %}



*   **Single File**

    Allows the application user to upload only one file.
*   **File List**

    Allows the application user to batch upload multiple files at once.

> For ease of operation, we will use a single file variable as an example.

**File Parsing**

There are two main ways to use file variables:

1. Using tool nodes to convert file content:
   * For document-type files, you can use the "Document Extractor" node to convert file content into text form.
   * This method is suitable for cases where file content needs to be parsed into a format that the model can understand (such as string, array\[string], etc.).
2. Using file variables directly in LLM nodes:
   * For certain types of files (such as images), you can use file variables directly in LLM nodes.
   * For example, for file variables of image type, you can enable the vision feature in the LLM node and then directly reference the corresponding file variable in the variable selector.

The choice between these methods depends on the file type and your specific requirements. Next, we will detail the specific steps for both methods.

**2. Add Document Extractor Node**

After uploading, files are stored in single file variables, which LLMs cannot directly read. Therefore, a **"Document Extractor"** node needs to be added first to extract content from uploaded document files and send it to the LLM node for information processing.

Use the file variable from the "Start" node as the input variable for the **"Document Extractor"** node.

<figure><img src="../../.gitbook/assets/image (4).png" alt=""><figcaption><p><strong>Document Extractor</strong></p></figcaption></figure>

Fill in the output variable of the "Document Extractor" node in the system prompt of the LLM node.

<figure><img src="../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

After completing these settings, application users can paste file URLs or upload local files in the WebApp, then interact with the LLM about the document content. Users can replace files at any time during the conversation, and the LLM will obtain the latest file content.

**Referencing File Variables in LLM Nodes**

For certain file types (such as images), file variables can be directly used within LLM nodes. This method is particularly suitable for scenarios requiring visual analysis. Here are the specific steps:

1. In the LLM node, enable the vision functionality. This allows the model to process image inputs (the model must support vision capabilities).
2. In the variable selector of the LLM node, directly reference the previously created file variable. If file upload was enabled through additional features, select the `sys.files` variable.
3. In the system prompt, guide the model on how to process the image input. For example, you can instruct the model to describe the image content or answer questions about the image.

Below is an example configuration:

<figure><img src="../../.gitbook/assets/image (6).png" alt=""><figcaption><p>Using file variables directly in LLM node</p></figcaption></figure>

It's important to note that when directly using file variables in LLM node, the developers need to ensure that the file variable contains only image files; otherwise, errors may occur. If users might upload different types of files, we need to use list operator node for filtering files.

**File Download**

Placing file variables in answer nodes or end nodes will provide a file download card in the conversation box when the application reaches that node. Clicking the card allows for file download.

<figure><img src="../../.gitbook/assets/image (7).png" alt=""><figcaption><p>file download</p></figcaption></figure>

## Advanced Usage

If you want the application to support uploading multiple types of files, such as allowing users to upload document files, images, and audio/video files simultaneously, you need to add a "File List" variable in the "Start Node" and use the "List Operation" node to process different file types. For detailed instructions, please refer to the List Operation node.
```

## File: en/guides/workflow/key-concepts.md
```markdown
# Key Concepts

### Nodes

**Nodes are the key components of a workflow**. By connecting nodes with different functionalities, you can execute a series of operations within the workflow.

For core workflow nodes, please refer to [Block Description](node/).

***

### Variables

**Variables are used to link the input and output of nodes within a workflow**, enabling complex processing logic throughout the process. Fore more details, please take refer to [Variables](variables.md).

***

### Chatflow and Workflow

**Application Scenarios**

* **Chatflow**: Designed for conversational scenarios, including customer service, semantic search, and other conversational applications that require multi-step logic in response construction.
* **Workflow**: Geared towards automation and batch processing scenarios, suitable for high-quality translation, data analysis, content generation, email automation, and more.

**Usage Entry Points**

![Chatflow](https://assets-docs.dify.ai/2024/12/befca8ff01ac5dccf4d32bcab08b8a11.png)

![Workflow](https://assets-docs.dify.ai/2024/12/56521297208916676acaf1c59e968e41.png)

**Differences in Available Nodes**

1. The [End node](node/end.md) is an ending node for Workflow and can only be selected at the end of the process.
2. The [Answer node](node/answer.md) is specific to Chatflow, used for streaming text output, and can output at intermediate steps in the process.
3. Chatflow has built-in chat memory (Memory) for storing and passing multi-turn conversation history, which can be enabled in nodes like LLM and question classifiers. Workflow does not have Memory-related configurations and cannot enable them.
4. Built-in variables for Chatflow's [start node](node/start.md) include: `sys.query`, `sys.files`, `sys.conversation_id`, `sys.user_id`. Built-in [variables](variables.md) for Workflow's start node include: `sys.files`, `sys_id`.
```

## File: en/guides/workflow/orchestrate-node.md
```markdown
# Orchestrate Node

Both Chatflow and Workflow applications support node orchestration through visual drag-and-drop, with two orchestration design patterns: serial and parallel.

![](../../.gitbook/assets/orchestrate-node.jpeg)

## Serial Node Design Pattern

In this pattern, nodes execute sequentially in a predefined order. Each node initiates its operation only after the preceding node has completed its task and generated output. This helps ensure tasks are executed in a logical sequence.

Consider a "Novel Generation" Workflow App implementing serial pattern: after the user inputs the novel style, rhythm, and characters, the LLM completes the novel outline, plot, and ending in sequence. Each node works based on the output of the previous node, ensuring consistency in the novel's style.

### Designing Serial Structure

1. Click the `+` icon between two nodes to insert a new serial node.
2. Sequentially link the nodes.
3. Converge all paths to the "End" node to finalize the workflow.

![](../../.gitbook/assets/orchestrate-node-serial-design.png)

### Viewing Serial Structure Application Logs

In a serial structure application, logs display node operations sequentially. Click "View Logs - Tracing" in the upper right corner of the dialog box to see the complete workflow process, including input/output, token consumption, and runtime for each node.

![](../../.gitbook/assets/viewing-serial-structure-app-logs.png)

## Designing Parallel Structure

This architectural pattern enables concurrent execution of multiple nodes. The preceding node can simultaneously trigger multiple nodes within the parallel structure. These parallel nodes operate independently, executing tasks concurrently and significantly enhancing overall workflow efficiency.

Consider a translation workflow application implementing parallel architecture: Once the user inputs the source text, triggering the workflow, all nodes within the parallel structure simultaneously receive instructions from the preceding node. This allows for concurrent translation into multiple languages, significantly reducing overall processing time.

### Parallel Structure Design Pattern

The following four methods demonstrate how to create a parallel structure through node addition or visual manipulation:

**Method 1** Hover over a node to reveal the `+` button. Click it to add multiple nodes, automatically forming a parallel structure.

![](../../.gitbook/assets/orchestrate-node-parallel-design-method-1.png)

**Method 2** Extend a connection from a node by dragging its `+` button, creating a parallel structure.

![](../../.gitbook/assets/orchestrate-node-parallel-design-method-2.png)

**Method 3** With multiple nodes on the canvas, visually drag and link them to form a parallel structure.

![](../../.gitbook/assets/orchestrate-node-parallel-design-method-3.png)

**Method 4** In addition to canvas-based methods, you can generate parallel structures by adding nodes through the "Next Step" section in a node's right-side panel. This approach automatically creates the parallel configuration.

![](../../../img/orchestrate-node-parallel-design-method-4.jpeg)

**Notes:**

* Any node can serve as the downstream node of a parallel structure;
* Workflow applications require a single, unique "end" node;
* Chatflow applications support multiple "answer" nodes. Each parallel structure in these applications must terminate with an "answer" node to ensure proper output of content;
* All parallel structures will run simultaneously; nodes within the parallel structure output results after completing their tasks, with no order relationship in output. The simpler the parallel structure, the faster the output of results.

![](../../.gitbook/assets/orchestrate-node-chatflow-multi-answer.png)

### Designing Parallel Structure Patterns

The following four patterns demonstrate common parallel structure designs:

#### 1. Normal Parallel

Normal parallel refers to the `Start | Parallel Nodes | End three-layer` relationship, which is also the smallest unit of parallel structure. This structure is intuitive, allowing the workflow to execute multiple tasks simultaneously after user input.

The upper limit for parallel branches is 10.

![](../../.gitbook/assets/orchestrate-node-simple-parallel.png)

#### 2. Nested Parallel

Nested parallel refers to the Start | Multiple Parallel Structures | End multi-layer relationship. It is suitable for more complex workflows, such as needing to request an external API within a certain node and simultaneously passing the returned results to downstream nodes for processing.

A workflow supports up to 3 layers of nesting relationships.

![](../../.gitbook/assets/orchestrate-node-nested-parallel.png)

#### 3. Conditional Branch + Parallel

Parallel structures can also be used in conjunction with conditional branches.

![](../../.gitbook/assets/orchestrate-node-conditional-branch-parallel.png)

#### 4. Iteration Branch + Parallel

This pattern integrates parallel structures within iteration branches, optimizing the execution efficiency of repetitive tasks.

![](../../.gitbook/assets/orchestrate-node-iteration-parallel.png)

### Viewing Parallel Structure Application Logs

Applications with parallel structures generate logs in a tree-like format. Collapsible parallel node groups facilitate easier viewing of individual node logs.

![](../../.gitbook/assets/orchestrate-node-parallel-logs.png)
```

## File: en/guides/workflow/publish.md
```markdown
# Application Publishing

After completing debugging, clicking "Publish" in the upper right corner allows you to save and quickly release the workflow as different types of applications.

<figure><img src="../../.gitbook/assets/output (4) (3).png" alt=""><figcaption></figcaption></figure>

Conversational applications can be published as:

* Run App
* Embed into Site
* Access API Reference

Workflow applications can be published as:

* Run App
* Batch Run App
* Access API Reference

You can also click "Restore" to preview the last published version of the application. Confirming the restore will use the last published workflow version to overwrite the current workflow version.
```

## File: en/guides/workflow/README.md
```markdown
# Workflow

### Basic Introduction

Workflows reduce system complexity by breaking down complex tasks into smaller steps (nodes), reducing reliance on prompt engineering and model inference capabilities, and enhancing the performance of LLM applications for complex tasks. This also improves the system's interpretability, stability, and fault tolerance.

Dify workflows are divided into two types:

* **Chatflow**: Designed for conversational scenarios, including customer service, semantic search, and other conversational applications that require multi-step logic in response construction.
* **Workflow**: Geared towards automation and batch processing scenarios, suitable for high-quality translation, data analysis, content generation, email automation, and more.

<figure><img src="../../.gitbook/assets/image (156).png" alt=""><figcaption></figcaption></figure>

To address the complexity of user intent recognition in natural language input, Chatflow provides question understanding nodes. Compared to Workflow, it adds support for Chatbot features such as conversation history (Memory), annotated replies, and Answer nodes.

To handle complex business logic in automation and batch processing scenarios, Workflow offers a variety of logic nodes, such as code nodes, IF/ELSE nodes, template transformation, iteration nodes, and more. Additionally, it provides capabilities for timed and event-triggered actions, facilitating the construction of automated processes.

### Common Use Cases

* Customer Service

By integrating LLM into your customer service system, you can automate responses to common questions, reducing the workload of your support team. LLM can understand the context and intent of customer queries and generate helpful and accurate answers in real-time.

* Content Generation

Whether you need to create blog posts, product descriptions, or marketing materials, LLM can assist by generating high-quality content. Simply provide an outline or topic, and LLM will use its extensive knowledge base to produce engaging, informative, and well-structured content.

* Task Automation

LLM can be integrated with various task management systems like Trello, Slack, and Lark to automate project and task management. Using natural language processing, LLM can understand and interpret user input, create tasks, update statuses, and assign priorities without manual intervention.

* Data Analysis and Reporting

LLM can analyze large datasets and generate reports or summaries. By providing relevant information to LLM, it can identify trends, patterns, and insights, transforming raw data into actionable intelligence. This is particularly valuable for businesses looking to make data-driven decisions.

* Email Automation

LLM can be used to draft emails, social media updates, and other forms of communication. By providing a brief outline or key points, LLM can generate well-structured, coherent, and contextually relevant messages. This saves significant time and ensures your responses are clear and professional.

### How to Get Started

* Start by building a workflow from scratch or use system templates to help you get started.
* Get familiar with basic operations, including creating nodes on the canvas, connecting and configuring nodes, debugging workflows, and viewing run history.
* Save and publish a workflow.
* Run the published application or call the workflow through an API.
```

## File: en/guides/workflow/shortcut-key.md
```markdown
# Shortcut Key

The Chatflow / Workflow application orchestration page supports the following shortcut keys to help you improve the efficiency of orchestration nodes.

| Windows          | macOS               | Explanation                    |
| ---------------- | ------------------- | ------------------------------ |
| Ctrl + C         | Command + C         | Copy nodes                     |
| Ctrl + V         | Command + V         | Paste nodes                    |
| Ctrl + D         | Command + D         | Duplicate nodes                |
| Ctrl + O         | Command + O         | Organize nodes                 |
| Ctrl + Z         | Command + Z         | Undo                           |
| Ctrl + Y         | Command + Y         | Redo                           |
| Ctrl + Shift + Z | Command + Shift + Z | Redo                           |
| Ctrl + 1         | Command + 1         | Canvas fits view               |
| Ctrl + (-)       | Command + (-)       | Canvas zooms out               |
| Ctrl + (=)       | Command + (=)       | Canvas zooms in                |
| Shift + 1        | Shift + 1           | Resets canvas view to 100%     |
| Shift + 5        | Shift + 5           | Scales canvas to 50%           |
| H                | H                   | Canvas toggles to Hand mode    |
| V                | V                   | Canvas toggles to Pointer mode |
| Delete/Backspace | Delete/Backspace    | Delete selected nodes          |
| Alt + R          | Option + R          | Workflow starts to run         |
```

## File: en/guides/workflow/variables.md
```markdown
---
description: Last edited by Allen, Dify Technical Writer
---

# Variables

**Workflow** and **Chatflow** Application are composed of independent nodes. Most nodes have input and output items, but the input and output information for each node is not consistent and dynamic.

**How to use a fixed symbol to refer dynamically changing content?** Variables, as dynamic data containers, can store and transmit unfixed content, being referenced mutually within different nodes, providing flexible information mobility between nodes.

### System Variables

System variables refer to pre-set system-level parameters within Chatflow / Workflow App that can be globally read by other nodes. All system-level variables begin with `sys.`

#### Workflow

Workflow type application provides the system variables below:

<table><thead><tr><th>Variables name</th><th>Data Type</th><th width="267">Description</th><th>Remark</th></tr></thead><tbody><tr><td><p><code>sys.files</code></p><p><code>[LEGACY]</code></p></td><td>Array[File]</td><td>File Parameter: Stores images uploaded by users</td><td>The image upload function needs to be enabled in the 'Features' section in the upper right corner of the application orchestration page</td></tr><tr><td><code>sys.user_id</code></td><td>String</td><td>User ID: A unique identifier automatically assigned by the system to each user when they use a workflow application. It is used to distinguish different users</td><td></td></tr><tr><td><code>sys.app_id</code></td><td>String</td><td>App ID: A unique identifier automatically assigned by the system to each App. This parameter is used to record the basic information of the current application. </td><td>This parameter is used to differentiate and locate distinct Workflow applications for users with development capabilities</td></tr><tr><td><code>sys.workflow_id</code></td><td>String</td><td>Workflow ID: This parameter records information about all nodes information in the current Workflow application.</td><td>This parameter can be used by users with development capabilities to track and record information about the nodes contained within a Workflow</td></tr><tr><td><code>sys.workflow_run_id</code></td><td>String</td><td>Workflow Run ID: Used to record the runtime status and execution logs of a Workflow application.</td><td>This parameter can be used by users with development capabilities to track the application's historical execution records</td></tr></tbody></table>

<figure><img src="../../.gitbook/assets/image (15).png" alt=""><figcaption><p>Workflow App System Variables</p></figcaption></figure>

#### Chatflow

Chatflow type application provides the following system variables:

<table><thead><tr><th>Variables name</th><th>Data Type</th><th width="283">Description</th><th>Remark</th></tr></thead><tbody><tr><td><code>sys.query</code></td><td>String</td><td>Content entered by the user in the chatting box.</td><td></td></tr><tr><td><code>sys.files</code></td><td>Array[File]</td><td>File Parameter: Stores images uploaded by users</td><td>The image upload function needs to be enabled in the 'Features' section in the upper right corner of the application orchestration page</td></tr><tr><td><code>sys.dialogue_count</code></td><td>Number</td><td><p>The number of conversations turns during the user's interaction with a Chatflow application. The count automatically increases by one after each chat round and can be combined with if-else nodes to create rich branching logic.<br></p><p>For example, LLM will review the conversation history at the X conversation turn and automatically provide an analysis.</p></td><td></td></tr><tr><td><code>sys.conversation_id</code></td><td>String</td><td>A unique ID for the chatting box interaction session, grouping all related messages into the same conversation, ensuring that the LLM continues the chatting on the same topic and context.</td><td></td></tr><tr><td><code>sys.user_id</code></td><td>String</td><td>A unique ID is assigned for each application user to distinguish different conversation users.</td><td></td></tr><tr><td><code>sys.workflow_id</code></td><td>String</td><td>Workflow ID: This parameter records information about all nodes information in the current Workflow application.</td><td>This parameter can be used by users with development capabilities to track and record information about the nodes contained within a Workflow</td></tr><tr><td><code>sys.workflow_run_id</code></td><td>String</td><td>Workflow Run ID: Used to record the runtime status and execution logs of a Workflow application.</td><td>This parameter can be used by users with development capabilities to track the application's historical execution records</td></tr></tbody></table>

<figure><img src="../../.gitbook/assets/image (1) (1) (1) (1).png" alt="chatflow app system variables"><figcaption><p>Chatflow App System Variables</p></figcaption></figure>

### Environment Variables

**Environment variables are used to protect sensitive information involved in workflows**, such as API keys and database passwords used when running workflows. They are stored in the workflow rather than in the code, allowing them to be shared across different environments.

<figure><img src="../../.gitbook/assets/en-env-variable.png" alt="Environment Variables"><figcaption><p>Environment Variables</p></figcaption></figure>

Supports the following 3 data types:

* String
* Number
* Secret

Environmental variables have the following characteristics:

* Environment variables can be globally referenced within most nodes;
* Environment variable names cannot be duplicated;
* Output variables of nodes are generally read-only and cannot be written to.

***

### Conversation Variables

> Conversation variables are only applicable to [Chatflow](variables.md#chatflow-and-workflow) App.

**Conversation variables allow application developers to specify particular information that needs to be temporarily stored within the same Chatflow session, ensuring that this information can be referenced across multiple rounds of chatting within the current chatflow**. This can include context, files uploaded to the chatting box(coming soon), user preferences input during the conversation, etc. It's like providing a "memo" for the LLM that can be checked at any time, avoiding information bias caused by LLM memory errors.

For example, you can store the language preference input by the user in the first round of chatting in a conversation variable. The LLM will refer to the information in the conversation variable when answering and use the specified language to reply to the user in subsequent chats.

<figure><img src="../../.gitbook/assets/conversation-var.png" alt=""><figcaption><p>Conversation Variable</p></figcaption></figure>

**Conversation variables** support the following six data types:

* String
* Number
* Object
* Array\[string]
* Array\[number]
* Array\[object]

**Conversation variables** have the following features:

* Conversation variables can be referenced globally within most nodes in the same Chatflow App;
* Writing to conversation variables requires using the [Variable Assigner](https://docs.dify.ai/guides/workflow/node/variable-assignment) node;
* Conversation variables are read-write variables;

About how to use conversation variables with the Variable Assigner node, please refer to the [Variable Assigner](node/variable-assignment.md).

To track changes in conversation variable values during debugging the application, click the conversation variable icon at the top of the Chatflow application preview page.

![](https://assets-docs.dify.ai/2024/11/cc8067fa4c96436f037f8210ebe3f65c.png)

### Notice

* To avoid variable name duplication, node naming must not be repeated
* The output variables of nodes are generally fixed variables and cannot be edited
```

## File: en/guides/workspace/app/README.md
```markdown
# Discover

## Template Applications

In the **Discover** section, several commonly used template applications are provided. These applications cover areas such as human resources, assistants, translation, programming, and writing.

<figure><img src="../../../../img/explore-apps-by-dify.png" alt=""><figcaption></figcaption></figure>

To use a template application, click the "Add to Workspace" button on the template. You can then use the application in the workspace on the left side.

<figure><img src="../../../../img/create-app.png" alt=""><figcaption></figcaption></figure>

To modify a template and create a new application, click the "Customize" button on the template.

## Workspace

The workspace serves as the navigation for applications. Click on an application within the workspace to use it directly.

Applications in the workspace include your own applications as well as those added to the workspace by other team members.
```

## File: en/guides/workspace/app.md
```markdown
# Discover

## Template Applications

In the **Discover** section, several commonly used template applications are provided. These applications cover areas such as human resources, assistants, translation, programming, and writing.

<figure><img src="../../.gitbook/assets/explore-apps-by-dify.png" alt=""><figcaption></figcaption></figure>

To use a template application, click the "Add to Workspace" button on the template. You can then use the application in the workspace on the left side.

<figure><img src="../../.gitbook/assets/create-app (3).png" alt=""><figcaption></figcaption></figure>

To modify a template and create a new application, click the "Customize" button on the template.

## Workspace

The workspace serves as the navigation for applications. Click on an application within the workspace to use it directly.

Applications in the workspace include your own applications as well as those added to the workspace by other team members.
```

## File: en/guides/workspace/billing.md
```markdown
---
description: Know more about Dify's billing plans to support expanding your usage.
---

# Billing

## Workspace-based Billing

The Dify platform has "workspaces" and "apps". A workspace can contain multiple apps. Each app has capabilities like prompt orchestration, LLM invocation, knowledge RAG, logging & annotation, and standard API delivery. **We recommend one team or organization use one workspace, because our system bills on a per-workspace basis (calculated from total resource consumption within a workspace)**. For example:

```Plaintext

Workspace 1  
App 1(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)
App 2(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API) 
App 3(Prompt, RAG, LLM, Knowledge base, Logging & Annotation, API)
...
Workspace 2
```

## Plan Quotas and Features

We offer a free plan for all users to test your AI app ideas, including 200 OpenAI model message calls. After using up the free allowance, you need to obtain LLM API keys from different model providers, and add them under **Settings --> Model Providers** to enable normal model capabilities.Upgrading your workspace to a paid plan means unlocking paid resources for that workspace. For example: upgrading to Professional allows creating over 10 apps (up to 50) with up to 200MB total vector storage quota combined across projects in that workspace. Different version quotas and features are as follows:

<table><thead><tr><th width="148">Metric</th><th width="237">Sandbox</th><th>Professional </th><th>Team</th></tr></thead><tbody><tr><td>pricing</td><td>Free</td><td>$59/month</td><td>$159/month</td></tr><tr><td><strong>Model Providers</strong></td><td>OpenAI,Anthropic,Azure OpenAI,Llama2,Hugging Face,Replicate</td><td>OpenAI,Anthropic,Azure OpenAI, Llama2,Hugging Face,Replicate</td><td>OpenAI,Anthropic,Azure OpenAI, Llama2,Hugging Face,Replicate</td></tr><tr><td><strong>Team Members</strong></td><td>1</td><td>3</td><td>Unlimited</td></tr><tr><td><strong>Apps</strong></td><td>10</td><td>50</td><td>Unlimited</td></tr><tr><td><strong>Vector Storage</strong></td><td>5MB</td><td>200MB</td><td>1GB</td></tr><tr><td><strong>Document Processing Priority</strong></td><td>Standard</td><td>Priority</td><td>Priority</td></tr><tr><td><strong>Logo Change</strong></td><td>/</td><td>/</td><td>√</td></tr><tr><td><strong>Message Requests</strong></td><td>500 per day</td><td>Unlimited</td><td>Unlimited</td></tr><tr><td><strong>RAG API Requests Quota Limits</strong></td><td>/</td><td>√ Coming soon</td><td>√ Coming soon</td></tr><tr><td><strong>Annotation Quota Limits</strong></td><td>10</td><td>2000</td><td>5000</td></tr><tr><td><strong>Agent Model</strong></td><td>/</td><td>√ Coming soon</td><td>√ Coming soon</td></tr><tr><td><strong>Logs History</strong></td><td>30 days</td><td>Unlimited</td><td>Unlimited</td></tr></tbody></table>

Check out the [pricing page ](https://dify.ai/pricing)to learn more.

> **Vector storage:**Vector storage is used to store knowledge bases as vectors for LLMs to understand. Each 1MB can store about 1.2million characters of vectorized data(estimated using OpenAI Embeddings, varies across models). How much the data shrinks depends on complexity and repetition in the content.
>
> **Annotation Quota Limits:**Manual editing and annotation of responses provides customizable high-quality question-answering abilities for apps.&#x20;
>
> **Message Requests:**The number of times the Dify API is called daily during application sessions (rather than LLM API resource usage). It includes all messages generated from your applications via API calls or during WebApp sessions. **Note:Daily quotas are refreshed at midnight Pacific Standard Time.**
>
> **RAG API Requests:**Refers to the number of API calls invoking only the knowledge base processing capabilities of Dify.

## Monitor Resource Usage

You can view capacity usage details on your workspace's Billing page.

<figure><img src="../.gitbook/assets/usage.png" alt=""><figcaption><p>monitor resource usage</p></figcaption></figure>

## FAQ

1.  What happens if my resource usage exceeds the Free plan before I upgrade to a paid plan?

    > During Dify's Beta stage, excess quotas were provided for free to seed users' workspaces. After Dify's billing system goes live, your existing data will not be lost, but your workspace can no longer process additional text vectorization storage. You will need to upgrade to a suitable plan to continue using Dify.
2.  What if neither the Professional nor Team plans meet my usage needs?

    > If you are a large enterprise requiring more advanced plans, please email us at [business@dify.ai](mailto:business@dify.ai).
3.  Under what circumstances do I need to pay when using the CE version?

    > When using the CE version, please follow our open source license terms. If you need commercial use, such as removing Dify's logo or requiring multiple workspaces, using Dify in a SaaS model, you will need to contact us at [business@dify.ai](mailto:business@dify.ai) for commercial licensing.
```

## File: en/guides/workspace/explore.md
```markdown
# Discovery

## Template application

In **Explore > Discovery**, some commonly used template applications are provided. These apps cover translate, writing, programming and assistant.

<figure><img src="../explore/images/explore-app.jpg" alt=""><figcaption></figcaption></figure>

If you want to use a template application, click the template's "Add to Workspace" button. In the workspace on the left, the app is available.

<figure><img src="../explore/images/creat-customize-app.jpg" alt=""><figcaption></figcaption></figure>

If you want to modify a template to create a new application, click the "Customize" button of the template.

## Workspace

The workspace is the application's navigation. Click an application in the workspace to use the application directly.

<figure><img src="../explore/images/workspace.jpg" alt=""><figcaption></figcaption></figure>

Apps in the workspace include: your own apps and apps added to the workspace by other teams.
```

## File: en/guides/workspace/invite-and-manage-members.md
```markdown
# Inviting and Managing Members

Members of a workspace can be invited and managed by the owner and administrators. After logging in, go to the settings under the user avatar dropdown in Dify, and open the member management interface from the left side of that screen.

### Inviting Members

Provide the email of the invitee. The system will immediately grant the invitee access to the workspace, and the invitee will also receive an email notification.

The system will automatically create a Dify account for the new member.

### Removing Members

Once a member is removed from the team, they will no longer have access to the current workspace. However, this will not affect their access to other workspaces they have already joined.
```

## File: en/guides/workspace/README.md
```markdown
# Collaboration

Dify is a multi-user platform where workspaces are the basic units of team collaboration. Members of a workspace can create and edit applications and knowledge bases, and can also directly use public applications created by other team members in the [Discover](app.md) area.

### Login Methods

It is important to note that the login methods supported by Dify's cloud service and community edition differ, as shown in the table below.

<table><thead><tr><th width="146"></th><th width="299">Community version &#x26; Dify Premium</th><th width="141">Dify Cloud</th><th>Enterprise version</th></tr></thead><tbody><tr><td>Email Login</td><td>Supported</td><td>Not Supported</td><td>Supported</td></tr><tr><td>GitHub Login</td><td>Not Supported</td><td>Supported</td><td>-</td></tr><tr><td>Google Login</td><td>Not Supported</td><td>Supported</td><td>-</td></tr><tr><td>SSO Login</td><td>Not Supported</td><td>Not Supported</td><td>Supported</td></tr></tbody></table>

### Creating an Account

If you are using the cloud service, a workspace will be automatically created for you upon your first login, and you will become the administrator.

In the community version, you will be prompted to set an administrator email and password during installation. The community edition does not support the creation of multiple workspaces.&#x20;
```
